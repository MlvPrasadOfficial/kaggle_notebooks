{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mlvprasad/pandas-in-depth-course-2023-for-indian-kagglers?scriptVersionId=141166995\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"ffe00531","metadata":{"papermill":{"duration":0.054708,"end_time":"2023-08-27T14:37:50.380625","exception":false,"start_time":"2023-08-27T14:37:50.325917","status":"completed"},"tags":[]},"source":["\n","![Pandas](https://github.com/MlvPrasadOfficial/Deep_Learning_Projects/raw/main/11111111l%20Art.png)"]},{"cell_type":"markdown","id":"8860cb9c","metadata":{"papermill":{"duration":0.052922,"end_time":"2023-08-27T14:37:50.484513","exception":false,"start_time":"2023-08-27T14:37:50.431591","status":"completed"},"tags":[]},"source":["![Pandas](https://github.com/MlvPrasadOfficial/ref/raw/main/pandas.png)\n"]},{"cell_type":"markdown","id":"da24f9a2","metadata":{"papermill":{"duration":0.051349,"end_time":"2023-08-27T14:37:50.587646","exception":false,"start_time":"2023-08-27T14:37:50.536297","status":"completed"},"tags":[]},"source":["# Pandas ðŸ’« Chapters index\n","\n","1. [Introduction to Pandas](#introduction-to-pandas)\n","2. [Overview of Data Manipulation and Analysis](#overview-of-data-manipulation-and-analysis)\n","3. [Installing Pandas and Required Dependencies](#installing-pandas-and-required-dependencies)\n","4. [Importing the Pandas Library](#importing-the-pandas-library)\n","5. [Series and DataFrame: Introduction to Data Structures](#series-and-dataframe-introduction-to-data-structures)\n","6. [Creating a DataFrame from Scratch](#creating-a-dataframe-from-scratch)\n","7. [Loading Data into a DataFrame: CSV, Excel, and other formats](#loading-data-into-a-dataframe-csv-excel-and-other-formats)\n","8. [Inspecting DataFrames: head(), tail(), shape, info(), describe()](#inspecting-dataframes-head-tail-shape-info-describe)\n","9. [Indexing and Selecting Data: loc, iloc, column selection](#indexing-and-selecting-data-loc-iloc-column-selection)\n","10. [Conditional Selection and Filtering Data](#conditional-selection-and-filtering-data)\n","    \n","11. [Sorting Data in a DataFrame](#sorting-data-in-a-dataframe)\n","12. [Adding, Updating, and Deleting Columns](#adding-updating-and-deleting-columns)\n","13. [Handling Missing Data: isnull(), dropna(), fillna()](#handling-missing-data-isnull-dropna-fillna)\n","14. [Removing Duplicate Rows: duplicated(), drop_duplicates()](#removing-duplicate-rows-duplicated-drop_duplicates)\n","15. [Data Type Conversion and Handling](#data-type-conversion-and-handling)\n","16. [Dealing with Outliers](#dealing-with-outliers)\n","17. [Concatenating and Merging DataFrames](#concatenating-and-merging-dataframes)\n","18. [Reshaping Data: Pivot, Stack, Unstack](#reshaping-data-pivot-stack-unstack)\n","19. [Grouping and Aggregating Data: groupby(), aggregating functions](#grouping-and-aggregating-data-groupby-aggregating-functions)\n","    \n","20. [Applying Functions to Data: apply(), applymap()](#applying-functions-to-data-apply-applymap)\n","21. [Working with Text Data: String Methods](#working-with-text-data-string-methods)\n","22. [Handling Categorical Data: Categorical Data Types](#handling-categorical-data-categorical-data-types)\n","23. [Working with Dates and Time: Timestamps, Time Zones](#working-with-dates-and-time-timestamps-time-zones)\n","24. [Resampling and Frequency Conversion: Time Series Data](#resampling-and-frequency-conversion-time-series-data)\n","25. [Working with Multi-level Indexing](#working-with-multi-level-indexing)\n","26. [Transformation and Filtering within Groups](#transformation-and-filtering-within-groups)\n","27. [Reading and Writing Data to Databases: SQL](#reading-and-writing-data-to-databases-sql)\n","28. [Working with APIs and Web Scraping](#working-with-apis-and-web-scraping)\n","29. [Integrating Pandas with other Libraries: NumPy, Matplotlib](#integrating-pandas-with-other-libraries-numpy-matplotlib)\n","30. [Data Visualization: Line Plots](#data-visualization-line-plots)\n","    \n","31. [Data Visualization: Bar Plots](#data-visualization-bar-plots)\n","32. [Data Visualization: Scatter Plots](#data-visualization-scatter-plots)\n","33. [Data Visualization: Histograms](#data-visualization-histograms)\n","34. [Data Visualization: Box Plots](#data-visualization-box-plots)\n","35. [Data Visualization: Customizing Plots](#data-visualization-customizing-plots)\n","36. [Handling Large Datasets with Chunking](#handling-large-datasets-with-chunking)\n","37. [Optimizing Memory Usage with Data Types](#optimizing-memory-usage-with-data-types)\n","38. [Performance Optimization Techniques: Vectorization](#performance-optimization-techniques-vectorization)\n","39. [Performance Optimization Techniques: Broadcasting](#performance-optimization-techniques-broadcasting)\n","40. [Performance Optimization Techniques: Indexing and Slicing](#performance-optimization-techniques-indexing-and-slicing)\n","    \n","41. [Working with Panel Data: Panel, Panel4D](#working-with-panel-data-panel-panel4d)\n","42. [Handling Sparse Data](#handling-sparse-data)\n","43. [Advanced Groupby Operations](#advanced-groupby-operations)\n","44. [Pivot Tables and Cross-tabulations](#pivot-tables-and-cross-tabulations)\n","45. [Working with Geospatial Data](#working-with-geospatial-data)\n","46. [Interactive Visualizations with Pandas](#interactive-visualizations-with-pandas)\n","47. [Best Practices and Tips for Pandas](#best-practices-and-tips-for-pandas)\n","48. [Integration with Machine Learning Libraries: Scikit-learn](#integration-with-machine-learning-libraries-scikit-learn)\n","49. [Pandas in a Production Environment](#pandas-in-a-production-environment)\n","50. [Recap and Next Steps](#recap-and-next-steps)\n","    \n","51. [Handling Imbalanced Data](#handling-imbalanced-data)\n","52. [Time Series Analysis with Pandas](#time-series-analysis-with-pandas)\n","53. [Handling JSON Data](#handling-json-data)\n","54. [Working with Excel Files: Reading and Writing](#working-with-excel-files-reading-and-writing)\n","55. [Handling XML Data](#handling-xml-data)\n","56. [Handling HTML Data](#handling-html-data)\n","57. [Dealing with Financial Data](#dealing-with-financial-data)\n","58. [Working with Images and Multimedia Data](#working-with-images-and-multimedia-data)\n","59. [Handling Big Data with Dask](#handling-big-data-with-dask)\n","60. [Parallel Processing with Pandas](#parallel-processing-with-pandas)\n","    \n","61. [Machine Learning Pipelines with Pandas](#machine-learning-pipelines-with-pandas)\n","62. [Time Zone Conversion and Localization](#time-zone-conversion-and-localization)\n","63. [Advanced Data Visualization: Heatmaps](#advanced-data-visualization-heatmaps)\n","64. [Advanced Data Visualization: 3D Plots](#advanced-data-visualization-3d-plots)\n","65. [Handling Network Data](#handling-network-data)\n","66. [Text Mining and Natural Language Processing with Pandas](#text-mining-and-natural-language-processing-with-pandas)\n","67. [Handling Time Series Anomalies](#handling-time-series-anomalies)\n","68. [Data Reshaping: Melting and Pivoting](#data-reshaping-melting-and-pivoting)\n","69. [Advanced Grouping Techniques: Groupby and Filter](#advanced-grouping-techniques-groupby-and-filter)\n","70. [Statistical Analysis with Pandas](#statistical-analysis-with-pandas)\n","    \n","71. [Handling Streaming Data with Pandas](#handling-streaming-data-with-pandas)\n","72. [Time Series Forecasting with Pandas](#time-series-forecasting-with-pandas)\n","73. [Advanced Data Cleaning Techniques](#advanced-data-cleaning-techniques)\n","74. [Regular Expressions in Pandas](#regular-expressions-in-pandas)\n","75. [Handling Encrypted Data](#handling-encrypted-data)\n","76. [Handling Encrypted Data](#handling-encrypted-data)\n","77. [Data Sampling Techniques](#data-sampling-techniques)\n","78. [Working with Sparse Time Series Data](#working-with-sparse-time-series-data)\n","79. [Advanced String Manipulation with Pandas](#advanced-string-manipulation-with-pandas)\n","80. [Handling Multi-dimensional Data: Panel, Panel4D](#handling-multi-dimensional-data-panel-panel4d)\n","    \n","81. [Data Normalization and Scaling](#data-normalization-and-scaling)\n","82. [Handling Multi-index Data: Indexing and Slicing](#handling-multi-index-data-indexing-and-slicing)\n","83. [Time Series Decomposition](#time-series-decomposition)\n","84. [Handling Multi-index Data: Aggregation and Reshaping](#handling-multi-index-data-aggregation-and-reshaping)\n","85. [Time Series Smoothing Techniques](#time-series-smoothing-techniques)\n","86. [Outlier Detection and Treatment](#outlier-detection-and-treatment)\n","87. [Feature Engineering with Pandas](#feature-engineering-with-pandas)\n","88. [Handling Imbalanced Time Series Data](#handling-imbalanced-time-series-data)\n","89. [Dimensionality Reduction with Pandas](#dimensionality-reduction-with-pandas)\n","90. [Working with Database APIs: SQLAlchemy](#working-with-database-apis-sqlalchemy)\n","    \n","91. [Time Series Analysis: Autocorrelation and Partial Autocorrelation](#time-series-analysis-autocorrelation-and-partial-autocorrelation)\n","92. [Working with Financial Time Series Data](#working-with-financial-time-series-data)\n","93. [Handling Seasonal Time Series Data](#handling-seasonal-time-series-data)\n","94. [Sentiment Analysis with Pandas](#sentiment-analysis-with-pandas)\n","95. [Time Series Forecast Evaluation Metrics](#time-series-forecast-evaluation-metrics)\n","96. [Working with Geospatial Time Series Data](#working-with-geospatial-time-series-data)\n","97. [Advanced Data Visualization: Maps and Geographical Plots](#advanced-data-visualization-maps-and-geographical-plots)\n","98. [Handling Unstructured Text Data with Pandas](#handling-unstructured-text-data-with-pandas)\n","99. [Anomaly Detection in Time Series Data](#anomaly-detection-in-time-series-data)\n","100. [Data Cleaning: Handling Inconsistent and Noisy Data](#data-cleaning-handling-inconsistent-and-noisy-data)\n"]},{"cell_type":"markdown","id":"e8ac6b01","metadata":{"papermill":{"duration":0.050541,"end_time":"2023-08-27T14:37:50.689723","exception":false,"start_time":"2023-08-27T14:37:50.639182","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>1</font></h1>\n"]},{"cell_type":"markdown","id":"1a056b1b","metadata":{"papermill":{"duration":0.051172,"end_time":"2023-08-27T14:37:50.791932","exception":false,"start_time":"2023-08-27T14:37:50.74076","status":"completed"},"tags":[]},"source":["# Chapter 1 : Introduction to Pandas\n","\n","## Welcome to Magic world of pandas, where it is bread & butter for Data Analyst and Data Scientist "]},{"cell_type":"markdown","id":"fbbb98a1","metadata":{"papermill":{"duration":0.050588,"end_time":"2023-08-27T14:37:50.893619","exception":false,"start_time":"2023-08-27T14:37:50.843031","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>2</font></h1>\n"]},{"cell_type":"markdown","id":"258c3053","metadata":{"papermill":{"duration":0.051037,"end_time":"2023-08-27T14:37:50.995863","exception":false,"start_time":"2023-08-27T14:37:50.944826","status":"completed"},"tags":[]},"source":["# Chapter 2 : Overview of Data Manipulation and Analysis\n","## 2.1 Introduction to Data Manipulation and Analysis\n","#### Data manipulation and analysis are essential steps in extracting valuable insights from raw data. These processes are crucial in the field of data science and analytics. Pandas is a powerful open-source library in Python that provides efficient tools for data manipulation and analysis."]},{"cell_type":"markdown","id":"3bd8d7fe","metadata":{"execution":{"iopub.execute_input":"2023-07-03T17:23:00.061506Z","iopub.status.busy":"2023-07-03T17:23:00.061142Z","iopub.status.idle":"2023-07-03T17:23:01.050019Z","shell.execute_reply":"2023-07-03T17:23:01.044789Z","shell.execute_reply.started":"2023-07-03T17:23:00.061475Z"},"papermill":{"duration":0.050574,"end_time":"2023-08-27T14:37:51.097327","exception":false,"start_time":"2023-08-27T14:37:51.046753","status":"completed"},"tags":[]},"source":["```python\n","# Example: Importing the Pandas library\n","import pandas as pd\n","\n","# Load data from a CSV file into a DataFrame\n","df = pd.read_csv('data.csv')\n","```"]},{"cell_type":"markdown","id":"4c56413d","metadata":{"papermill":{"duration":0.050884,"end_time":"2023-08-27T14:37:51.199041","exception":false,"start_time":"2023-08-27T14:37:51.148157","status":"completed"},"tags":[]},"source":["# 2.2 Key Features of Pandas\n","#### Pandas offers key features that make it a popular choice for data manipulation and analysis. The two primary data structures in Pandas are Series and DataFrame. A Series is a one-dimensional array-like object, and a DataFrame is a two-dimensional tabular data structure.\n"]},{"cell_type":"code","execution_count":1,"id":"fa6dd3a6","metadata":{"execution":{"iopub.execute_input":"2023-08-27T14:37:51.30321Z","iopub.status.busy":"2023-08-27T14:37:51.302782Z","iopub.status.idle":"2023-08-27T14:37:51.325318Z","shell.execute_reply":"2023-08-27T14:37:51.324333Z"},"papermill":{"duration":0.07792,"end_time":"2023-08-27T14:37:51.328118","exception":false,"start_time":"2023-08-27T14:37:51.250198","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["A    10\n","B    20\n","C    30\n","D    40\n","dtype: int64"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["\n","# Example: Creating a Series\n","import pandas as pd\n","\n","# Create a Series with data and custom index\n","s = pd.Series([10, 20, 30, 40], index=['A', 'B', 'C', 'D'])\n","s"]},{"cell_type":"markdown","id":"08f22015","metadata":{"papermill":{"duration":0.051228,"end_time":"2023-08-27T14:37:51.4308","exception":false,"start_time":"2023-08-27T14:37:51.379572","status":"completed"},"tags":[]},"source":["\n","## 2.3 Data Manipulation with Pandas\n","#### Pandas provides various techniques for data manipulation. You can load data from different sources, clean and preprocess the data, perform transformations, and merge or reshape datasets.\n"]},{"cell_type":"markdown","id":"b236f508","metadata":{"execution":{"iopub.execute_input":"2023-07-03T16:24:07.159143Z","iopub.status.busy":"2023-07-03T16:24:07.158162Z","iopub.status.idle":"2023-07-03T16:24:07.166594Z","shell.execute_reply":"2023-07-03T16:24:07.164957Z","shell.execute_reply.started":"2023-07-03T16:24:07.159108Z"},"papermill":{"duration":0.051911,"end_time":"2023-08-27T14:37:51.536794","exception":false,"start_time":"2023-08-27T14:37:51.484883","status":"completed"},"tags":[]},"source":["```python\n","# Example: Loading data from a CSV file\n","import pandas as pd\n","\n","# Load data from a CSV file into a DataFrame\n","df = pd.read_csv('data.csv')\n","\n","# Example: Cleaning data and handling missing values\n","import pandas as pd\n","\n","# Remove rows with missing values\n","df.dropna()\n","\n","# Fill missing values with a specific value\n","df.fillna(0)\n","\n","# Example: Data transformation and feature engineering\n","import pandas as pd\n","\n","# Create a new column by performing calculations on existing columns\n","df['total'] = df['column1'] + df['column2']\n","\n","# Example: Merging datasets\n","import pandas as pd\n","\n","# Merge two DataFrames based on a common column\n","merged_df = pd.merge(df1, df2, on='common_column')\n","\n","# Example: Reshaping data\n","import pandas as pd\n","\n","# Pivot a DataFrame based on specific columns\n","pivoted_df = df.pivot(index='index_column', columns='column_to_pivot')\n","```"]},{"cell_type":"markdown","id":"f9c62411","metadata":{"papermill":{"duration":0.066744,"end_time":"2023-08-27T14:37:51.655401","exception":false,"start_time":"2023-08-27T14:37:51.588657","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>3</font></h1>\n"]},{"cell_type":"markdown","id":"47f955d4","metadata":{"papermill":{"duration":0.071483,"end_time":"2023-08-27T14:37:51.809888","exception":false,"start_time":"2023-08-27T14:37:51.738405","status":"completed"},"tags":[]},"source":["# Chapter 3: Installing Pandas and Required Dependencies\n","## 3.1 Installation of Pandas\n","#### To install Pandas, you can use the pip package installer, which is the recommended method. Open your command prompt or terminal and run the following command:"]},{"cell_type":"markdown","id":"fddd300c","metadata":{"papermill":{"duration":0.071741,"end_time":"2023-08-27T14:37:51.953913","exception":false,"start_time":"2023-08-27T14:37:51.882172","status":"completed"},"tags":[]},"source":["```python\n","pip install pandas\n","```"]},{"cell_type":"markdown","id":"439f2cfa","metadata":{"papermill":{"duration":0.073183,"end_time":"2023-08-27T14:37:52.100374","exception":false,"start_time":"2023-08-27T14:37:52.027191","status":"completed"},"tags":[]},"source":["\n","## 3.2 Installing Required Dependencies\n","#### Pandas has dependencies, such as NumPy and matplotlib, that need to be installed for its proper functioning. You can install these dependencies using pip.\n"]},{"cell_type":"markdown","id":"7e2e7ff6","metadata":{"papermill":{"duration":0.072316,"end_time":"2023-08-27T14:37:52.245262","exception":false,"start_time":"2023-08-27T14:37:52.172946","status":"completed"},"tags":[]},"source":["```python\n","# Example: Installing NumPy\n","pip install numpy\n","\n","# Example: Installing matplotlib\n","pip install matplotlib\n","\n","```"]},{"cell_type":"markdown","id":"b46113ae","metadata":{"papermill":{"duration":0.075693,"end_time":"2023-08-27T14:37:52.397394","exception":false,"start_time":"2023-08-27T14:37:52.321701","status":"completed"},"tags":[]},"source":["\n","## 3.3 Verifying the Installation\n","#### To ensure that Pandas is installed correctly, you can import it into your Python environment and check its version."]},{"cell_type":"code","execution_count":2,"id":"eba9ce4f","metadata":{"execution":{"iopub.execute_input":"2023-08-27T14:37:52.5492Z","iopub.status.busy":"2023-08-27T14:37:52.54869Z","iopub.status.idle":"2023-08-27T14:37:52.554235Z","shell.execute_reply":"2023-08-27T14:37:52.553421Z"},"papermill":{"duration":0.088155,"end_time":"2023-08-27T14:37:52.560062","exception":false,"start_time":"2023-08-27T14:37:52.471907","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["1.5.3\n"]}],"source":["# Example: Importing Pandas and checking the version\n","import pandas as pd\n","\n","print(pd.__version__)"]},{"cell_type":"markdown","id":"ecadc85f","metadata":{"papermill":{"duration":0.086454,"end_time":"2023-08-27T14:37:52.725861","exception":false,"start_time":"2023-08-27T14:37:52.639407","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>4</font></h1>\n"]},{"cell_type":"markdown","id":"5b2a1da6","metadata":{"papermill":{"duration":0.081038,"end_time":"2023-08-27T14:37:52.89023","exception":false,"start_time":"2023-08-27T14:37:52.809192","status":"completed"},"tags":[]},"source":["# Chapter 4: Importing the Pandas Library\n","## 4.1 Importing Pandas in Python\n","#### You can import the Pandas library using the import statement. It is convention to alias Pandas as pd for brevity in your code.\n"]},{"cell_type":"code","execution_count":3,"id":"6e42d829","metadata":{"execution":{"iopub.execute_input":"2023-08-27T14:37:53.055567Z","iopub.status.busy":"2023-08-27T14:37:53.055132Z","iopub.status.idle":"2023-08-27T14:37:53.059646Z","shell.execute_reply":"2023-08-27T14:37:53.058694Z"},"papermill":{"duration":0.092046,"end_time":"2023-08-27T14:37:53.064181","exception":false,"start_time":"2023-08-27T14:37:52.972135","status":"completed"},"tags":[]},"outputs":[],"source":["\n","# Example: Importing Pandas\n","import pandas as pd\n"]},{"cell_type":"markdown","id":"b03fde31","metadata":{"papermill":{"duration":0.07935,"end_time":"2023-08-27T14:37:53.223645","exception":false,"start_time":"2023-08-27T14:37:53.144295","status":"completed"},"tags":[]},"source":["\n","## 4.2 Exploring the Pandas Namespace\n","#### The Pandas library provides various functions, classes, and attributes that you can access using the Pandas namespace."]},{"cell_type":"markdown","id":"ce1f1dd0","metadata":{"execution":{"iopub.execute_input":"2023-07-03T17:23:57.536382Z","iopub.status.busy":"2023-07-03T17:23:57.535988Z","iopub.status.idle":"2023-07-03T17:23:57.543747Z","shell.execute_reply":"2023-07-03T17:23:57.542351Z","shell.execute_reply.started":"2023-07-03T17:23:57.536351Z"},"papermill":{"duration":0.053049,"end_time":"2023-08-27T14:37:53.348508","exception":false,"start_time":"2023-08-27T14:37:53.295459","status":"completed"},"tags":[]},"source":["```python\n","# Example: Exploring the Pandas namespace\n","import pandas as pd\n","\n","# Accessing functions\n","pd.function_name()\n","\n","# Accessing classes\n","pd.ClassName()\n","\n","# Accessing attributes\n","pd.attribute_name\n","\n","```"]},{"cell_type":"markdown","id":"7189e1f2","metadata":{"papermill":{"duration":0.05122,"end_time":"2023-08-27T14:37:53.451842","exception":false,"start_time":"2023-08-27T14:37:53.400622","status":"completed"},"tags":[]},"source":["## 4.3 Common Importing Techniques\n","#### In addition to importing the entire Pandas library, you can use different techniques to import specific modules, submodules, functions, or classes from Pandas. This allows you to only import what you need, reducing the memory footprint and potential naming conflicts."]},{"cell_type":"markdown","id":"ec7e5c83","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.06025Z","iopub.status.idle":"2023-07-03T17:23:01.061088Z","shell.execute_reply":"2023-07-03T17:23:01.060839Z","shell.execute_reply.started":"2023-07-03T17:23:01.060817Z"},"papermill":{"duration":0.051489,"end_time":"2023-08-27T14:37:53.557096","exception":false,"start_time":"2023-08-27T14:37:53.505607","status":"completed"},"tags":[]},"source":["```python\n","# Example: Importing specific modules or submodules\n","import pandas.module_name\n","import pandas.module_name.submodule\n","\n","# Example: Importing specific functions or classes\n","from pandas import function_name\n","from pandas.module_name import ClassName\n","\n","# Example: Importing Pandas alongside other commonly used libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","```"]},{"cell_type":"markdown","id":"b3d441c5","metadata":{"papermill":{"duration":0.051868,"end_time":"2023-08-27T14:37:53.660549","exception":false,"start_time":"2023-08-27T14:37:53.608681","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>5</font></h1>\n"]},{"cell_type":"markdown","id":"cb2dd498","metadata":{"papermill":{"duration":0.051333,"end_time":"2023-08-27T14:37:53.762928","exception":false,"start_time":"2023-08-27T14:37:53.711595","status":"completed"},"tags":[]},"source":["# Chapter 5: Series and DataFrame: Introduction to Data Structures\n","## 5.1 Introduction to Series\n","#### A Series is a one-dimensional labeled array in Pandas that can hold any data type. In this subchapter, we will explore the basic properties and functionalities of Series, such as indexing, slicing, and mathematical operations."]},{"cell_type":"code","execution_count":4,"id":"e209a5ad","metadata":{"execution":{"iopub.execute_input":"2023-08-27T14:37:53.866838Z","iopub.status.busy":"2023-08-27T14:37:53.866486Z","iopub.status.idle":"2023-08-27T14:37:53.876136Z","shell.execute_reply":"2023-08-27T14:37:53.875207Z"},"papermill":{"duration":0.064199,"end_time":"2023-08-27T14:37:53.878142","exception":false,"start_time":"2023-08-27T14:37:53.813943","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["1    20\n","2    30\n","dtype: int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Example: Creating a Series\n","import pandas as pd\n","\n","# Create a Series from a list\n","s = pd.Series([10, 20, 30, 40])\n","\n","# Example: Indexing and slicing a Series\n","import pandas as pd\n","\n","# Accessing values by index\n","value = s[0]\n","\n","# Slicing the Series\n","sliced_series = s[1:3]\n","sliced_series"]},{"cell_type":"markdown","id":"2c621533","metadata":{"papermill":{"duration":0.05149,"end_time":"2023-08-27T14:37:53.980821","exception":false,"start_time":"2023-08-27T14:37:53.929331","status":"completed"},"tags":[]},"source":["\n","## 5.2 Introduction to DataFrame\n","#### A DataFrame is a two-dimensional tabular data structure in Pandas that consists of rows and columns. It is the primary data structure used for data analysis and manipulation. In this subchapter, we will explore the basic properties and functionalities of DataFrames, including indexing, slicing, and column operations.\n"]},{"cell_type":"code","execution_count":5,"id":"1c9e4750","metadata":{"execution":{"iopub.execute_input":"2023-08-27T14:37:54.086571Z","iopub.status.busy":"2023-08-27T14:37:54.086037Z","iopub.status.idle":"2023-08-27T14:37:54.100968Z","shell.execute_reply":"2023-08-27T14:37:54.099934Z"},"papermill":{"duration":0.070398,"end_time":"2023-08-27T14:37:54.103241","exception":false,"start_time":"2023-08-27T14:37:54.032843","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>Age</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>Emily</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Michael</td>\n","      <td>35</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Name  Age\n","1    Emily   30\n","2  Michael   35"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Example: Creating a DataFrame\n","import pandas as pd\n","\n","# Create a DataFrame from a dictionary\n","data = {'Name': ['John', 'Emily', 'Michael'],\n","        'Age': [25, 30, 35]}\n","df = pd.DataFrame(data)\n","\n","# Example: Indexing and slicing a DataFrame\n","import pandas as pd\n","\n","# Accessing a column by name\n","column = df['Name']\n","\n","# Slicing the DataFrame\n","sliced_df = df.iloc[1:3]\n","sliced_df"]},{"cell_type":"markdown","id":"0abc61d6","metadata":{"papermill":{"duration":0.051775,"end_time":"2023-08-27T14:37:54.209877","exception":false,"start_time":"2023-08-27T14:37:54.158102","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>6</font></h1>\n"]},{"cell_type":"markdown","id":"9b87b624","metadata":{"papermill":{"duration":0.051826,"end_time":"2023-08-27T14:37:54.313529","exception":false,"start_time":"2023-08-27T14:37:54.261703","status":"completed"},"tags":[]},"source":["# Chapter 6: Creating a DataFrame from Scratch\n","## 6.1 Creating a DataFrame from a Dictionary\n","#### In this subchapter, we will learn how to create a DataFrame from scratch using a Python dictionary. We will explore different ways of specifying the index and columns of the DataFrame, as well as handling missing values.\n","\n"]},{"cell_type":"code","execution_count":6,"id":"3e21a215","metadata":{"execution":{"iopub.execute_input":"2023-08-27T14:37:54.418581Z","iopub.status.busy":"2023-08-27T14:37:54.418187Z","iopub.status.idle":"2023-08-27T14:37:54.435377Z","shell.execute_reply":"2023-08-27T14:37:54.434315Z"},"papermill":{"duration":0.072878,"end_time":"2023-08-27T14:37:54.438336","exception":false,"start_time":"2023-08-27T14:37:54.365458","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>Age</th>\n","      <th>City</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>A</th>\n","      <td>John</td>\n","      <td>25</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>B</th>\n","      <td>Emily</td>\n","      <td>30</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>C</th>\n","      <td>Michael</td>\n","      <td>35</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Name  Age City\n","A     John   25  NaN\n","B    Emily   30  NaN\n","C  Michael   35  NaN"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Example: Creating a DataFrame from a dictionary\n","import pandas as pd\n","\n","# Create a DataFrame from a dictionary with default index and columns\n","data = {'Name': ['John', 'Emily', 'Michael'],\n","        'Age': [25, 30, 35]}\n","df = pd.DataFrame(data)\n","\n","# Create a DataFrame with custom index and columns\n","df_custom = pd.DataFrame(data, index=['A', 'B', 'C'], columns=['Name', 'Age', 'City'])\n","df_custom"]},{"cell_type":"markdown","id":"9cd08bfe","metadata":{"papermill":{"duration":0.052277,"end_time":"2023-08-27T14:37:54.547886","exception":false,"start_time":"2023-08-27T14:37:54.495609","status":"completed"},"tags":[]},"source":["\n","## 6.2 Creating a DataFrame from a List of Lists or Numpy Arrays\n","#### In this subchapter, we will explore how to create a DataFrame from a list of lists or NumPy arrays. We will cover scenarios where the lists or arrays have different lengths, and how to handle missing or mismatched values.\n"]},{"cell_type":"code","execution_count":7,"id":"1114a4f4","metadata":{"execution":{"iopub.execute_input":"2023-08-27T14:37:54.654417Z","iopub.status.busy":"2023-08-27T14:37:54.654049Z","iopub.status.idle":"2023-08-27T14:37:54.66599Z","shell.execute_reply":"2023-08-27T14:37:54.665089Z"},"papermill":{"duration":0.067152,"end_time":"2023-08-27T14:37:54.667923","exception":false,"start_time":"2023-08-27T14:37:54.600771","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   A  B  C\n","0  1  2  3\n","1  4  5  6"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Example: Creating a DataFrame from a list of lists\n","import pandas as pd\n","\n","# Create a DataFrame from a list of lists\n","data = [['John', 25], ['Emily', 30], ['Michael', 35]]\n","df = pd.DataFrame(data, columns=['Name', 'Age'])\n","\n","# Example: Creating a DataFrame from NumPy arrays\n","import pandas as pd\n","import numpy as np\n","\n","# Create a DataFrame from NumPy arrays\n","data = np.array([[1, 2, 3], [4, 5, 6]])\n","df = pd.DataFrame(data, columns=['A', 'B', 'C'])\n","df"]},{"cell_type":"markdown","id":"3737788f","metadata":{"papermill":{"duration":0.05195,"end_time":"2023-08-27T14:37:54.773616","exception":false,"start_time":"2023-08-27T14:37:54.721666","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>7</font></h1>"]},{"cell_type":"markdown","id":"d85607a0","metadata":{"papermill":{"duration":0.051704,"end_time":"2023-08-27T14:37:54.876744","exception":false,"start_time":"2023-08-27T14:37:54.82504","status":"completed"},"tags":[]},"source":["# Chapter 7: Loading Data into a DataFrame: CSV, Excel, and other formats\n","## 7.1 Loading Data from a CSV File\n","#### In this subchapter, we will learn how to load data from a CSV file into a DataFrame using Pandas. We will cover different options for customizing the import process, such as specifying delimiter, encoding, and handling missing values."]},{"cell_type":"markdown","id":"944a092d","metadata":{"execution":{"iopub.execute_input":"2023-07-03T17:25:43.035196Z","iopub.status.busy":"2023-07-03T17:25:43.0348Z","iopub.status.idle":"2023-07-03T17:25:43.234331Z","shell.execute_reply":"2023-07-03T17:25:43.231144Z","shell.execute_reply.started":"2023-07-03T17:25:43.035167Z"},"papermill":{"duration":0.097814,"end_time":"2023-08-27T14:37:55.02596","exception":false,"start_time":"2023-08-27T14:37:54.928146","status":"completed"},"tags":[]},"source":["```python\n","# Example: Loading data from a CSV file\n","import pandas as pd\n","\n","# Load data from a CSV file into a DataFrame\n","df = pd.read_csv('data.csv')\n","\n","# Example: Customizing the import process\n","import pandas as pd\n","\n","# Load data with custom delimiter and missing values handling\n","df = pd.read_csv('data.csv', delimiter=';', na_values=['NA', 'N/A'])\n","df\n","```"]},{"cell_type":"markdown","id":"6dedf243","metadata":{"papermill":{"duration":0.05203,"end_time":"2023-08-27T14:37:55.13038","exception":false,"start_time":"2023-08-27T14:37:55.07835","status":"completed"},"tags":[]},"source":["\n","\n","## 7.2 Loading Data from an Excel File\n","#### In this subchapter, we will explore how to load data from an Excel file into a DataFrame using Pandas. We will cover different options for importing specific sheets, selecting columns, and skipping rows.\n"]},{"cell_type":"markdown","id":"27e5513e","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.075334Z","iopub.status.idle":"2023-07-03T17:23:01.076154Z","shell.execute_reply":"2023-07-03T17:23:01.075903Z","shell.execute_reply.started":"2023-07-03T17:23:01.07588Z"},"papermill":{"duration":0.054295,"end_time":"2023-08-27T14:37:55.237135","exception":false,"start_time":"2023-08-27T14:37:55.18284","status":"completed"},"tags":[]},"source":["```python\n","# Example: Loading data from an Excel file\n","import pandas as pd\n","\n","# Load data from an Excel file into a DataFrame\n","df = pd.read_excel('data.xlsx')\n","\n","# Example: Importing specific sheets and selecting columns\n","import pandas as pd\n","\n","# Load data from specific sheets and select columns\n","df_sheet1 = pd.read_excel('data.xlsx', sheet_name='Sheet1', usecols=['A', 'B'])\n","df_sheet2 = pd.read_excel('data.xlsx', sheet_name='Sheet2', usecols=['C', 'D'])\n","```"]},{"cell_type":"markdown","id":"97f6cfda","metadata":{"papermill":{"duration":0.053166,"end_time":"2023-08-27T14:37:55.343211","exception":false,"start_time":"2023-08-27T14:37:55.290045","status":"completed"},"tags":[]},"source":["\n","\n","## 7.3 Loading Data from Other Formats\n","#### In this subchapter, we will discuss other common formats for loading data into a DataFrame using Pandas. We will cover loading data from JSON, SQL databases, and other file formats.\n","\n"]},{"cell_type":"markdown","id":"0926c6bb","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.077613Z","iopub.status.idle":"2023-07-03T17:23:01.078457Z","shell.execute_reply":"2023-07-03T17:23:01.078215Z","shell.execute_reply.started":"2023-07-03T17:23:01.07819Z"},"papermill":{"duration":0.057163,"end_time":"2023-08-27T14:37:55.454857","exception":false,"start_time":"2023-08-27T14:37:55.397694","status":"completed"},"tags":[]},"source":["```python\n","# Example: Loading data from a JSON file\n","import pandas as pd\n","\n","# Load data from a JSON file into a DataFrame\n","df = pd.read_json('data.json')\n","\n","# Example: Loading data from a SQL database\n","import pandas as pd\n","import sqlite3\n","\n","# Connect to a SQLite database\n","conn = sqlite3.connect('database.db')\n","\n","# Load data from a SQL query into a DataFrame\n","df = pd.read_sql_query('SELECT * FROM table', conn)\n","```\n"]},{"cell_type":"markdown","id":"b94c9643","metadata":{"papermill":{"duration":0.052463,"end_time":"2023-08-27T14:37:55.564678","exception":false,"start_time":"2023-08-27T14:37:55.512215","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>8</font></h1>"]},{"cell_type":"markdown","id":"73c59315","metadata":{"papermill":{"duration":0.052796,"end_time":"2023-08-27T14:37:55.671835","exception":false,"start_time":"2023-08-27T14:37:55.619039","status":"completed"},"tags":[]},"source":["# Chapter 8: Inspecting DataFrames: head(), tail(), shape, info(), describe()\n","## 8.1 The head() and tail() Methods\n","#### In this subchapter, we will explore two useful methods for inspecting DataFrames: head() and tail(). These methods allow us to view a sample of the DataFrame's rows, providing a quick overview of the data."]},{"cell_type":"markdown","id":"0eeef2d6","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.080059Z","iopub.status.idle":"2023-07-03T17:23:01.080911Z","shell.execute_reply":"2023-07-03T17:23:01.080675Z","shell.execute_reply.started":"2023-07-03T17:23:01.080652Z"},"papermill":{"duration":0.053035,"end_time":"2023-08-27T14:37:55.777222","exception":false,"start_time":"2023-08-27T14:37:55.724187","status":"completed"},"tags":[]},"source":["```python\n","# Example: Using head() to display the first few rows of a DataFrame\n","import pandas as pd\n","\n","# Display the first 5 rows of the DataFrame\n","df.head()\n","\n","# Example: Using tail() to display the last few rows of a DataFrame\n","import pandas as pd\n","\n","# Display the last 5 rows of the DataFrame\n","df.tail()\n","```\n"]},{"cell_type":"markdown","id":"04d50874","metadata":{"papermill":{"duration":0.052807,"end_time":"2023-08-27T14:37:55.883053","exception":false,"start_time":"2023-08-27T14:37:55.830246","status":"completed"},"tags":[]},"source":["## 8.2 The shape Attribute\n","#### The shape attribute of a DataFrame provides information about its dimensions, i.e., the number of rows and columns."]},{"cell_type":"markdown","id":"b85769a0","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.082399Z","iopub.status.idle":"2023-07-03T17:23:01.083411Z","shell.execute_reply":"2023-07-03T17:23:01.083167Z","shell.execute_reply.started":"2023-07-03T17:23:01.083144Z"},"papermill":{"duration":0.052027,"end_time":"2023-08-27T14:37:55.987221","exception":false,"start_time":"2023-08-27T14:37:55.935194","status":"completed"},"tags":[]},"source":["```python\n","# Example: Using shape to get the dimensions of a DataFrame\n","import pandas as pd\n","\n","# Get the dimensions of the DataFrame\n","rows, columns = df.shape\n","print(f\"The DataFrame has {rows} rows and {columns} columns.\")\n","```"]},{"cell_type":"markdown","id":"5e330b30","metadata":{"papermill":{"duration":0.052803,"end_time":"2023-08-27T14:37:56.092712","exception":false,"start_time":"2023-08-27T14:37:56.039909","status":"completed"},"tags":[]},"source":["## 8.3 The info() Method\n","#### The info() method provides a summary of the DataFrame's structure, including the number of non-null values and the data types of each column."]},{"cell_type":"markdown","id":"5ad59ce6","metadata":{"execution":{"iopub.execute_input":"2023-07-03T17:28:05.300825Z","iopub.status.busy":"2023-07-03T17:28:05.300318Z","iopub.status.idle":"2023-07-03T17:28:05.308941Z","shell.execute_reply":"2023-07-03T17:28:05.307143Z","shell.execute_reply.started":"2023-07-03T17:28:05.300792Z"},"papermill":{"duration":0.052183,"end_time":"2023-08-27T14:37:56.198033","exception":false,"start_time":"2023-08-27T14:37:56.14585","status":"completed"},"tags":[]},"source":["```python\n","# Example: Using info() to get the summary of a DataFrame\n","import pandas as pd\n","\n","# Display the summary of the DataFrame\n","df.info()\n","```\n"]},{"cell_type":"markdown","id":"a800bc48","metadata":{"papermill":{"duration":0.052686,"end_time":"2023-08-27T14:37:56.30329","exception":false,"start_time":"2023-08-27T14:37:56.250604","status":"completed"},"tags":[]},"source":["## 8.4 The describe() Method\n","#### The describe() method generates descriptive statistics of the DataFrame's numerical columns, such as count, mean, standard deviation, minimum, and maximum values."]},{"cell_type":"markdown","id":"20e20eff","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.087245Z","iopub.status.idle":"2023-07-03T17:23:01.088059Z","shell.execute_reply":"2023-07-03T17:23:01.087828Z","shell.execute_reply.started":"2023-07-03T17:23:01.087805Z"},"papermill":{"duration":0.054147,"end_time":"2023-08-27T14:37:56.410507","exception":false,"start_time":"2023-08-27T14:37:56.35636","status":"completed"},"tags":[]},"source":["```python\n","# Example: Using describe() to get descriptive statistics of a DataFrame\n","import pandas as pd\n","\n","# Generate descriptive statistics of the DataFrame\n","df.describe()\n","```"]},{"cell_type":"markdown","id":"2e50a219","metadata":{"papermill":{"duration":0.054722,"end_time":"2023-08-27T14:37:56.517719","exception":false,"start_time":"2023-08-27T14:37:56.462997","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>9</font></h1>\n"]},{"cell_type":"markdown","id":"be85bc29","metadata":{"papermill":{"duration":0.059794,"end_time":"2023-08-27T14:37:56.629468","exception":false,"start_time":"2023-08-27T14:37:56.569674","status":"completed"},"tags":[]},"source":["# Chapter 9: Indexing and Selecting Data: loc, iloc, column selection\n","## 9.1 Indexing with loc\n","#### The loc indexer is used for label-based indexing in Pandas. It allows you to select specific rows and columns using their labels."]},{"cell_type":"markdown","id":"9372c2fd","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.089611Z","iopub.status.idle":"2023-07-03T17:23:01.090462Z","shell.execute_reply":"2023-07-03T17:23:01.090223Z","shell.execute_reply.started":"2023-07-03T17:23:01.0902Z"},"papermill":{"duration":0.052099,"end_time":"2023-08-27T14:37:56.734293","exception":false,"start_time":"2023-08-27T14:37:56.682194","status":"completed"},"tags":[]},"source":["```python\n","# Example: Using loc to select rows and columns by labels\n","import pandas as pd\n","\n","# Select a single row by label\n","row = df.loc['label']\n","\n","# Select multiple rows by labels\n","rows = df.loc[['label1', 'label2']]\n","\n","# Select rows and columns by labels\n","subset = df.loc[['label1', 'label2'], ['column1', 'column2']]\n","```"]},{"cell_type":"markdown","id":"87b2843f","metadata":{"papermill":{"duration":0.051964,"end_time":"2023-08-27T14:37:56.838143","exception":false,"start_time":"2023-08-27T14:37:56.786179","status":"completed"},"tags":[]},"source":["# 9.2 Indexing with iloc\n","#### The iloc indexer is used for integer-based indexing in Pandas. It allows you to select specific rows and columns using their integer positions."]},{"cell_type":"markdown","id":"2b9b3910","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.091943Z","iopub.status.idle":"2023-07-03T17:23:01.092778Z","shell.execute_reply":"2023-07-03T17:23:01.092546Z","shell.execute_reply.started":"2023-07-03T17:23:01.092522Z"},"papermill":{"duration":0.052094,"end_time":"2023-08-27T14:37:56.942052","exception":false,"start_time":"2023-08-27T14:37:56.889958","status":"completed"},"tags":[]},"source":["```python\n","# Example: Using iloc to select rows and columns by integer positions\n","import pandas as pd\n","\n","# Select a single row by integer position\n","row = df.iloc[0]\n","\n","# Select multiple rows by integer positions\n","rows = df.iloc[[0, 1, 2]]\n","\n","# Select rows and columns by integer positions\n","subset = df.iloc[[0, 1, 2], [0, 1, 2]]\n","```\n"]},{"cell_type":"markdown","id":"841b6d32","metadata":{"papermill":{"duration":0.052071,"end_time":"2023-08-27T14:37:57.046087","exception":false,"start_time":"2023-08-27T14:37:56.994016","status":"completed"},"tags":[]},"source":["## 9.3 Selecting Columns\n","#### You can select specific columns from a DataFrame using indexing notation or the loc and iloc indexers."]},{"cell_type":"markdown","id":"e510dc4d","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.094276Z","iopub.status.idle":"2023-07-03T17:23:01.095173Z","shell.execute_reply":"2023-07-03T17:23:01.094905Z","shell.execute_reply.started":"2023-07-03T17:23:01.09488Z"},"papermill":{"duration":0.052068,"end_time":"2023-08-27T14:37:57.150256","exception":false,"start_time":"2023-08-27T14:37:57.098188","status":"completed"},"tags":[]},"source":["```python\n","# Example: Selecting columns from a DataFrame\n","import pandas as pd\n","\n","# Select a single column by label\n","column = df['column_name']\n","\n","# Select multiple columns by labels\n","columns = df[['column1', 'column2']]\n","\n","# Select a single column by integer position using iloc\n","column = df.iloc[:, 0]\n","\n","# Select multiple columns by integer positions using iloc\n","columns = df.iloc[:, [0, 1, 2]]\n","```"]},{"cell_type":"markdown","id":"bb810f49","metadata":{"papermill":{"duration":0.052078,"end_time":"2023-08-27T14:37:57.254149","exception":false,"start_time":"2023-08-27T14:37:57.202071","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>10</font></h1>"]},{"cell_type":"markdown","id":"62b3e10b","metadata":{"papermill":{"duration":0.052404,"end_time":"2023-08-27T14:37:57.358707","exception":false,"start_time":"2023-08-27T14:37:57.306303","status":"completed"},"tags":[]},"source":["# Chapter 10: Conditional Selection and Filtering Data\n","## 10.1 Conditional Selection with Comparison Operators\n","#### In this subchapter, we will learn how to use comparison operators to perform conditional selection on DataFrames. This allows us to filter rows based on specific conditions."]},{"cell_type":"markdown","id":"70767e6b","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.096662Z","iopub.status.idle":"2023-07-03T17:23:01.097504Z","shell.execute_reply":"2023-07-03T17:23:01.097263Z","shell.execute_reply.started":"2023-07-03T17:23:01.09724Z"},"papermill":{"duration":0.052275,"end_time":"2023-08-27T14:37:57.463683","exception":false,"start_time":"2023-08-27T14:37:57.411408","status":"completed"},"tags":[]},"source":["```python\n","# Example: Conditional selection using comparison operators\n","import pandas as pd\n","\n","# Select rows where a column meets a specific condition\n","subset = df[df['column'] > 10]\n","\n","# Select rows based on multiple conditions\n","subset = df[(df['column1'] > 10) & (df['column2'] < 20)]\n","```"]},{"cell_type":"markdown","id":"1e9c6a37","metadata":{"papermill":{"duration":0.052222,"end_time":"2023-08-27T14:37:57.575705","exception":false,"start_time":"2023-08-27T14:37:57.523483","status":"completed"},"tags":[]},"source":["## 10.2 Conditional Selection with isin() Method\n","### The isin() method allows you to perform conditional selection based on multiple values in a column."]},{"cell_type":"markdown","id":"830eec9a","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.098983Z","iopub.status.idle":"2023-07-03T17:23:01.099905Z","shell.execute_reply":"2023-07-03T17:23:01.099638Z","shell.execute_reply.started":"2023-07-03T17:23:01.099613Z"},"papermill":{"duration":0.05334,"end_time":"2023-08-27T14:37:57.681421","exception":false,"start_time":"2023-08-27T14:37:57.628081","status":"completed"},"tags":[]},"source":["```python\n","# Example: Conditional selection using isin() method\n","import pandas as pd\n","\n","# Select rows where a column's value is in a list of values\n","subset = df[df['column'].isin(['value1', 'value2', 'value3'])]\n","```"]},{"cell_type":"markdown","id":"abb64b5d","metadata":{"papermill":{"duration":0.05174,"end_time":"2023-08-27T14:37:57.785378","exception":false,"start_time":"2023-08-27T14:37:57.733638","status":"completed"},"tags":[]},"source":["## 10.3 Conditional Selection with String Methods\n","#### Pandas provides various string methods that can be used for conditional selection based on string values."]},{"cell_type":"markdown","id":"8f133897","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.101402Z","iopub.status.idle":"2023-07-03T17:23:01.102233Z","shell.execute_reply":"2023-07-03T17:23:01.101985Z","shell.execute_reply.started":"2023-07-03T17:23:01.101962Z"},"papermill":{"duration":0.052042,"end_time":"2023-08-27T14:37:57.889219","exception":false,"start_time":"2023-08-27T14:37:57.837177","status":"completed"},"tags":[]},"source":["```python\n","# Example: Conditional selection using string methods\n","import pandas as pd\n","\n","# Select rows where a column's value contains a specific substring\n","subset = df[df['column'].str.contains('substring')]\n","\n","# Select rows where a column's value starts with a specific string\n","subset = df[df['column'].str.startswith('string')]\n","```\n"]},{"cell_type":"markdown","id":"012cfb6a","metadata":{"papermill":{"duration":0.05201,"end_time":"2023-08-27T14:37:57.993187","exception":false,"start_time":"2023-08-27T14:37:57.941177","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>11</font></h1>"]},{"cell_type":"markdown","id":"1fef3b11","metadata":{"papermill":{"duration":0.052281,"end_time":"2023-08-27T14:37:58.097917","exception":false,"start_time":"2023-08-27T14:37:58.045636","status":"completed"},"tags":[]},"source":["# Chapter 11: Sorting Data in a DataFrame\n","## 11.1 Sorting by Columns\n","#### In this subchapter, we will explore how to sort a DataFrame based on one or more columns. We can specify the sorting order as ascending or descending."]},{"cell_type":"markdown","id":"55747315","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.103695Z","iopub.status.idle":"2023-07-03T17:23:01.104544Z","shell.execute_reply":"2023-07-03T17:23:01.104299Z","shell.execute_reply.started":"2023-07-03T17:23:01.104275Z"},"papermill":{"duration":0.052049,"end_time":"2023-08-27T14:37:58.205156","exception":false,"start_time":"2023-08-27T14:37:58.153107","status":"completed"},"tags":[]},"source":["```python\n","# Example: Sorting a DataFrame by one column\n","import pandas as pd\n","\n","# Sort the DataFrame by a single column in ascending order\n","sorted_df = df.sort_values(by='column_name')\n","\n","# Sort the DataFrame by a single column in descending order\n","sorted_df = df.sort_values(by='column_name', ascending=False)\n","```\n"]},{"cell_type":"markdown","id":"309913d3","metadata":{"papermill":{"duration":0.051929,"end_time":"2023-08-27T14:37:58.309644","exception":false,"start_time":"2023-08-27T14:37:58.257715","status":"completed"},"tags":[]},"source":["## 11.2 Sorting by Multiple Columns\n","#### We can also sort a DataFrame based on multiple columns. The sorting order can be specified independently for each column."]},{"cell_type":"markdown","id":"b63a60e2","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.106014Z","iopub.status.idle":"2023-07-03T17:23:01.106838Z","shell.execute_reply":"2023-07-03T17:23:01.106603Z","shell.execute_reply.started":"2023-07-03T17:23:01.10658Z"},"papermill":{"duration":0.058818,"end_time":"2023-08-27T14:37:58.421115","exception":false,"start_time":"2023-08-27T14:37:58.362297","status":"completed"},"tags":[]},"source":["```python\n","# Example: Sorting a DataFrame by multiple columns\n","import pandas as pd\n","\n","# Sort the DataFrame by multiple columns in ascending order\n","sorted_df = df.sort_values(by=['column1', 'column2'])\n","\n","# Sort the DataFrame by multiple columns with different sorting orders\n","sorted_df = df.sort_values(by=['column1', 'column2'], ascending=[True, False])\n","```\n"]},{"cell_type":"markdown","id":"ce08616a","metadata":{"papermill":{"duration":0.058213,"end_time":"2023-08-27T14:37:58.531653","exception":false,"start_time":"2023-08-27T14:37:58.47344","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>12</font></h1>\n"]},{"cell_type":"markdown","id":"f11a984c","metadata":{"papermill":{"duration":0.052315,"end_time":"2023-08-27T14:37:58.671953","exception":false,"start_time":"2023-08-27T14:37:58.619638","status":"completed"},"tags":[]},"source":["# Chapter 12: Adding, Updating, and Deleting Columns\n","## 12.1 Adding Columns\n","#### In this subchapter, we will learn how to add new columns to a DataFrame. We can assign constant values or perform computations based on existing columns."]},{"cell_type":"markdown","id":"2871a1d4","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.108309Z","iopub.status.idle":"2023-07-03T17:23:01.109217Z","shell.execute_reply":"2023-07-03T17:23:01.108944Z","shell.execute_reply.started":"2023-07-03T17:23:01.108918Z"},"papermill":{"duration":0.05223,"end_time":"2023-08-27T14:37:58.7786","exception":false,"start_time":"2023-08-27T14:37:58.72637","status":"completed"},"tags":[]},"source":["```python\n","# Example: Adding a new column to a DataFrame\n","import pandas as pd\n","\n","# Add a new column with a constant value\n","df['new_column'] = 10\n","\n","# Add a new column based on computations with existing columns\n","df['total'] = df['column1'] + df['column2']\n","\n","```"]},{"cell_type":"markdown","id":"83afb196","metadata":{"papermill":{"duration":0.052138,"end_time":"2023-08-27T14:37:58.882701","exception":false,"start_time":"2023-08-27T14:37:58.830563","status":"completed"},"tags":[]},"source":["## 12.2 Updating Columns\n","#### We can update the values of existing columns in a DataFrame by assigning new values or applying functions to the column values."]},{"cell_type":"markdown","id":"288ac42c","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.110821Z","iopub.status.idle":"2023-07-03T17:23:01.111724Z","shell.execute_reply":"2023-07-03T17:23:01.111497Z","shell.execute_reply.started":"2023-07-03T17:23:01.111473Z"},"papermill":{"duration":0.052614,"end_time":"2023-08-27T14:37:58.987321","exception":false,"start_time":"2023-08-27T14:37:58.934707","status":"completed"},"tags":[]},"source":["```python\n","# Example: Updating column values in a DataFrame\n","import pandas as pd\n","\n","# Update column values based on a condition\n","df.loc[df['column'] > 10, 'column'] = 20\n","\n","# Update column values using a function\n","df['column'] = df['column'].apply(lambda x: x * 2)\n","```"]},{"cell_type":"markdown","id":"1f51a793","metadata":{"papermill":{"duration":0.053343,"end_time":"2023-08-27T14:37:59.092696","exception":false,"start_time":"2023-08-27T14:37:59.039353","status":"completed"},"tags":[]},"source":["## 12.3 Deleting Columns\n","#### In this subchapter, we will explore how to delete columns from a DataFrame. We can remove columns using the drop() method or the del statement."]},{"cell_type":"markdown","id":"65965fab","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.113186Z","iopub.status.idle":"2023-07-03T17:23:01.114003Z","shell.execute_reply":"2023-07-03T17:23:01.113771Z","shell.execute_reply.started":"2023-07-03T17:23:01.113748Z"},"papermill":{"duration":0.051994,"end_time":"2023-08-27T14:37:59.196737","exception":false,"start_time":"2023-08-27T14:37:59.144743","status":"completed"},"tags":[]},"source":["```python\n","# Example: Deleting columns from a DataFrame\n","import pandas as pd\n","\n","# Remove columns using the drop() method\n","df.drop(['column1', 'column2'], axis=1, inplace=True)\n","\n","# Remove columns using the del statement\n","del df['column']\n","```\n"]},{"cell_type":"markdown","id":"2c78d767","metadata":{"papermill":{"duration":0.051963,"end_time":"2023-08-27T14:37:59.301169","exception":false,"start_time":"2023-08-27T14:37:59.249206","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>13</font></h1>\n"]},{"cell_type":"markdown","id":"f1a29a5d","metadata":{"papermill":{"duration":0.051706,"end_time":"2023-08-27T14:37:59.405378","exception":false,"start_time":"2023-08-27T14:37:59.353672","status":"completed"},"tags":[]},"source":["# Chapter 13: Handling Missing Data: isnull(), dropna(), fillna()\n","## 13.1 Checking for Missing Data\n","#### In this subchapter, we will learn how to identify missing or null values in a DataFrame using the isnull() method. This will help us in understanding the presence of missing data in our dataset."]},{"cell_type":"markdown","id":"1e668d55","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.115521Z","iopub.status.idle":"2023-07-03T17:23:01.11644Z","shell.execute_reply":"2023-07-03T17:23:01.116187Z","shell.execute_reply.started":"2023-07-03T17:23:01.116162Z"},"papermill":{"duration":0.054213,"end_time":"2023-08-27T14:37:59.511474","exception":false,"start_time":"2023-08-27T14:37:59.457261","status":"completed"},"tags":[]},"source":["```python\n","# Example: Checking for missing data in a DataFrame\n","import pandas as pd\n","\n","# Check for missing values in the entire DataFrame\n","missing_values = df.isnull().sum()\n","\n","# Check for missing values in a specific column\n","missing_values = df['column'].isnull().sum()\n","```\n"]},{"cell_type":"markdown","id":"5924211b","metadata":{"papermill":{"duration":0.052761,"end_time":"2023-08-27T14:37:59.621605","exception":false,"start_time":"2023-08-27T14:37:59.568844","status":"completed"},"tags":[]},"source":["## 13.2 Dropping Rows or Columns with Missing Data\n","#### We can remove rows or columns that contain missing data using the dropna() method. This allows us to clean our dataset and ensure data integrity.\n","\n"]},{"cell_type":"markdown","id":"2e330571","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.117883Z","iopub.status.idle":"2023-07-03T17:23:01.118769Z","shell.execute_reply":"2023-07-03T17:23:01.118505Z","shell.execute_reply.started":"2023-07-03T17:23:01.118482Z"},"papermill":{"duration":0.052349,"end_time":"2023-08-27T14:37:59.726143","exception":false,"start_time":"2023-08-27T14:37:59.673794","status":"completed"},"tags":[]},"source":["```python\n","# Example: Dropping rows or columns with missing data\n","import pandas as pd\n","\n","# Drop rows with missing values\n","df.dropna(axis=0, inplace=True)\n","\n","# Drop columns with missing values\n","df.dropna(axis=1, inplace=True)\n","```\n"]},{"cell_type":"markdown","id":"3d391420","metadata":{"papermill":{"duration":0.052163,"end_time":"2023-08-27T14:37:59.830298","exception":false,"start_time":"2023-08-27T14:37:59.778135","status":"completed"},"tags":[]},"source":["## 13.3 Filling Missing Data\n","#### In this subchapter, we will explore how to fill missing values in a DataFrame using the fillna() method. This allows us to impute or replace missing data with appropriate values."]},{"cell_type":"markdown","id":"8a076ab9","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.120342Z","iopub.status.idle":"2023-07-03T17:23:01.121179Z","shell.execute_reply":"2023-07-03T17:23:01.120929Z","shell.execute_reply.started":"2023-07-03T17:23:01.120906Z"},"papermill":{"duration":0.051892,"end_time":"2023-08-27T14:37:59.93461","exception":false,"start_time":"2023-08-27T14:37:59.882718","status":"completed"},"tags":[]},"source":["```python\n","# Example: Filling missing data in a DataFrame\n","import pandas as pd\n","\n","# Fill missing values with a constant value\n","df.fillna(0, inplace=True)\n","\n","# Fill missing values with the mean of the column\n","df.fillna(df.mean(), inplace=True)\n","```"]},{"cell_type":"markdown","id":"18725a18","metadata":{"papermill":{"duration":0.053223,"end_time":"2023-08-27T14:38:00.040328","exception":false,"start_time":"2023-08-27T14:37:59.987105","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>14</font></h1>\n"]},{"cell_type":"markdown","id":"30481c44","metadata":{"papermill":{"duration":0.052969,"end_time":"2023-08-27T14:38:00.145962","exception":false,"start_time":"2023-08-27T14:38:00.092993","status":"completed"},"tags":[]},"source":["# Chapter 14: Removing Duplicate Rows: duplicated(), drop_duplicates()\n","## 14.1 Checking for Duplicate Rows\n","#### In this subchapter, we will learn how to identify duplicate rows in a DataFrame using the duplicated() method. This will help us in detecting and handling duplicate data."]},{"cell_type":"markdown","id":"89422a3b","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.122626Z","iopub.status.idle":"2023-07-03T17:23:01.123468Z","shell.execute_reply":"2023-07-03T17:23:01.12322Z","shell.execute_reply.started":"2023-07-03T17:23:01.123196Z"},"papermill":{"duration":0.052639,"end_time":"2023-08-27T14:38:00.252483","exception":false,"start_time":"2023-08-27T14:38:00.199844","status":"completed"},"tags":[]},"source":["```python\n","# Example: Checking for duplicate rows in a DataFrame\n","import pandas as pd\n","\n","# Check for duplicate rows in the entire DataFrame\n","duplicate_rows = df.duplicated()\n","\n","# Check for duplicate rows in specific columns\n","duplicate_rows = df.duplicated(subset=['column1', 'column2'])\n","```\n"]},{"cell_type":"markdown","id":"699a30c6","metadata":{"papermill":{"duration":0.053749,"end_time":"2023-08-27T14:38:00.360248","exception":false,"start_time":"2023-08-27T14:38:00.306499","status":"completed"},"tags":[]},"source":["## 14.2 Dropping Duplicate Rows\n","#### We can remove duplicate rows from a DataFrame using the drop_duplicates() method. This ensures that our dataset only contains unique rows."]},{"cell_type":"markdown","id":"c073b9a0","metadata":{"execution":{"iopub.status.busy":"2023-07-03T17:23:01.124928Z","iopub.status.idle":"2023-07-03T17:23:01.12577Z","shell.execute_reply":"2023-07-03T17:23:01.125534Z","shell.execute_reply.started":"2023-07-03T17:23:01.125511Z"},"papermill":{"duration":0.058574,"end_time":"2023-08-27T14:38:00.471838","exception":false,"start_time":"2023-08-27T14:38:00.413264","status":"completed"},"tags":[]},"source":["```python\n","# Example: Dropping duplicate rows from a DataFrame\n","import pandas as pd\n","\n","# Drop duplicate rows based on all columns\n","df.drop_duplicates(inplace=True)\n","\n","# Drop duplicate rows based on specific columns\n","df.drop_duplicates(subset=['column1', 'column2'], inplace=True)\n","\n","```"]},{"cell_type":"markdown","id":"87266646","metadata":{"papermill":{"duration":0.052453,"end_time":"2023-08-27T14:38:00.580221","exception":false,"start_time":"2023-08-27T14:38:00.527768","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>15</font></h1>\n"]},{"cell_type":"markdown","id":"69f35298","metadata":{"papermill":{"duration":0.052628,"end_time":"2023-08-27T14:38:00.686123","exception":false,"start_time":"2023-08-27T14:38:00.633495","status":"completed"},"tags":[]},"source":["# Chapter 15: Data Type Conversion and Handling\n","## 15.1 Converting Data Types\n","#### In this subchapter, we will explore how to convert the data types of columns in a DataFrame. This can be useful when the original data types need to be adjusted for specific operations or analysis."]},{"cell_type":"markdown","id":"b95402c3","metadata":{"papermill":{"duration":0.051889,"end_time":"2023-08-27T14:38:00.791462","exception":false,"start_time":"2023-08-27T14:38:00.739573","status":"completed"},"tags":[]},"source":["```python\n","# Example: Converting data types in a DataFrame\n","import pandas as pd\n","\n","# Convert a column to a different data type\n","df['column'] = df['column'].astype('new_data_type')\n","\n","# Convert multiple columns to different data types\n","df = df.astype({'column1': 'new_data_type1', 'column2': 'new_data_type2'})\n","```\n"]},{"cell_type":"markdown","id":"350c8da1","metadata":{"papermill":{"duration":0.051751,"end_time":"2023-08-27T14:38:00.895417","exception":false,"start_time":"2023-08-27T14:38:00.843666","status":"completed"},"tags":[]},"source":["## 15.2 Handling Categorical Data\n","#### Categorical data can be represented as strings or numerical codes. In this subchapter, we will learn how to handle categorical data in a DataFrame, including converting strings to categories and performing operations on categorical columns.\n"]},{"cell_type":"markdown","id":"5c3fc41b","metadata":{"papermill":{"duration":0.051998,"end_time":"2023-08-27T14:38:00.999454","exception":false,"start_time":"2023-08-27T14:38:00.947456","status":"completed"},"tags":[]},"source":["```python\n","# Example: Handling categorical data in a DataFrame\n","import pandas as pd\n","\n","# Convert a column to categorical data type\n","df['column'] = df['column'].astype('category')\n","\n","# Perform operations on categorical columns\n","df['column'] = df['column'].cat.codes\n","```"]},{"cell_type":"markdown","id":"277436bf","metadata":{"papermill":{"duration":0.052572,"end_time":"2023-08-27T14:38:01.104252","exception":false,"start_time":"2023-08-27T14:38:01.05168","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>16</font></h1>\n"]},{"cell_type":"markdown","id":"fe98f50c","metadata":{"papermill":{"duration":0.0525,"end_time":"2023-08-27T14:38:01.209919","exception":false,"start_time":"2023-08-27T14:38:01.157419","status":"completed"},"tags":[]},"source":["# Chapter 16: Dealing with Outliers\n","## 16.1 Identifying Outliers\n","#### In this subchapter, we will learn how to identify outliers in a DataFrame. Outliers are extreme values that deviate significantly from the majority of the data points."]},{"cell_type":"markdown","id":"dfd89368","metadata":{"papermill":{"duration":0.052226,"end_time":"2023-08-27T14:38:01.314681","exception":false,"start_time":"2023-08-27T14:38:01.262455","status":"completed"},"tags":[]},"source":["```python\n","# Example: Identifying outliers in a DataFrame\n","import pandas as pd\n","\n","# Calculate the z-score for each data point\n","z_scores = (df - df.mean()) / df.std()\n","\n","# Identify outliers based on a threshold\n","outliers = df[z_scores > threshold]\n","````\n"]},{"cell_type":"markdown","id":"aa53d241","metadata":{"papermill":{"duration":0.05217,"end_time":"2023-08-27T14:38:01.419148","exception":false,"start_time":"2023-08-27T14:38:01.366978","status":"completed"},"tags":[]},"source":["## 16.2 Handling Outliers\n","#### Once outliers are identified, we can choose to handle them in different ways. Some common approaches include removing the outliers, capping/extending the outliers, or transforming the data."]},{"cell_type":"markdown","id":"d1cfd882","metadata":{"execution":{"iopub.execute_input":"2023-07-03T17:43:12.83392Z","iopub.status.busy":"2023-07-03T17:43:12.833524Z","iopub.status.idle":"2023-07-03T17:43:12.854945Z","shell.execute_reply":"2023-07-03T17:43:12.853006Z","shell.execute_reply.started":"2023-07-03T17:43:12.833888Z"},"papermill":{"duration":0.053489,"end_time":"2023-08-27T14:38:01.524605","exception":false,"start_time":"2023-08-27T14:38:01.471116","status":"completed"},"tags":[]},"source":["```python\n","# Example: Handling outliers in a DataFrame\n","import pandas as pd\n","\n","# Remove outliers from the DataFrame\n","df = df[~(z_scores > threshold).any(axis=1)]\n","\n","# Cap or extend the outliers to a specific range\n","df[z_scores > threshold] = cap_value\n","\n","# Transform the data using a suitable transformation\n","df['column'] = np.log(df['column'])\n","```"]},{"cell_type":"markdown","id":"0e8745f5","metadata":{"papermill":{"duration":0.05191,"end_time":"2023-08-27T14:38:01.675121","exception":false,"start_time":"2023-08-27T14:38:01.623211","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>17</font></h1>\n"]},{"cell_type":"markdown","id":"6468bef3","metadata":{"papermill":{"duration":0.051993,"end_time":"2023-08-27T14:38:01.780316","exception":false,"start_time":"2023-08-27T14:38:01.728323","status":"completed"},"tags":[]},"source":["# Chapter 17: Concatenating and Merging DataFrames\n","## 17.1 Concatenating DataFrames\n","#### In this subchapter, we will explore how to concatenate multiple DataFrames along different axes (rows or columns). This allows us to combine data from multiple sources into a single DataFrame."]},{"cell_type":"markdown","id":"96bde7c6","metadata":{"papermill":{"duration":0.051675,"end_time":"2023-08-27T14:38:01.884397","exception":false,"start_time":"2023-08-27T14:38:01.832722","status":"completed"},"tags":[]},"source":["```python\n","# Example: Concatenating DataFrames along rows\n","import pandas as pd\n","\n","# Concatenate DataFrames along rows\n","concatenated_df = pd.concat([df1, df2, df3], axis=0)\n","\n","# Example: Concatenating DataFrames along columns\n","import pandas as pd\n","\n","# Concatenate DataFrames along columns\n","concatenated_df = pd.concat([df1, df2, df3], axis=1)\n","```\n"]},{"cell_type":"markdown","id":"d58338f3","metadata":{"papermill":{"duration":0.053152,"end_time":"2023-08-27T14:38:01.990197","exception":false,"start_time":"2023-08-27T14:38:01.937045","status":"completed"},"tags":[]},"source":["## 17.2 Merging DataFrames\n","#### We can merge DataFrames based on common columns or indices using the merge() function. This allows us to combine data from multiple DataFrames into a single DataFrame based on specified merge keys."]},{"cell_type":"markdown","id":"35aff1f5","metadata":{"papermill":{"duration":0.055996,"end_time":"2023-08-27T14:38:02.100904","exception":false,"start_time":"2023-08-27T14:38:02.044908","status":"completed"},"tags":[]},"source":["```python\n","# Example: Merging DataFrames based on common columns\n","import pandas as pd\n","\n","# Merge DataFrames based on common columns\n","merged_df = pd.merge(df1, df2, on='common_column')\n","\n","# Example: Merging DataFrames based on common indices\n","import pandas as pd\n","\n","# Merge DataFrames based on common indices\n","merged_df = pd.merge(df1, df2, left_index=True, right_index=True)\n","```"]},{"cell_type":"markdown","id":"e904583f","metadata":{"papermill":{"duration":0.055074,"end_time":"2023-08-27T14:38:02.210737","exception":false,"start_time":"2023-08-27T14:38:02.155663","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>18</font></h1>\n"]},{"cell_type":"markdown","id":"e1a8d2f0","metadata":{"papermill":{"duration":0.055778,"end_time":"2023-08-27T14:38:02.322735","exception":false,"start_time":"2023-08-27T14:38:02.266957","status":"completed"},"tags":[]},"source":["## Chapter 18: Reshaping Data: Pivot, Stack, Unstack\n","## 18.1 Reshaping with Pivot\n","#### In this subchapter, we will learn how to reshape data using the pivot() function. Pivot allows us to convert data from a long format to a wide format, reorganizing the data based on specified columns.\n"]},{"cell_type":"markdown","id":"15c02719","metadata":{"papermill":{"duration":0.052974,"end_time":"2023-08-27T14:38:02.432021","exception":false,"start_time":"2023-08-27T14:38:02.379047","status":"completed"},"tags":[]},"source":["```python\n","# Example: Reshaping data using pivot\n","import pandas as pd\n","\n","# Reshape the data from long to wide format\n","pivot_df = df.pivot(index='index_column', columns='columns_column', values='values_column')\n","```"]},{"cell_type":"markdown","id":"0b037aea","metadata":{"papermill":{"duration":0.05691,"end_time":"2023-08-27T14:38:02.542132","exception":false,"start_time":"2023-08-27T14:38:02.485222","status":"completed"},"tags":[]},"source":["## 18.2 Reshaping with Stack and Unstack\n","#### We can also reshape data using the stack() and unstack() methods. These methods transform data between wide and long formats, allowing us to manipulate hierarchical index levels."]},{"cell_type":"markdown","id":"11b69319","metadata":{"papermill":{"duration":0.051952,"end_time":"2023-08-27T14:38:02.64755","exception":false,"start_time":"2023-08-27T14:38:02.595598","status":"completed"},"tags":[]},"source":["```python\n","# Example: Reshaping data using stack and unstack\n","import pandas as pd\n","\n","# Reshape data from wide to long format using stack\n","stacked_df = df.stack()\n","\n","# Reshape data from long to wide format using unstack\n","unstacked_df = df.unstack().\n","```\n"]},{"cell_type":"markdown","id":"8f46e770","metadata":{"papermill":{"duration":0.057558,"end_time":"2023-08-27T14:38:02.75938","exception":false,"start_time":"2023-08-27T14:38:02.701822","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>19</font></h1>"]},{"cell_type":"markdown","id":"802b7fee","metadata":{"papermill":{"duration":0.057038,"end_time":"2023-08-27T14:38:02.874646","exception":false,"start_time":"2023-08-27T14:38:02.817608","status":"completed"},"tags":[]},"source":["# Chapter 19: Grouping and Aggregating Data: groupby(), aggregating functions\n","## 19.1 Grouping Data\n","#### In this subchapter, we will explore how to group data in a DataFrame using the groupby() function. Grouping allows us to split the data into groups based on specified criteria."]},{"cell_type":"markdown","id":"dc591baf","metadata":{"papermill":{"duration":0.054493,"end_time":"2023-08-27T14:38:02.98672","exception":false,"start_time":"2023-08-27T14:38:02.932227","status":"completed"},"tags":[]},"source":["```python\n","# Example: Grouping data in a DataFrame\n","import pandas as pd\n","\n","# Group the data based on a column\n","grouped_data = df.groupby('column')\n","\n","# Group the data based on multiple columns\n","grouped_data = df.groupby(['column1', 'column2'])\n","```\n"]},{"cell_type":"markdown","id":"0a668278","metadata":{"papermill":{"duration":0.059528,"end_time":"2023-08-27T14:38:03.103396","exception":false,"start_time":"2023-08-27T14:38:03.043868","status":"completed"},"tags":[]},"source":["## 19.2 Aggregating Data\n","#### Once the data is grouped, we can apply aggregating functions to obtain summary statistics for each group. Common aggregating functions include mean, sum, count, max, min, etc."]},{"cell_type":"markdown","id":"5207a5f7","metadata":{"papermill":{"duration":0.05797,"end_time":"2023-08-27T14:38:03.220697","exception":false,"start_time":"2023-08-27T14:38:03.162727","status":"completed"},"tags":[]},"source":["```python\n","# Example: Aggregating data in grouped DataFrame\n","import pandas as pd\n","\n","# Calculate the mean of each group\n","mean_values = grouped_data.mean()\n","\n","# Calculate the sum of each group\n","sum_values = grouped_data.sum()\n","\n","# Calculate the count of each group\n","count_values = grouped_data.size()\n","```\n"]},{"cell_type":"markdown","id":"4a540c4f","metadata":{"papermill":{"duration":0.052545,"end_time":"2023-08-27T14:38:03.328001","exception":false,"start_time":"2023-08-27T14:38:03.275456","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>20</font></h1>\n"]},{"cell_type":"markdown","id":"ce237856","metadata":{"papermill":{"duration":0.054026,"end_time":"2023-08-27T14:38:03.436194","exception":false,"start_time":"2023-08-27T14:38:03.382168","status":"completed"},"tags":[]},"source":["# Chapter 20: Applying Functions to Data: apply(), applymap()\n","## 20.1 Applying Functions to Series\n","#### In this subchapter, we will learn how to apply functions to Series objects using the apply() method. This allows us to perform custom operations on each element of a Series."]},{"cell_type":"markdown","id":"abeae8d0","metadata":{"papermill":{"duration":0.054611,"end_time":"2023-08-27T14:38:03.544465","exception":false,"start_time":"2023-08-27T14:38:03.489854","status":"completed"},"tags":[]},"source":["```python\n","# Example: Applying a function to a Series\n","import pandas as pd\n","\n","# Apply a function to each element of a Series\n","result = series.apply(function)\n","```\n"]},{"cell_type":"markdown","id":"3738979f","metadata":{"papermill":{"duration":0.051854,"end_time":"2023-08-27T14:38:03.649264","exception":false,"start_time":"2023-08-27T14:38:03.59741","status":"completed"},"tags":[]},"source":["## 20.2 Applying Functions to DataFrames\n","#### We can also apply functions to DataFrames using the apply() method. This allows us to apply custom operations to either rows or columns of a DataFrame."]},{"cell_type":"markdown","id":"1689b0b2","metadata":{"papermill":{"duration":0.052157,"end_time":"2023-08-27T14:38:03.753827","exception":false,"start_time":"2023-08-27T14:38:03.70167","status":"completed"},"tags":[]},"source":["```python\n","# Example: Applying a function to a DataFrame\n","import pandas as pd\n","\n","# Apply a function to each row of a DataFrame\n","result = df.apply(function, axis=1)\n","\n","# Apply a function to each column of a DataFrame\n","result = df.apply(function, axis=0)\n","```\n"]},{"cell_type":"markdown","id":"97c51eed","metadata":{"papermill":{"duration":0.052266,"end_time":"2023-08-27T14:38:03.859252","exception":false,"start_time":"2023-08-27T14:38:03.806986","status":"completed"},"tags":[]},"source":["## 20.3 Applying Element-wise Functions\n","#### To apply functions element-wise to a DataFrame, we can use the applymap() method. This allows us to perform custom operations on each element of a DataFrame."]},{"cell_type":"markdown","id":"172273e3","metadata":{"papermill":{"duration":0.051732,"end_time":"2023-08-27T14:38:03.963554","exception":false,"start_time":"2023-08-27T14:38:03.911822","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>21</font></h1>\n"]},{"cell_type":"markdown","id":"70375894","metadata":{"papermill":{"duration":0.061512,"end_time":"2023-08-27T14:38:04.07853","exception":false,"start_time":"2023-08-27T14:38:04.017018","status":"completed"},"tags":[]},"source":["# Chapter 21: Working with Text Data: String Methods\n","## 21.1 String Methods for Series\n","#### In this subchapter, we will explore the string methods available for Series objects in Pandas. These methods allow us to manipulate and extract information from text data."]},{"cell_type":"markdown","id":"0971473b","metadata":{"papermill":{"duration":0.0517,"end_time":"2023-08-27T14:38:04.189382","exception":false,"start_time":"2023-08-27T14:38:04.137682","status":"completed"},"tags":[]},"source":["```python\n","# Example: String methods for Series\n","import pandas as pd\n","\n","# Convert all elements to lowercase\n","series_lower = series.str.lower()\n","\n","# Extract a substring from each element\n","series_substr = series.str.extract('(\\d+)')\n","```"]},{"cell_type":"markdown","id":"9e9762e3","metadata":{"papermill":{"duration":0.05203,"end_time":"2023-08-27T14:38:04.293366","exception":false,"start_time":"2023-08-27T14:38:04.241336","status":"completed"},"tags":[]},"source":["## 21.2 String Methods for DataFrames\n","#### We can also apply string methods to entire DataFrames, either on specific columns or the entire DataFrame. This allows us to perform text operations on multiple columns simultaneously."]},{"cell_type":"markdown","id":"9c61745f","metadata":{"papermill":{"duration":0.054968,"end_time":"2023-08-27T14:38:04.401712","exception":false,"start_time":"2023-08-27T14:38:04.346744","status":"completed"},"tags":[]},"source":["```python\n","# Example: String methods for DataFrames\n","import pandas as pd\n","\n","# Apply a string method to a column in a DataFrame\n","df['column_lower'] = df['column'].str.lower()\n","\n","# Apply a string method to the entire DataFrame\n","df_lower = df.apply(lambda x: x.str.lower() if x.dtype == 'object' else x)\n","```\n"]},{"cell_type":"markdown","id":"51b91359","metadata":{"papermill":{"duration":0.052188,"end_time":"2023-08-27T14:38:04.507975","exception":false,"start_time":"2023-08-27T14:38:04.455787","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>22</font></h1>\n"]},{"cell_type":"markdown","id":"245ebd98","metadata":{"papermill":{"duration":0.052441,"end_time":"2023-08-27T14:38:04.615658","exception":false,"start_time":"2023-08-27T14:38:04.563217","status":"completed"},"tags":[]},"source":["# Chapter 22: Handling Categorical Data: Categorical Data Types\n","## 22.1 Converting to Categorical Data Type\n","#### In this subchapter, we will learn how to convert columns in a DataFrame to the categorical data type. This is useful when working with data that has a limited set of unique values."]},{"cell_type":"markdown","id":"602a0a54","metadata":{"papermill":{"duration":0.052197,"end_time":"2023-08-27T14:38:04.720445","exception":false,"start_time":"2023-08-27T14:38:04.668248","status":"completed"},"tags":[]},"source":["```python\n","# Example: Converting columns to categorical data type\n","import pandas as pd\n","\n","# Convert a column to categorical data type\n","df['column'] = df['column'].astype('category')\n","\n","# Convert multiple columns to categorical data type\n","df = df.astype({'column1': 'category', 'column2': 'category'})\n","```"]},{"cell_type":"markdown","id":"751aaebd","metadata":{"papermill":{"duration":0.052075,"end_time":"2023-08-27T14:38:04.825893","exception":false,"start_time":"2023-08-27T14:38:04.773818","status":"completed"},"tags":[]},"source":["## 22.2 Categorical Data Attributes and Methods\n","#### Once the columns are converted to the categorical data type, we can leverage the attributes and methods specific to categorical data. This includes accessing categories, codes, and performing operations like sorting."]},{"cell_type":"markdown","id":"0a03f6c4","metadata":{"papermill":{"duration":0.052277,"end_time":"2023-08-27T14:38:04.930649","exception":false,"start_time":"2023-08-27T14:38:04.878372","status":"completed"},"tags":[]},"source":["```python\n","# Example: Categorical data attributes and methods\n","import pandas as pd\n","\n","# Access the categories of a categorical column\n","categories = df['column'].cat.categories\n","\n","# Access the codes of a categorical column\n","codes = df['column'].cat.codes\n","\n","# Sort a DataFrame based on a categorical column\n","sorted_df = df.sort_values('column')\n","```"]},{"cell_type":"markdown","id":"86a50173","metadata":{"papermill":{"duration":0.051914,"end_time":"2023-08-27T14:38:05.034921","exception":false,"start_time":"2023-08-27T14:38:04.983007","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>23</font></h1>\n"]},{"cell_type":"markdown","id":"c9aed4f3","metadata":{"papermill":{"duration":0.053219,"end_time":"2023-08-27T14:38:05.141023","exception":false,"start_time":"2023-08-27T14:38:05.087804","status":"completed"},"tags":[]},"source":["# Chapter 23: Working with Dates and Time: Timestamps, Time Zones\n","## 23.1 Working with Timestamps\n","#### In this subchapter, we will explore how to work with timestamps in Pandas. Timestamps are useful for representing and manipulating dates and times."]},{"cell_type":"markdown","id":"1bba9c8f","metadata":{"papermill":{"duration":0.053309,"end_time":"2023-08-27T14:38:05.24714","exception":false,"start_time":"2023-08-27T14:38:05.193831","status":"completed"},"tags":[]},"source":["```python\n","# Example: Working with timestamps\n","import pandas as pd\n","\n","# Convert a column to datetime format\n","df['timestamp_column'] = pd.to_datetime(df['timestamp_column'])\n","\n","# Extract year, month, and day from a timestamp column\n","df['year'] = df['timestamp_column'].dt.year\n","df['month'] = df['timestamp_column'].dt.month\n","df['day'] = df['timestamp_column'].dt.day\n","```"]},{"cell_type":"markdown","id":"14e2d589","metadata":{"papermill":{"duration":0.0535,"end_time":"2023-08-27T14:38:05.35427","exception":false,"start_time":"2023-08-27T14:38:05.30077","status":"completed"},"tags":[]},"source":["## 23.2 Working with Time Zones\n","#### Pandas also provides functionality for working with time zones. This allows us to convert timestamps between different time zones and perform time zone-aware operations."]},{"cell_type":"markdown","id":"057a36c5","metadata":{"papermill":{"duration":0.052779,"end_time":"2023-08-27T14:38:05.459632","exception":false,"start_time":"2023-08-27T14:38:05.406853","status":"completed"},"tags":[]},"source":["```python\n","# Example: Working with time zones\n","import pandas as pd\n","\n","# Convert timestamps to a specific time zone\n","df['timestamp_column'] = df['timestamp_column'].dt.tz_convert('America/New_York')\n","\n","# Perform time zone-aware calculations\n","df['time_difference'] = df['timestamp_column1'] - df['timestamp_column2']\n","\n","```"]},{"cell_type":"markdown","id":"9f54f54e","metadata":{"papermill":{"duration":0.053295,"end_time":"2023-08-27T14:38:05.571498","exception":false,"start_time":"2023-08-27T14:38:05.518203","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>24</font></h1>\n"]},{"cell_type":"markdown","id":"bcc686a8","metadata":{"papermill":{"duration":0.05224,"end_time":"2023-08-27T14:38:05.676324","exception":false,"start_time":"2023-08-27T14:38:05.624084","status":"completed"},"tags":[]},"source":["# Chapter 24: Resampling and Frequency Conversion: Time Series Data\n","## 24.1 Resampling Time Series Data\n","#### In this subchapter, we will learn how to resample time series data in Pandas. Resampling allows us to change the frequency of the data, such as converting daily data to monthly or yearly data."]},{"cell_type":"markdown","id":"aef55304","metadata":{"papermill":{"duration":0.052487,"end_time":"2023-08-27T14:38:05.781095","exception":false,"start_time":"2023-08-27T14:38:05.728608","status":"completed"},"tags":[]},"source":["```python\n","# Example: Resampling time series data\n","import pandas as pd\n","\n","# Resample daily data to monthly data\n","monthly_data = df.resample('M').sum()\n","\n","# Resample daily data to yearly data\n","yearly_data = df.resample('Y').mean()\n","```"]},{"cell_type":"markdown","id":"8bfafb4a","metadata":{"papermill":{"duration":0.05208,"end_time":"2023-08-27T14:38:05.886165","exception":false,"start_time":"2023-08-27T14:38:05.834085","status":"completed"},"tags":[]},"source":["## 24.2 Frequency Conversion\n","#### Frequency conversion is another way to change the frequency of time series data. This allows us to upsample or downsample the data to a different frequency."]},{"cell_type":"markdown","id":"3fd4858e","metadata":{"papermill":{"duration":0.052751,"end_time":"2023-08-27T14:38:05.991318","exception":false,"start_time":"2023-08-27T14:38:05.938567","status":"completed"},"tags":[]},"source":["```python\n","# Example: Frequency conversion of time series data\n","import pandas as pd\n","\n","# Upsample daily data to hourly data\n","hourly_data = df.asfreq('H')\n","\n","# Downsample hourly data to daily data\n","daily_data = df.asfreq('D')\n","```"]},{"cell_type":"markdown","id":"103da016","metadata":{"papermill":{"duration":0.05478,"end_time":"2023-08-27T14:38:06.09999","exception":false,"start_time":"2023-08-27T14:38:06.04521","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>25</font></h1>\n"]},{"cell_type":"markdown","id":"8656502c","metadata":{"papermill":{"duration":0.052018,"end_time":"2023-08-27T14:38:06.205163","exception":false,"start_time":"2023-08-27T14:38:06.153145","status":"completed"},"tags":[]},"source":["# Chapter 25: Working with Multi-level Indexing\n","## 25.1 Creating Multi-level Index\n","#### In this subchapter, we will explore how to create and work with multi-level indexing in Pandas. Multi-level indexing allows us to have multiple index levels, providing a hierarchical structure to the data."]},{"cell_type":"markdown","id":"b981e9dc","metadata":{"papermill":{"duration":0.053661,"end_time":"2023-08-27T14:38:06.312191","exception":false,"start_time":"2023-08-27T14:38:06.25853","status":"completed"},"tags":[]},"source":["```python\n","# Example: Creating multi-level index\n","import pandas as pd\n","\n","# Create a DataFrame with multi-level index\n","df = pd.DataFrame(data, index=[['A', 'A', 'B', 'B'], [1, 2, 1, 2]], columns=['Column1', 'Column2'])\n","\n","```"]},{"cell_type":"markdown","id":"9510d0e6","metadata":{"papermill":{"duration":0.052353,"end_time":"2023-08-27T14:38:06.417533","exception":false,"start_time":"2023-08-27T14:38:06.36518","status":"completed"},"tags":[]},"source":["## 25.2 Indexing and Selecting with Multi-level Index\n","#### Once the multi-level index is created, we can perform indexing and selection operations to access specific subsets of the data."]},{"cell_type":"markdown","id":"3daa9f70","metadata":{"papermill":{"duration":0.052886,"end_time":"2023-08-27T14:38:06.522801","exception":false,"start_time":"2023-08-27T14:38:06.469915","status":"completed"},"tags":[]},"source":["```python\n","# Example: Indexing and selecting with multi-level index\n","import pandas as pd\n","\n","# Access a specific level of the index\n","level1_data = df.index.get_level_values(0)\n","\n","# Access a specific subset of the data\n","subset = df.loc[('A', 1)]\n","```\n"]},{"cell_type":"markdown","id":"2955bcca","metadata":{"papermill":{"duration":0.052183,"end_time":"2023-08-27T14:38:06.630131","exception":false,"start_time":"2023-08-27T14:38:06.577948","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>26</font></h1>\n"]},{"cell_type":"markdown","id":"c4580736","metadata":{"papermill":{"duration":0.052157,"end_time":"2023-08-27T14:38:06.734489","exception":false,"start_time":"2023-08-27T14:38:06.682332","status":"completed"},"tags":[]},"source":["# Chapter 26: Transformation and Filtering within Groups\n","## 26.1 Transforming Data within Groups\n","#### In this subchapter, we will learn how to transform data within groups in Pandas. This allows us to perform operations on subsets of data based on group criteria."]},{"cell_type":"markdown","id":"3ebd3eee","metadata":{"papermill":{"duration":0.051747,"end_time":"2023-08-27T14:38:06.839255","exception":false,"start_time":"2023-08-27T14:38:06.787508","status":"completed"},"tags":[]},"source":["```python\n","# Example: Transforming data within groups\n","import pandas as pd\n","\n","# Apply a transformation function to each group\n","df['transformed_column'] = df.groupby('group_column')['column'].transform(function)\n","```\n"]},{"cell_type":"markdown","id":"66514471","metadata":{"papermill":{"duration":0.051661,"end_time":"2023-08-27T14:38:06.945785","exception":false,"start_time":"2023-08-27T14:38:06.894124","status":"completed"},"tags":[]},"source":["## 26.2 Filtering Data within Groups\n","#### We can also filter data within groups based on specific conditions. This allows us to retain or exclude groups based on certain criteria."]},{"cell_type":"markdown","id":"9b04a562","metadata":{"papermill":{"duration":0.051836,"end_time":"2023-08-27T14:38:07.050179","exception":false,"start_time":"2023-08-27T14:38:06.998343","status":"completed"},"tags":[]},"source":["```python\n","# Example: Filtering data within groups\n","import pandas as pd\n","\n","# Filter groups based on a condition\n","filtered_df = df.groupby('group_column').filter(lambda x: x['column'].mean() > threshold)\n","\n","```"]},{"cell_type":"markdown","id":"65642d3b","metadata":{"papermill":{"duration":0.054017,"end_time":"2023-08-27T14:38:07.158196","exception":false,"start_time":"2023-08-27T14:38:07.104179","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>27</font></h1>\n"]},{"cell_type":"markdown","id":"3b5d758d","metadata":{"papermill":{"duration":0.052378,"end_time":"2023-08-27T14:38:07.26334","exception":false,"start_time":"2023-08-27T14:38:07.210962","status":"completed"},"tags":[]},"source":["# Chapter 27: Reading and Writing Data to Databases: SQL\n","## 27.1 Reading Data from a SQL Database\n","#### In this subchapter, we will explore how to read data from a SQL database into a Pandas DataFrame. This allows us to leverage SQL queries to retrieve specific data."]},{"cell_type":"markdown","id":"f1334fe9","metadata":{"papermill":{"duration":0.052714,"end_time":"2023-08-27T14:38:07.36888","exception":false,"start_time":"2023-08-27T14:38:07.316166","status":"completed"},"tags":[]},"source":["```python \n","# Example: Reading data from a SQL database\n","import pandas as pd\n","import sqlite3\n","\n","# Connect to the database\n","conn = sqlite3.connect('database.db')\n","\n","# Read data from a SQL query into a DataFrame\n","df = pd.read_sql_query('SELECT * FROM table_name', conn)\n","\n","```"]},{"cell_type":"markdown","id":"380d1904","metadata":{"papermill":{"duration":0.05172,"end_time":"2023-08-27T14:38:07.472879","exception":false,"start_time":"2023-08-27T14:38:07.421159","status":"completed"},"tags":[]},"source":["## 27.2 Writing Data to a SQL Database\n","#### We can also write data from a Pandas DataFrame to a SQL database. This allows us to store and manipulate data in a database for further analysis."]},{"cell_type":"markdown","id":"dcb2df95","metadata":{"papermill":{"duration":0.054901,"end_time":"2023-08-27T14:38:07.580508","exception":false,"start_time":"2023-08-27T14:38:07.525607","status":"completed"},"tags":[]},"source":["```python\n","# Example: Writing data to a SQL database\n","import pandas as pd\n","import sqlite3\n","\n","# Connect to the database\n","conn = sqlite3.connect('database.db')\n","\n","# Write data from a DataFrame to a SQL table\n","df.to_sql('table_name', conn, if_exists='replace', index=False)\n","```\n"]},{"cell_type":"markdown","id":"ac1738f9","metadata":{"papermill":{"duration":0.052741,"end_time":"2023-08-27T14:38:07.686031","exception":false,"start_time":"2023-08-27T14:38:07.63329","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>28</font></h1>\n"]},{"cell_type":"markdown","id":"5d9d4f7b","metadata":{"papermill":{"duration":0.051983,"end_time":"2023-08-27T14:38:07.79067","exception":false,"start_time":"2023-08-27T14:38:07.738687","status":"completed"},"tags":[]},"source":["# Chapter 28: Working with APIs and Web Scraping\n","## 28.1 Accessing Data from APIs\n","#### In this subchapter, we will learn how to access data from APIs using Pandas. APIs allow us to retrieve data from various online sources and incorporate it into our analysis."]},{"cell_type":"markdown","id":"9496e34a","metadata":{"papermill":{"duration":0.051704,"end_time":"2023-08-27T14:38:07.894735","exception":false,"start_time":"2023-08-27T14:38:07.843031","status":"completed"},"tags":[]},"source":["```python\n","# Example: Accessing data from an API\n","import pandas as pd\n","import requests\n","\n","# Make a request to the API\n","response = requests.get('https://api.example.com/data')\n","\n","# Convert the response to a DataFrame\n","df = pd.DataFrame(response.json())\n","```"]},{"cell_type":"markdown","id":"ce26d255","metadata":{"papermill":{"duration":0.052505,"end_time":"2023-08-27T14:38:07.999452","exception":false,"start_time":"2023-08-27T14:38:07.946947","status":"completed"},"tags":[]},"source":["## 28.2 Web Scraping with Pandas\n","#### Pandas also provides functionality for web scraping. This allows us to extract data from HTML tables on web pages and convert it into a DataFrame."]},{"cell_type":"markdown","id":"e1bd54b3","metadata":{"papermill":{"duration":0.05221,"end_time":"2023-08-27T14:38:08.104353","exception":false,"start_time":"2023-08-27T14:38:08.052143","status":"completed"},"tags":[]},"source":["```python\n","# Example: Web scraping with Pandas\n","import pandas as pd\n","\n","# Read HTML tables from a web page\n","dfs = pd.read_html('https://example.com/page')\n","\n","# Select the desired table\n","df = dfs[table_index]\n","```"]},{"cell_type":"markdown","id":"d7736fb7","metadata":{"papermill":{"duration":0.052865,"end_time":"2023-08-27T14:38:08.254175","exception":false,"start_time":"2023-08-27T14:38:08.20131","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>29</font></h1>"]},{"cell_type":"markdown","id":"c4ec9f58","metadata":{"papermill":{"duration":0.053075,"end_time":"2023-08-27T14:38:08.360262","exception":false,"start_time":"2023-08-27T14:38:08.307187","status":"completed"},"tags":[]},"source":["# Chapter 29: Integrating Pandas with other Libraries: NumPy, Matplotlib\n","## 29.1 Integrating Pandas with NumPy\n","#### In this subchapter, we will explore the integration between Pandas and NumPy. NumPy is a powerful library for numerical computing, and Pandas leverages many of its functionalities."]},{"cell_type":"markdown","id":"49c05543","metadata":{"papermill":{"duration":0.052732,"end_time":"2023-08-27T14:38:08.473842","exception":false,"start_time":"2023-08-27T14:38:08.42111","status":"completed"},"tags":[]},"source":["```python\n","# Example: Integrating Pandas with NumPy\n","import pandas as pd\n","import numpy as np\n","\n","# Perform operations with NumPy arrays and Pandas DataFrames\n","result = np.sin(df['column'])\n","```"]},{"cell_type":"markdown","id":"f2327d7f","metadata":{"papermill":{"duration":0.088308,"end_time":"2023-08-27T14:38:08.61944","exception":false,"start_time":"2023-08-27T14:38:08.531132","status":"completed"},"tags":[]},"source":["## 29.2 Integrating Pandas with Matplotlib\n","#### Pandas also integrates seamlessly with Matplotlib, a popular library for data visualization. This allows us to create visualizations directly from Pandas DataFrames."]},{"cell_type":"markdown","id":"ab870ac8","metadata":{"papermill":{"duration":0.052531,"end_time":"2023-08-27T14:38:08.724335","exception":false,"start_time":"2023-08-27T14:38:08.671804","status":"completed"},"tags":[]},"source":["```python\n","# Example: Integrating Pandas with Matplotlib\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Create a line plot from a DataFrame\n","df.plot(x='x_column', y='y_column', kind='line')\n","\n","# Display the plot\n","plt.show()\n","```\n"]},{"cell_type":"markdown","id":"5fca6dbd","metadata":{"papermill":{"duration":0.05289,"end_time":"2023-08-27T14:38:08.829121","exception":false,"start_time":"2023-08-27T14:38:08.776231","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>30</font></h1>"]},{"cell_type":"markdown","id":"af532d0c","metadata":{"papermill":{"duration":0.052958,"end_time":"2023-08-27T14:38:08.934525","exception":false,"start_time":"2023-08-27T14:38:08.881567","status":"completed"},"tags":[]},"source":["# Chapter 30: Data Visualization: Line Plots\n","## 30.1 Creating Line Plots\n","#### In this subchapter, we will learn how to create line plots using Pandas. Line plots are useful for visualizing the relationship between two variables over a continuous range.\n","\n","```python\n","# Example: Creating line plots\n","import pandas as pd\n","\n","# Create a line plot from a DataFrame\n","df.plot(x='x_column', y='y_column', kind='line')\n","```\n","\n","## 30.2 Customizing Line Plots\n","#### We can also customize line plots by adding labels, titles, legends, and adjusting the plot aesthetics to enhance the visualization.\n","\n","```python\n","# Example: Customizing line plots\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Create a line plot with customized settings\n","df.plot(x='x_column', y='y_column', kind='line', label='Line 1', color='red')\n","\n","# Add labels and title\n","plt.xlabel('X-axis')\n","plt.ylabel('Y-axis')\n","plt.title('Line Plot')\n","\n","# Display the legend\n","plt.legend()\n","\n","# Show the plot\n","plt.show()\n","```"]},{"cell_type":"markdown","id":"690ca64a","metadata":{"papermill":{"duration":0.052768,"end_time":"2023-08-27T14:38:09.039797","exception":false,"start_time":"2023-08-27T14:38:08.987029","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>31</font></h1>\n"]},{"cell_type":"markdown","id":"7578e6d8","metadata":{"papermill":{"duration":0.053174,"end_time":"2023-08-27T14:38:09.146549","exception":false,"start_time":"2023-08-27T14:38:09.093375","status":"completed"},"tags":[]},"source":["# Chapter 31: Data Visualization: Bar Plots\n","## 31.1 Creating Bar Plots\n","#### Bar plots are useful for comparing categorical data or displaying frequencies. In this subchapter, we will learn how to create bar plots using Pandas."]},{"cell_type":"markdown","id":"2377cedd","metadata":{"papermill":{"duration":0.052929,"end_time":"2023-08-27T14:38:09.252053","exception":false,"start_time":"2023-08-27T14:38:09.199124","status":"completed"},"tags":[]},"source":["```python\n","# Example: Creating bar plots\n","import pandas as pd\n","\n","# Create a bar plot from a DataFrame\n","df.plot(x='category_column', y='value_column', kind='bar')\n","```\n"]},{"cell_type":"markdown","id":"dd8aeeef","metadata":{"papermill":{"duration":0.05232,"end_time":"2023-08-27T14:38:09.356868","exception":false,"start_time":"2023-08-27T14:38:09.304548","status":"completed"},"tags":[]},"source":["## 31.2 Customizing Bar Plots\n","### We can customize bar plots by adding labels, titles, legends, and adjusting the plot aesthetics to enhance the visualization."]},{"cell_type":"markdown","id":"ecc4e7fe","metadata":{"papermill":{"duration":0.052952,"end_time":"2023-08-27T14:38:09.462088","exception":false,"start_time":"2023-08-27T14:38:09.409136","status":"completed"},"tags":[]},"source":["```python\n","# Example: Customizing bar plots\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Create a bar plot with customized settings\n","df.plot(x='category_column', y='value_column', kind='bar', color='blue')\n","\n","# Add labels and title\n","plt.xlabel('Categories')\n","plt.ylabel('Values')\n","plt.title('Bar Plot')\n","\n","# Display the legend\n","plt.legend()\n","\n","# Show the plot\n","plt.show()\n","```"]},{"cell_type":"markdown","id":"ba0e8434","metadata":{"papermill":{"duration":0.054854,"end_time":"2023-08-27T14:38:09.569085","exception":false,"start_time":"2023-08-27T14:38:09.514231","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>32</font></h1>"]},{"cell_type":"markdown","id":"ab50c5d0","metadata":{"papermill":{"duration":0.052948,"end_time":"2023-08-27T14:38:09.675105","exception":false,"start_time":"2023-08-27T14:38:09.622157","status":"completed"},"tags":[]},"source":["# Chapter 32: Data Visualization: Scatter Plots\n","## 32.1 Creating Scatter Plots\n","#### Scatter plots are useful for visualizing the relationship between two continuous variables. In this subchapter, we will learn how to create scatter plots using Pandas."]},{"cell_type":"markdown","id":"edf6f789","metadata":{"papermill":{"duration":0.051748,"end_time":"2023-08-27T14:38:09.779101","exception":false,"start_time":"2023-08-27T14:38:09.727353","status":"completed"},"tags":[]},"source":["```python\n","# Example: Creating scatter plots\n","import pandas as pd\n","\n","# Create a scatter plot from a DataFrame\n","df.plot(x='x_column', y='y_column', kind='scatter')\n","\n","````"]},{"cell_type":"markdown","id":"d1a46a18","metadata":{"papermill":{"duration":0.052246,"end_time":"2023-08-27T14:38:09.88418","exception":false,"start_time":"2023-08-27T14:38:09.831934","status":"completed"},"tags":[]},"source":["## 32.2 Customizing Scatter Plots\n","#### We can customize scatter plots by adding labels, titles, legends, and adjusting the plot aesthetics to enhance the visualization."]},{"cell_type":"markdown","id":"e02ad48b","metadata":{"papermill":{"duration":0.051971,"end_time":"2023-08-27T14:38:09.989608","exception":false,"start_time":"2023-08-27T14:38:09.937637","status":"completed"},"tags":[]},"source":["```python\n","# Example: Customizing bar plots\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Create a bar plot with customized settings\n","df.plot(x='category_column', y='value_column', kind='bar', color='blue')\n","\n","# Add labels and title\n","plt.xlabel('Categories')\n","plt.ylabel('Values')\n","plt.title('Bar Plot')\n","\n","# Display the legend\n","plt.legend()\n","\n","# Show the plot\n","plt.show()\n","```\n"]},{"cell_type":"markdown","id":"58267fdf","metadata":{"papermill":{"duration":0.052074,"end_time":"2023-08-27T14:38:10.09481","exception":false,"start_time":"2023-08-27T14:38:10.042736","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>33</font></h1>"]},{"cell_type":"markdown","id":"ab8c9a9b","metadata":{"papermill":{"duration":0.052614,"end_time":"2023-08-27T14:38:10.200378","exception":false,"start_time":"2023-08-27T14:38:10.147764","status":"completed"},"tags":[]},"source":["# Chapter 33: Data Visualization: Histograms\n","## 33.1 Creating Histograms\n","#### Histograms are useful for visualizing the distribution of a continuous variable. In this subchapter, we will learn how to create histograms using Pandas.\n","\n","```python\n","# Example: Creating histograms\n","import pandas as pd\n","\n","# Create a histogram from a DataFrame column\n","df['column'].plot(kind='hist')\n","```\n","\n","## 33.2 Customizing Histograms\n","#### We can customize histograms by adjusting the bin size, adding labels, titles, and adjusting the plot aesthetics to enhance the visualization.\n","\n","```python\n","# Example: Customizing histograms\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Create a histogram with customized settings\n","df['column'].plot(kind='hist', bins=20, color='green')\n","\n","# Add labels and title\n","plt.xlabel('Values')\n","plt.ylabel('Frequency')\n","plt.title('Histogram')\n","\n","# Show the plot\n","plt.show()\n","```"]},{"cell_type":"markdown","id":"f2ccccec","metadata":{"papermill":{"duration":0.052607,"end_time":"2023-08-27T14:38:10.305667","exception":false,"start_time":"2023-08-27T14:38:10.25306","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>34</font></h1>\n"]},{"cell_type":"markdown","id":"8749b7a1","metadata":{"papermill":{"duration":0.051748,"end_time":"2023-08-27T14:38:10.410711","exception":false,"start_time":"2023-08-27T14:38:10.358963","status":"completed"},"tags":[]},"source":["# Chapter 34: Data Visualization: Box Plots\n","## 34.1 Creating Box Plots\n","#### Box plots are useful for visualizing the distribution and statistical summary of a continuous variable. In this subchapter, we will learn how to create box plots using Pandas.\n","\n","```python\n","# Example: Creating box plots\n","import pandas as pd\n","\n","# Create a box plot from a DataFrame\n","df['column'].plot(kind='box')\n","```\n","\n","## 34.2 Customizing Box Plots\n","#### We can customize box plots by adding labels, titles, and adjusting the plot aesthetics to enhance the visualization.\n","\n","```python\n","# Example: Customizing box plots\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Create a box plot with customized settings\n","df['column'].plot(kind='box', color='purple')\n","\n","# Add labels and title\n","plt.ylabel('Values')\n","plt.title('Box Plot')\n","\n","# Show the plot\n","plt.show()\n","```"]},{"cell_type":"markdown","id":"fa3b4260","metadata":{"papermill":{"duration":0.058788,"end_time":"2023-08-27T14:38:10.522076","exception":false,"start_time":"2023-08-27T14:38:10.463288","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>35</font></h1>\n"]},{"cell_type":"markdown","id":"a022e8be","metadata":{"papermill":{"duration":0.052877,"end_time":"2023-08-27T14:38:10.630013","exception":false,"start_time":"2023-08-27T14:38:10.577136","status":"completed"},"tags":[]},"source":["# Chapter 35: Data Visualization: Customizing Plots\n","## 35.1 Customizing Plot Aesthetics\n","#### In this subchapter, we will explore various ways to customize the aesthetics of plots created with Pandas. We can modify the colors, markers, line styles, and other visual elements to create visually appealing and informative plots.\n","\n","```python\n","# Example: Customizing plot aesthetics\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Create a line plot with customized aesthetics\n","df.plot(x='x_column', y='y_column', color='red', linestyle='--', marker='o')\n","\n","# Add labels and title\n","plt.xlabel('X-axis')\n","plt.ylabel('Y-axis')\n","plt.title('Customized Plot')\n","\n","# Show the plot\n","plt.show()\n","```"]},{"cell_type":"markdown","id":"f3f2fa12","metadata":{"papermill":{"duration":0.052229,"end_time":"2023-08-27T14:38:10.735223","exception":false,"start_time":"2023-08-27T14:38:10.682994","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>36</font></h1>"]},{"cell_type":"markdown","id":"3312d070","metadata":{"papermill":{"duration":0.052263,"end_time":"2023-08-27T14:38:10.839874","exception":false,"start_time":"2023-08-27T14:38:10.787611","status":"completed"},"tags":[]},"source":["# Chapter 36: Handling Large Datasets with Chunking\n","## 36.1 Processing Data in Chunks\n","#### When working with large datasets that may not fit into memory, we can use chunking to process the data in manageable portions. In this subchapter, we will learn how to read and process data in chunks using Pandas.\n","\n","```python\n","# Example: Processing data in chunks\n","import pandas as pd\n","\n","# Read data in chunks\n","chunk_size = 1000\n","for chunk in pd.read_csv('data.csv', chunksize=chunk_size):\n","    # Process each chunk of data\n","    # ...\n","  ```\n","  \n","## 36.2 Aggregating Chunk Results\n","#### We can aggregate the results obtained from processing data chunks to obtain the final result. This allows us to work with large datasets efficiently.\n","\n","```python\n","# Example: Aggregating chunk results\n","import pandas as pd\n","\n","# Initialize an empty DataFrame for result aggregation\n","result = pd.DataFrame()\n","\n","# Read data in chunks and aggregate results\n","chunk_size = 1000\n","for chunk in pd.read_csv('data.csv', chunksize=chunk_size):\n","    # Process each chunk of data\n","    processed_chunk = process_chunk(chunk)\n","    \n","    # Aggregate the results\n","    result = result.append(processed_chunk)\n","\n","# Perform further analysis on the aggregated result\n","# ...\n","```"]},{"cell_type":"markdown","id":"74cdbd88","metadata":{"papermill":{"duration":0.051724,"end_time":"2023-08-27T14:38:10.944102","exception":false,"start_time":"2023-08-27T14:38:10.892378","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>37</font></h1>"]},{"cell_type":"markdown","id":"99ebeff5","metadata":{"papermill":{"duration":0.051781,"end_time":"2023-08-27T14:38:11.048775","exception":false,"start_time":"2023-08-27T14:38:10.996994","status":"completed"},"tags":[]},"source":["# Chapter 37: Optimizing Memory Usage with Data Types\n","## 37.1 Data Type Selection for Memory Optimization\n","#### In this subchapter, we will learn how to optimize memory usage in Pandas by selecting appropriate data types for columns. Choosing the right data types can significantly reduce memory consumption, especially for large datasets.\n","\n","```python\n","# Example: Data type selection for memory optimization\n","import pandas as pd\n","\n","# Specify data types for columns during data loading\n","dtypes = {'column1': 'int32', 'column2': 'float32', 'column3': 'category'}\n","df = pd.read_csv('data.csv', dtype=dtypes)\n","```\n","\n","## 37.2 Downsampling and Data Type Conversion\n","#### We can further optimize memory usage by downsampling large datasets or converting data types to more memory-efficient alternatives.\n","\n","```python\n","# Example: Downsampling and data type conversion for memory optimization\n","import pandas as pd\n","\n","# Downsample a DataFrame by selecting every nth row\n","df_downsampled = df[::n]\n","\n","# Convert data types to more memory-efficient alternatives\n","df['column1'] = df['column1'].astype('int16')\n","df['column2'] = df['column2'].astype('float16')\n","```"]},{"cell_type":"markdown","id":"ff3897a0","metadata":{"papermill":{"duration":0.052044,"end_time":"2023-08-27T14:38:11.152954","exception":false,"start_time":"2023-08-27T14:38:11.10091","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>38</font></h1>"]},{"cell_type":"markdown","id":"4d52247b","metadata":{"papermill":{"duration":0.052493,"end_time":"2023-08-27T14:38:11.263395","exception":false,"start_time":"2023-08-27T14:38:11.210902","status":"completed"},"tags":[]},"source":["# Chapter 38: Performance Optimization Techniques: Vectorization\n","## 38.1 Vectorized Operations\n","#### Vectorization is a performance optimization technique that leverages the capabilities of NumPy arrays to perform operations on entire arrays instead of individual elements. In this subchapter, we will learn how to utilize vectorization in Pandas to improve performance.\n","\n","```python\n","# Example: Vectorized operations\n","import pandas as pd\n","\n","# Perform vectorized operations on DataFrame columns\n","df['result_column'] = df['column1'] + df['column2']\n","```\n","## 38.2 Using Built-in Pandas Functions\n","#### Pandas provides many built-in functions that are optimized for performance. Utilizing these functions can significantly improve the execution time of data manipulation and analysis tasks.\n","\n","```python\n","# Example: Using built-in Pandas functions\n","import pandas as pd\n","\n","# Apply a built-in Pandas function to a DataFrame column\n","df['result_column'] = df['column'].apply(pd.function_name)\n","```"]},{"cell_type":"markdown","id":"2d9c1cb7","metadata":{"papermill":{"duration":0.052955,"end_time":"2023-08-27T14:38:11.369177","exception":false,"start_time":"2023-08-27T14:38:11.316222","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>39</font></h1>\n"]},{"cell_type":"markdown","id":"8df434dc","metadata":{"papermill":{"duration":0.052998,"end_time":"2023-08-27T14:38:11.475065","exception":false,"start_time":"2023-08-27T14:38:11.422067","status":"completed"},"tags":[]},"source":["# Chapter 39: Performance Optimization Techniques: Broadcasting\n","## 39.1 Broadcasting Operations\n","#### Broadcasting is a performance optimization technique that allows operations to be performed efficiently on arrays of different shapes. In this subchapter, we will learn how to leverage broadcasting in Pandas to improve performance.\n","\n","```python\n","# Example: Broadcasting operations\n","import pandas as pd\n","\n","# Perform broadcasting operations on DataFrame columns\n","df['result_column'] = df['column1'] * df['column2']\n","```\n","## 39.2 Avoiding Iteration\n","#### Iterating over rows or elements of a DataFrame can be slow and inefficient. It is recommended to avoid iteration whenever possible and leverage vectorized operations or broadcasting instead.\n","\n","```python\n","# Example: Avoiding iteration\n","import pandas as pd\n","\n","# Avoid iterating over rows\n","df['result_column'] = df['column'].apply(lambda x: perform_operation(x))\n","```"]},{"cell_type":"markdown","id":"6c29617a","metadata":{"papermill":{"duration":0.055714,"end_time":"2023-08-27T14:38:11.584629","exception":false,"start_time":"2023-08-27T14:38:11.528915","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>40</font></h1>\n"]},{"cell_type":"markdown","id":"02651bed","metadata":{"papermill":{"duration":0.052696,"end_time":"2023-08-27T14:38:11.690807","exception":false,"start_time":"2023-08-27T14:38:11.638111","status":"completed"},"tags":[]},"source":["# Chapter 40: Performance Optimization Techniques: Indexing and Slicing\n","## 40.1 Efficient Indexing\n","#### Efficient indexing can significantly improve the performance of data retrieval and manipulation in Pandas. In this subchapter, we will explore techniques for efficient indexing and slicing.\n","\n","```python\n","# Example: Efficient indexing\n","import pandas as pd\n","\n","# Set a column as the DataFrame index for faster data retrieval\n","df.set_index('column', inplace=True)\n","\n","# Retrieve data using the index\n","result = df.loc['index_value']\n","```\n","\n","\n","## 40.2 Slicing with iloc and loc\n","#### Using the iloc and loc indexers, we can perform efficient slicing operations on Pandas DataFrames. These indexers allow us to select specific rows or columns based on their position or labels.\n","\n","```python\n","# Example: Slicing with iloc and loc\n","import pandas as pd\n","\n","# Select rows and columns using iloc\n","df.iloc[2:5, 1:4]\n","\n","# Select rows and columns using loc\n","df.loc[['row1', 'row2'], ['column1', 'column2']]\n","```"]},{"cell_type":"markdown","id":"60173c7c","metadata":{"papermill":{"duration":0.052204,"end_time":"2023-08-27T14:38:11.795565","exception":false,"start_time":"2023-08-27T14:38:11.743361","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>41</font></h1>"]},{"cell_type":"markdown","id":"de9075e3","metadata":{"papermill":{"duration":0.052355,"end_time":"2023-08-27T14:38:11.900907","exception":false,"start_time":"2023-08-27T14:38:11.848552","status":"completed"},"tags":[]},"source":["# Chapter 41: Working with Panel Data: Panel, Panel4D\n","## 41.1 Introduction to Panel Data\n","#### Panel data is a type of three-dimensional data structure that extends the concepts of DataFrame to handle data with an additional dimension. In this subchapter, we will explore the Panel and Panel4D data structures in Pandas.\n","\n","```python\n","# Example: Creating a Panel\n","import pandas as pd\n","\n","# Create a Panel with random data\n","data = {'Item1': df1, 'Item2': df2, 'Item3': df3}\n","panel = pd.Panel(data)\n","```\n","\n","## 41.2 Working with Panel Data\n","#### We can perform various operations on Panel data, including indexing, slicing, and mathematical operations across different items and axes.\n","\n","```python\n","# Example: Indexing and slicing a Panel\n","import pandas as pd\n","\n","# Select data from a Panel\n","panel['Item1']\n","panel.loc[:, :, 'column_label']\n","```"]},{"cell_type":"markdown","id":"0378a76c","metadata":{"papermill":{"duration":0.053362,"end_time":"2023-08-27T14:38:12.00705","exception":false,"start_time":"2023-08-27T14:38:11.953688","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>42</font></h1>"]},{"cell_type":"markdown","id":"2c9d4bbe","metadata":{"papermill":{"duration":0.052069,"end_time":"2023-08-27T14:38:12.111564","exception":false,"start_time":"2023-08-27T14:38:12.059495","status":"completed"},"tags":[]},"source":["# Chapter 42: Handling Sparse Data\n","## 42.1 Sparse Data Representation\n","#### Sparse data is data that contains a significant number of missing or zero values. In this subchapter, we will explore how to efficiently represent and handle sparse data in Pandas.\n","\n","```python\n","# Example: Sparse data representation\n","import pandas as pd\n","\n","# Convert a DataFrame to a sparse DataFrame\n","sparse_df = df.to_sparse()\n","```\n","\n","## 42.2 Operations with Sparse Data\n","#### Pandas allows us to perform various operations on sparse data, such as filling missing values and compressing the data for storage.\n","\n","```python\n","# Example: Operations with sparse data\n","import pandas as pd\n","\n","# Fill missing values in a sparse DataFrame\n","sparse_df.fillna(0, inplace=True)\n","\n","# Compress the sparse DataFrame for storage\n","sparse_df.to_sparse(fill_value=0)\n","```"]},{"cell_type":"markdown","id":"9dba5f2d","metadata":{"papermill":{"duration":0.051963,"end_time":"2023-08-27T14:38:12.215594","exception":false,"start_time":"2023-08-27T14:38:12.163631","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>43</font></h1>\n"]},{"cell_type":"markdown","id":"5d7f3e9c","metadata":{"papermill":{"duration":0.052569,"end_time":"2023-08-27T14:38:12.320393","exception":false,"start_time":"2023-08-27T14:38:12.267824","status":"completed"},"tags":[]},"source":["# Chapter 43: Advanced Groupby Operations\n","## 43.1 Advanced Grouping Techniques\n","#### In this subchapter, we will explore advanced techniques for grouping data in Pandas. These techniques include grouping by multiple columns, custom aggregation functions, and handling of missing data during grouping.\n","\n","```python\n","# Example: Advanced grouping with custom aggregation\n","import pandas as pd\n","\n","# Group by multiple columns and apply custom aggregation functions\n","grouped = df.groupby(['column1', 'column2']).agg({'column3': 'sum', 'column4': custom_function})\n","```\n","\n","\n","## 43.2 Transformation and Filtration within Groups\n","#### Pandas allows us to perform data transformation and filtration within groups using the transform() and filter() functions.\n","\n","```python\n","# Example: Transformation and filtration within groups\n","import pandas as pd\n","\n","# Apply transformation within groups\n","df['normalized_column'] = df.groupby('group_column')['data_column'].transform(lambda x: (x - x.mean()) / x.std())\n","\n","# Filter groups based on custom conditions\n","filtered_groups = df.groupby('group_column').filter(lambda x: len(x) > 10)\n","```"]},{"cell_type":"markdown","id":"59954a37","metadata":{"papermill":{"duration":0.052427,"end_time":"2023-08-27T14:38:12.425782","exception":false,"start_time":"2023-08-27T14:38:12.373355","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>44</font></h1>"]},{"cell_type":"markdown","id":"51eb219e","metadata":{"papermill":{"duration":0.05298,"end_time":"2023-08-27T14:38:12.531196","exception":false,"start_time":"2023-08-27T14:38:12.478216","status":"completed"},"tags":[]},"source":["# Chapter 44: Pivot Tables and Cross-tabulations\n","## 44.1 Creating Pivot Tables\n","#### Pivot tables are a powerful tool for summarizing and aggregating data in Pandas. In this subchapter, we will learn how to create pivot tables to analyze data from different perspectives.\n","\n","```python\n","# Example: Creating a pivot table\n","import pandas as pd\n","\n","# Create a pivot table\n","pivot_table = df.pivot_table(index='index_column', columns='columns_column', values='values_column', aggfunc='mean')\n","```\n","\n","## 44.2 Cross-tabulations\n","#### Cross-tabulations are another useful way to summarize data and calculate frequencies of occurrences between variables.\n","\n","```python\n","# Example: Creating a cross-tabulation\n","import pandas as pd\n","\n","# Create a cross-tabulation\n","cross_tab = pd.crosstab(df['column1'], df['column2'])\n","```"]},{"cell_type":"markdown","id":"49ad1040","metadata":{"papermill":{"duration":0.053029,"end_time":"2023-08-27T14:38:12.640415","exception":false,"start_time":"2023-08-27T14:38:12.587386","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>45</font></h1>\n"]},{"cell_type":"markdown","id":"be4cfaee","metadata":{"papermill":{"duration":0.053222,"end_time":"2023-08-27T14:38:12.747143","exception":false,"start_time":"2023-08-27T14:38:12.693921","status":"completed"},"tags":[]},"source":["# Chapter 45: Working with Geospatial Data\n","## 45.1 Geospatial Data Formats\n","#### Geospatial data represents information about the physical location and characteristics of objects on the Earth's surface. In this subchapter, we will explore how to work with geospatial data in Pandas using common formats such as GeoJSON and shapefiles.\n","\n","```python\n","# Example: Working with geospatial data\n","import pandas as pd\n","import geopandas as gpd\n","\n","# Read a shapefile into a GeoDataFrame\n","gdf = gpd.read_file('path/to/shapefile.shp')\n","\n","# Perform geospatial operations on the GeoDataFrame\n","gdf_buffer = gdf.buffer(100)\n","```\n","\n","## 45.2 Geospatial Data Analysis\n","#### Pandas provides functionality to perform various geospatial data analysis tasks, such as spatial joins, distance calculations, and spatial aggregations.\n","\n","```python\n","# Example: Geospatial data analysis\n","import pandas as pd\n","import geopandas as gpd\n","\n","# Perform a spatial join between two GeoDataFrames\n","join_result = gpd.sjoin(gdf1, gdf2, how='inner', op='intersects')\n","\n","# Calculate distances between points in a GeoDataFrame\n","gdf['distance'] = gdf.distance(other_point)\n","\n","# Aggregate data within spatial boundaries\n","aggregated = gdf1.groupby('polygon_id').agg({'column': 'sum'})\n","```"]},{"cell_type":"markdown","id":"a9dea85b","metadata":{"papermill":{"duration":0.052391,"end_time":"2023-08-27T14:38:12.852034","exception":false,"start_time":"2023-08-27T14:38:12.799643","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>46</font></h1>\n"]},{"cell_type":"markdown","id":"584878f4","metadata":{"papermill":{"duration":0.05223,"end_time":"2023-08-27T14:38:12.957687","exception":false,"start_time":"2023-08-27T14:38:12.905457","status":"completed"},"tags":[]},"source":["# Chapter 46: Interactive Visualizations with Pandas\n","## 46.1 Introduction to Interactive Visualizations\n","#### Interactive visualizations allow users to explore and interact with data in real-time. In this subchapter, we will explore how to create interactive visualizations using Pandas in combination with libraries such as Plotly and Bokeh.\n","\n","```python\n","# Example: Creating an interactive visualization with Plotly\n","import pandas as pd\n","import plotly.express as px\n","\n","# Create an interactive line plot with Plotly\n","fig = px.line(df, x='x_column', y='y_column', title='Interactive Line Plot')\n","fig.show()\n","```\n","\n","## 46.2 Interactive Visualizations with Bokeh\n","#### Bokeh is a powerful library for creating interactive visualizations in Python. In this subchapter, we will learn how to use Bokeh to create interactive plots and dashboards.\n","\n","```python\n","# Example: Creating an interactive scatter plot with Bokeh\n","import pandas as pd\n","from bokeh.plotting import figure, output_file, show\n","\n","# Create an interactive scatter plot with Bokeh\n","p = figure(title='Interactive Scatter Plot')\n","p.scatter(df['x_column'], df['y_column'])\n","show(p)\n","```"]},{"cell_type":"markdown","id":"e0ede674","metadata":{"papermill":{"duration":0.052937,"end_time":"2023-08-27T14:38:13.063925","exception":false,"start_time":"2023-08-27T14:38:13.010988","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>47</font></h1>"]},{"cell_type":"markdown","id":"a3c39c6c","metadata":{"papermill":{"duration":0.052639,"end_time":"2023-08-27T14:38:13.169838","exception":false,"start_time":"2023-08-27T14:38:13.117199","status":"completed"},"tags":[]},"source":["# Chapter 47: Best Practices and Tips for Pandas\n","## 47.1 Performance Optimization Tips\n","#### To improve the performance of your Pandas code, it is important to follow best practices and utilize optimization techniques. In this subchapter, we will explore some tips and tricks for optimizing your Pandas workflows.\n","\n","```python\n","# Example: Performance optimization tips\n","import pandas as pd\n","\n","# Use vectorized operations instead of iterating over rows\n","df['result_column'] = df['column1'] + df['column2']\n","\n","# Set the appropriate data types for columns to save memory\n","df['column'] = df['column'].astype('category')\n","```\n","\n","## 47.2 Memory Optimization Tips\n","#### Pandas provides various techniques to optimize memory usage, especially for large datasets. In this subchapter, we will learn how to reduce memory consumption by selecting appropriate data types and utilizing sparse data structures.\n","\n","```python\n","# Example: Memory optimization tips\n","import pandas as pd\n","\n","# Convert columns to more memory-efficient data types\n","df['column'] = pd.to_numeric(df['column'], downcast='integer')\n","\n","# Convert DataFrame to a sparse DataFrame\n","sparse_df = df.to_sparse(fill_value=0)\n","```"]},{"cell_type":"markdown","id":"7e9a2cd8","metadata":{"papermill":{"duration":0.052243,"end_time":"2023-08-27T14:38:13.274931","exception":false,"start_time":"2023-08-27T14:38:13.222688","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>48</font></h1>"]},{"cell_type":"markdown","id":"66a6b919","metadata":{"papermill":{"duration":0.052223,"end_time":"2023-08-27T14:38:13.379992","exception":false,"start_time":"2023-08-27T14:38:13.327769","status":"completed"},"tags":[]},"source":["# Chapter 48: Integration with Machine Learning Libraries: Scikit-learn\n","## 48.1 Introduction to Scikit-learn\n","#### Scikit-learn is a popular machine learning library in Python. In this subchapter, we will explore how to integrate Pandas with Scikit-learn for data preprocessing, feature engineering, and model training.\n","\n","```python\n","# Example: Integration with Scikit-learn\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression\n","\n","# Preprocess data with Pandas and Scikit-learn\n","X = df[['feature1', 'feature2']]\n","y = df['target']\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Train a model with Scikit-learn\n","model = LinearRegression()\n","model.fit(X_scaled, y)\n","```\n","\n","## 48.2 Feature Engineering with Pandas\n","#### Pandas provides powerful tools for feature engineering, which is the process of creating new features from existing data. In this subchapter, we will learn how to perform feature engineering tasks using Pandas before feeding the data to Scikit-learn models.\n","\n","```python\n","# Example: Feature engineering with Pandas\n","import pandas as pd\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","# Create polynomial features with Pandas\n","poly_features = PolynomialFeatures(degree=2)\n","X_poly = poly_features.fit_transform(df[['feature1', 'feature2']])\n","```"]},{"cell_type":"markdown","id":"35ff647c","metadata":{"papermill":{"duration":0.051756,"end_time":"2023-08-27T14:38:13.483945","exception":false,"start_time":"2023-08-27T14:38:13.432189","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>49</font></h1>\n"]},{"cell_type":"markdown","id":"987cc06e","metadata":{"papermill":{"duration":0.05478,"end_time":"2023-08-27T14:38:13.59172","exception":false,"start_time":"2023-08-27T14:38:13.53694","status":"completed"},"tags":[]},"source":["# Chapter 49: Pandas in a Production Environment\n","## 49.1 Scaling Pandas for Production\n","#### When working with large datasets or in a production environment, it is important to consider scaling Pandas for performance and efficiency. In this subchapter, we will explore techniques for optimizing Pandas workflows and working with distributed computing frameworks.\n","\n","```python\n","# Example: Scaling Pandas for production\n","import pandas as pd\n","import dask.dataframe as dd\n","\n","# Load data with Dask for distributed computing\n","ddf = dd.read_csv('large_dataset.csv')\n","\n","# Perform computations in parallel with Dask\n","result = ddf.groupby('column').sum().compute()\n","```\n","\n","## 49.2 Deploying Pandas Applications\n","#### To deploy Pandas applications in a production environment, it is essential to consider factors such as performance, scalability, and reliability. In this subchapter, we will explore different deployment options and best practices for deploying Pandas applications.\n","\n","```python\n","# Example: Deploying Pandas applications\n","import pandas as pd\n","from flask import Flask, request, jsonify\n","\n","# Create a Flask application\n","app = Flask(__name__)\n","\n","# Define an API endpoint for data processing\n","@app.route('/process_data', methods=['POST'])\n","def process_data():\n","    data = request.json['data']\n","    # Perform data processing with Pandas\n","    processed_data = process_data_with_pandas(data)\n","    return jsonify({'result': processed_data})\n","\n","if __name__ == '__main__':\n","    app.run()\n","```"]},{"cell_type":"markdown","id":"28d1793a","metadata":{"papermill":{"duration":0.05266,"end_time":"2023-08-27T14:38:13.697042","exception":false,"start_time":"2023-08-27T14:38:13.644382","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>50</font></h1>"]},{"cell_type":"markdown","id":"2bb33294","metadata":{"papermill":{"duration":0.055803,"end_time":"2023-08-27T14:38:13.810753","exception":false,"start_time":"2023-08-27T14:38:13.75495","status":"completed"},"tags":[]},"source":["# Chapter 50: Recap and Next Steps\n","## 50.1 Recap of Key Concepts\n","#### In this final chapter, we will recap the key concepts and techniques covered throughout the book. We will summarize the important features and functionalities of Pandas and provide a quick reference guide for future use.\n","\n","## 50.2 Next Steps\n","#### After completing this book, you should have a solid understanding of Pandas and its various capabilities for data manipulation and analysis. In this section, we will provide guidance on further resources and next steps to continue your learning journey with Pandas.\n","\n","#### Congratulations on completing the book! Pandas is a powerful library that enables efficient data manipulation and analysis. With the knowledge gained from this book, you are well-equipped to tackle a wide range of data-related tasks using Pandas. Keep exploring, practicing, and applying your skills to real-world scenarios to further enhance your proficiency in Pandas and data analysis."]},{"cell_type":"markdown","id":"3fa1bd99","metadata":{"papermill":{"duration":0.057585,"end_time":"2023-08-27T14:38:13.928482","exception":false,"start_time":"2023-08-27T14:38:13.870897","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>51</font></h1>\n"]},{"cell_type":"markdown","id":"3e9c9ab3","metadata":{"papermill":{"duration":0.057924,"end_time":"2023-08-27T14:38:14.045056","exception":false,"start_time":"2023-08-27T14:38:13.987132","status":"completed"},"tags":[]},"source":["# Chapter 51 Handling Imbalanced Data\n","#### Imbalanced data refers to a dataset where the number of instances in one class is significantly higher or lower than the number of instances in another class. In this chapter, we will explore techniques for handling imbalanced data using Pandas.\n","\n","## 51.1 Understanding Imbalanced Data\n","#### Before we dive into handling imbalanced data, it's important to understand the challenges it presents and the impact it can have on data analysis and machine learning models. In this subchapter, we will discuss the characteristics of imbalanced data and the issues it can cause.\n","\n","## 51.2 Techniques for Handling Imbalanced Data\n","#### There are various techniques available to address imbalanced data and mitigate its impact on data analysis and modeling. In this subchapter, we will explore some commonly used techniques, such as undersampling, oversampling, and generating synthetic samples.\n","\n","```python\n","# Example: Handling imbalanced data using undersampling and oversampling\n","import pandas as pd\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.over_sampling import RandomOverSampler\n","\n","# Undersample the majority class\n","rus = RandomUnderSampler()\n","X_resampled, y_resampled = rus.fit_resample(X, y)\n","\n","# Oversample the minority class\n","ros = RandomOverSampler()\n","X_resampled, y_resampled = ros.fit_resample(X, y)\n","```\n","\n","## 51.3 Evaluating Model Performance with Imbalanced Data\n","#### When working with imbalanced data, it's important to evaluate model performance using appropriate metrics that account for the class imbalance. In this subchapter, we will discuss evaluation metrics such as precision, recall, F1-score, and area under the ROC curve (AUC-ROC).\n","\n","```python\n","# Example: Evaluating model performance with imbalanced data\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, roc_auc_score\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","# Train a classification model\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate model performance\n","print(classification_report(y_test, y_pred))\n","auc = roc_auc_score(y_test, y_pred)\n","print(f\"AUC-ROC: {auc}\")\n","```"]},{"cell_type":"markdown","id":"48bbdd0d","metadata":{"papermill":{"duration":0.05674,"end_time":"2023-08-27T14:38:14.15917","exception":false,"start_time":"2023-08-27T14:38:14.10243","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>52</font></h1>\n"]},{"cell_type":"markdown","id":"3b501556","metadata":{"papermill":{"duration":0.057959,"end_time":"2023-08-27T14:38:14.273588","exception":false,"start_time":"2023-08-27T14:38:14.215629","status":"completed"},"tags":[]},"source":["# Chapter 52 Time Series Analysis with Pandas\n","#### Time series data represents observations collected over a sequence of time intervals. In this chapter, we will explore how to perform time series analysis using Pandas, including handling date/time data, resampling, and analyzing trends and seasonality.\n","\n","## 52.1 Handling Date/Time Data in Pandas\n","#### Before diving into time series analysis, it's important to understand how to handle date/time data in Pandas. In this subchapter, we will explore the DatetimeIndex and how to perform date/time operations on a DataFrame.\n","\n","```python\n","# Example: Handling date/time data in Pandas\n","import pandas as pd\n","\n","# Create a DataFrame with date/time data\n","df = pd.DataFrame({'date': ['2022-01-01', '2022-01-02', '2022-01-03'],\n","                   'value': [10, 20, 30]})\n","\n","# Convert the 'date' column to a datetime type\n","df['date'] = pd.to_datetime(df['date'])\n","\n","# Set the 'date' column as the DataFrame index\n","df.set_index('date', inplace=True)\n","```\n","\n","## 52.2 Resampling Time Series Data\n","#### Resampling is the process of changing the frequency of time series data. In this subchapter, we will explore how to resample time series data using Pandas, including upsampling and downsampling.\n","\n","```python\n","# Example: Resampling time series data in Pandas\n","import pandas as pd\n","\n","# Resample daily data to monthly data (upsampling)\n","monthly_data = df.resample('M').mean()\n","\n","# Resample daily data to weekly data (downsampling)\n","weekly_data = df.resample('W').sum()\n","```\n","\n","## 52.3 Analyzing Time Series Trends and Seasonality\n","#### Time series data often exhibits trends and seasonality. In this subchapter, we will explore techniques for analyzing and visualizing trends and seasonality in time series data using Pandas.\n","\n","```python\n","# Example: Analyzing time series trends and seasonality\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Compute rolling mean and rolling standard deviation\n","rolling_mean = df['value'].rolling(window=7).mean()\n","rolling_std = df['value'].rolling(window=7).std()\n","\n","# Plot the original data, rolling mean, and rolling standard deviation\n","plt.plot(df.index, df['value'], label='Original')\n","plt.plot(df.index, rolling_mean, label='Rolling Mean')\n","plt.plot(df.index, rolling_std, label='Rolling Std')\n","plt.legend()\n","plt.show()\n","```"]},{"cell_type":"markdown","id":"02d9ded7","metadata":{"papermill":{"duration":0.061256,"end_time":"2023-08-27T14:38:14.393464","exception":false,"start_time":"2023-08-27T14:38:14.332208","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>54</font></h1>\n"]},{"cell_type":"markdown","id":"b0d3f704","metadata":{"papermill":{"duration":0.054207,"end_time":"2023-08-27T14:38:14.50641","exception":false,"start_time":"2023-08-27T14:38:14.452203","status":"completed"},"tags":[]},"source":["# 54 Working with Excel Files: Reading and Writing\n","#### Excel files are a common data storage format used in many organizations. In this chapter, we will explore how to read and write Excel files using Pandas, allowing you to interact with Excel data in a tabular format.\n","\n","## 54.1 Reading Excel Files into Pandas\n","#### Pandas provides functionality to read data from Excel files into a DataFrame. In this subchapter, we will explore different methods for reading Excel files using Pandas.\n","\n","```python\n","# Example: Reading Excel files into Pandas\n","import pandas as pd\n","\n","# Read an Excel file into a DataFrame\n","df = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n","\n","# Read multiple sheets from an Excel file\n","data = pd.read_excel('data.xlsx', sheet_name=['Sheet1', 'Sheet2'])\n","```\n","\n","## 54.2 Writing Pandas DataFrames to Excel\n","#### Pandas allows you to write data from DataFrames to Excel files. In this subchapter, we will explore how to write Pandas DataFrames to Excel files, including writing multiple DataFrames to different sheets.\n","\n","```python\n","# Example: Writing Pandas DataFrames to Excel\n","import pandas as pd\n","\n","# Write a DataFrame to an Excel file\n","df.to_excel('output.xlsx', sheet_name='Sheet1', index=False)\n","\n","# Write multiple DataFrames to different sheets in an Excel file\n","with pd.ExcelWriter('output.xlsx') as writer:\n","    df1.to_excel(writer, sheet_name='Sheet1', index=False)\n","    df2.to_excel(writer, sheet_name='Sheet2', index=False)\n","    \n"," ```"]},{"cell_type":"markdown","id":"bde8c59c","metadata":{"papermill":{"duration":0.057737,"end_time":"2023-08-27T14:38:14.620234","exception":false,"start_time":"2023-08-27T14:38:14.562497","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>55</font></h1>\n"]},{"cell_type":"markdown","id":"d2e1456e","metadata":{"papermill":{"duration":0.056215,"end_time":"2023-08-27T14:38:14.734691","exception":false,"start_time":"2023-08-27T14:38:14.678476","status":"completed"},"tags":[]},"source":["## 55 Handling XML Data\n","#### XML (eXtensible Markup Language) is a popular data format for representing structured data. In this chapter, we will explore how to handle XML data using Pandas, including parsing XML files and working with XML data in a tabular format.\n","\n","## 55.1 Parsing XML Files into Pandas\n","#### Pandas does not provide native support for parsing XML files. However, we can use external libraries such as xml.etree.ElementTree to parse XML data and convert it into a DataFrame.\n","\n","```python\n","# Example: Parsing XML files into Pandas DataFrame\n","import pandas as pd\n","import xml.etree.ElementTree as ET\n","\n","# Parse XML file\n","tree = ET.parse('data.xml')\n","root = tree.getroot()\n","\n","# Extract data from XML and create a DataFrame\n","data = []\n","for child in root:\n","    data.append({\n","        'name': child.find('name').text,\n","        'age': int(child.find('age').text),\n","        'city': child.find('city').text\n","    })\n","\n","df = pd.DataFrame(data)\n","```\n","\n","## 55.2 Working with XML Data in a Tabular Format\n","#### Once the XML data is parsed and converted into a DataFrame, you can perform various data manipulations and analysis using Pandas. In this subchapter, we will explore how to work with XML data in a tabular format using Pandas.\n","\n","```python\n","# Example: Working with XML data in a tabular format using Pandas\n","import pandas as pd\n","\n","# Perform data manipulations on the XML data DataFrame\n","# ...\n","\n","# Save the modified DataFrame to a new XML file\n","df.to_xml('output.xml', root_name='data', row_name='row', indent='    ')\n","```"]},{"cell_type":"markdown","id":"967704e2","metadata":{"execution":{"iopub.execute_input":"2023-07-04T09:07:35.133366Z","iopub.status.busy":"2023-07-04T09:07:35.132403Z","iopub.status.idle":"2023-07-04T09:07:35.140191Z","shell.execute_reply":"2023-07-04T09:07:35.138935Z","shell.execute_reply.started":"2023-07-04T09:07:35.133322Z"},"papermill":{"duration":0.053978,"end_time":"2023-08-27T14:38:14.890518","exception":false,"start_time":"2023-08-27T14:38:14.83654","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>56</font></h1>\n"]},{"cell_type":"markdown","id":"cd70e78b","metadata":{"papermill":{"duration":0.056377,"end_time":"2023-08-27T14:38:15.001368","exception":false,"start_time":"2023-08-27T14:38:14.944991","status":"completed"},"tags":[]},"source":["# 56 Handling HTML Data\n","#### HTML (Hypertext Markup Language) is the standard markup language used for creating web pages. In this chapter, we will explore how to handle HTML data using Pandas, including web scraping and extracting data from HTML tables.\n","\n","## 56.1 Web Scraping HTML Tables into Pandas\n","#### Web scraping is the process of extracting data from websites. Pandas provides functionality to scrape HTML tables from web pages and convert them into DataFrames.\n","\n","```python\n","# Example: Web scraping HTML tables into Pandas\n","import pandas as pd\n","\n","# Scrape HTML tables from a web page\n","tables = pd.read_html('https://www.example.com')\n","\n","# Get the first table as a DataFrame\n","df = tables[0]\n","```\n","\n","\n","## 56.2 Extracting Data from HTML Tables\n","#### Pandas allows you to extract data from specific HTML tables on a web page. In this subchapter, we will explore how to extract data from specific HTML tables and perform further data analysis.\n","\n","\n","```python\n","# Example: Extracting data from HTML tables using Pandas\n","import pandas as pd\n","\n","# Extract data from specific HTML tables\n","url = 'https://www.example.com'\n","tables = pd.read_html(url, match='Table Name')\n","\n","# Get the desired table as a DataFrame\n","df = tables[0]\n","\n","# Perform data analysis on the extracted data\n","# \n","```"]},{"cell_type":"markdown","id":"c70bab0f","metadata":{"papermill":{"duration":0.056976,"end_time":"2023-08-27T14:38:15.117321","exception":false,"start_time":"2023-08-27T14:38:15.060345","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>57</font></h1>\n"]},{"cell_type":"markdown","id":"cb97616d","metadata":{"papermill":{"duration":0.055792,"end_time":"2023-08-27T14:38:15.230128","exception":false,"start_time":"2023-08-27T14:38:15.174336","status":"completed"},"tags":[]},"source":["# Chapter 57 Dealing with Financial Data\n","#### Financial data often requires specialized techniques for analysis and processing. In this chapter, we will explore how to deal with financial data using Pandas, including handling time series data, calculating financial indicators, and performing portfolio analysis.\n","\n","## 57.1 Handling Financial Time Series Data\n","#### Financial data is often represented as time series data, where observations are collected at regular intervals. In this subchapter, we will explore how to handle financial time series data using Pandas, including resampling, shifting, and lagging.\n","\n","\n","```python\n","# Example: Handling financial time series data with Pandas\n","import pandas as pd\n","\n","# Read financial data into a DataFrame\n","df = pd.read_csv('financial_data.csv', parse_dates=['date'], index_col='date')\n","\n","# Resample daily data to monthly data\n","monthly_data = df.resample('M').last()\n","\n","# Shift data by a certain number of periods\n","df['previous_close'] = df['close'].shift()\n","\n","# Compute returns\n","df['returns'] = df['close'].pct_change()\n","```\n","\n","\n","## 57.2 Calculating Financial Indicators\n","#### Financial indicators provide insights into the performance and trends of financial data. In this subchapter, we will explore how to calculate financial indicators, such as moving averages, exponential moving averages, and Bollinger Bands, using Pandas.\n","\n","```python\n","# Example: Calculating financial indicators with Pandas\n","import pandas as pd\n","\n","# Compute the 50-day simple moving average\n","df['sma_50'] = df['close'].rolling(window=50).mean()\n","\n","# Compute the 20-day exponential moving average\n","df['ema_20'] = df['close'].ewm(span=20, adjust=False).mean()\n","\n","# Compute Bollinger Bands\n","rolling_mean = df['close'].rolling(window=20).mean()\n","rolling_std = df['close'].rolling(window=20).std()\n","df['upper_band'] = rolling_mean + 2 * rolling_std\n","df['lower_band'] = rolling_mean - 2 * rolling_std\n","\n","\n","```\n","\n","## 57.3 Performing Portfolio Analysis\n","#### Portfolio analysis involves analyzing the performance and risk of a collection of financial assets. In this subchapter, we will explore how to perform portfolio analysis using Pandas, including calculating portfolio returns and risks, and optimizing portfolio allocations.\n","```python\n","# Example: Performing portfolio analysis with Pandas\n","import pandas as pd\n","import numpy as np\n","\n","# Read portfolio data into a DataFrame\n","df = pd.read_csv('portfolio_data.csv')\n","\n","# Compute portfolio returns\n","df['portfolio_return'] = np.dot(df[['stock1_return', 'stock2_return', 'stock3_return']].values,\n","                                [0.4, 0.3, 0.3])\n","\n","# Compute portfolio risk (standard deviation)\n","df['portfolio_risk'] = np.sqrt(np.dot(df[['stock1_return', 'stock2_return', 'stock3_return']].cov(),\n","                                      [0.4, 0.3, 0.3]))\n","\n","# Optimize portfolio allocations\n","# ...\n","```"]},{"cell_type":"markdown","id":"489b6b34","metadata":{"execution":{"iopub.execute_input":"2023-07-04T09:07:37.82402Z","iopub.status.busy":"2023-07-04T09:07:37.823311Z","iopub.status.idle":"2023-07-04T09:07:37.829896Z","shell.execute_reply":"2023-07-04T09:07:37.828739Z","shell.execute_reply.started":"2023-07-04T09:07:37.823988Z"},"papermill":{"duration":0.055008,"end_time":"2023-08-27T14:38:15.342014","exception":false,"start_time":"2023-08-27T14:38:15.287006","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>58</font></h1>\n"]},{"cell_type":"markdown","id":"2093f644","metadata":{"papermill":{"duration":0.054626,"end_time":"2023-08-27T14:38:15.450886","exception":false,"start_time":"2023-08-27T14:38:15.39626","status":"completed"},"tags":[]},"source":["# Chapter 58 Working with Images and Multimedia Data\n","#### Images and multimedia data require specialized techniques for handling and analyzing. In this chapter, we will explore how to work with images and multimedia data using libraries such as Pillow and OpenCV in conjunction with Pandas.\n","\n","## 58.1 Reading and Displaying Images\n","#### Pandas can work together with the Pillow library to read and display images. In this subchapter, we will explore how to read images into Pandas and display them.\n","\n","```python\n","# Example: Reading and displaying images with Pandas and Pillow\n","import pandas as pd\n","from PIL import Image\n","\n","# Read an image into a DataFrame\n","image_path = 'image.jpg'\n","image = Image.open(image_path)\n","image_array = np.array(image)\n","df = pd.DataFrame(image_array)\n","\n","# Display the image\n","plt.imshow(image_array)\n","plt.axis('off')\n","plt.show()\n","```\n","\n","\n","## 58.2 Image Processing and Analysis\n","#### Pandas can be used in conjunction with libraries like Pillow and OpenCV to perform image processing and analysis. In this subchapter, we will explore how to apply various image processing techniques to images stored as DataFrames.\n","\n","```python\n","# Example: Image processing and analysis with Pandas, Pillow, and OpenCV\n","import pandas as pd\n","from PIL import Image\n","import cv2\n","\n","# Read an image into a DataFrame using Pillow\n","image_path = 'image.jpg'\n","image = Image.open(image_path)\n","image_array = np.array(image)\n","df = pd.DataFrame(image_array)\n","\n","# Convert the image to grayscale using OpenCV\n","gray_image = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n","gray_df = pd.DataFrame(gray_image)\n","\n","# Apply image filters using Pillow\n","filtered_image = image.filter(ImageFilter.BLUR)\n","filtered_array = np.array(filtered_image)\n","filtered_df = pd.DataFrame(filtered_array)\n","\n","```"]},{"cell_type":"markdown","id":"7fdfbd95","metadata":{"papermill":{"duration":0.059131,"end_time":"2023-08-27T14:38:15.564355","exception":false,"start_time":"2023-08-27T14:38:15.505224","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>59</font></h1>\n"]},{"cell_type":"markdown","id":"f4b9fc6b","metadata":{"papermill":{"duration":0.053544,"end_time":"2023-08-27T14:38:15.674121","exception":false,"start_time":"2023-08-27T14:38:15.620577","status":"completed"},"tags":[]},"source":["# Chapter 59 Handling Big Data with Dask\n","#### Dask is a flexible parallel computing library that enables efficient processing of large datasets. In this chapter, we will explore how to handle big data using Dask and Pandas, including reading and writing large datasets, parallel processing, and out-of-memory computations.\n","\n","## 59.1 Dask DataFrames\n","#### Dask provides a DataFrame API that mimics the Pandas DataFrame interface but can handle larger-than-memory datasets. In this subchapter, we will explore how to work with Dask DataFrames, including reading and writing large datasets.\n","\n","```python\n","# Example: Working with Dask DataFrames\n","import dask.dataframe as dd\n","\n","# Read a large CSV file into a Dask DataFrame\n","df = dd.read_csv('large_data.csv')\n","\n","# Perform computations on the Dask DataFrame\n","result = df.groupby('category').mean()\n","\n","# Write the result to a new CSV file\n","result.to_csv('result.csv')\n","```\n","\n","\n","## 59.2 Parallel Processing with Dask\n","#### Dask allows you to perform parallel processing on large datasets, enabling faster computations. In this subchapter, we will explore how to leverage Dask's parallel computing capabilities to process data in parallel.\n","\n","```python\n","# Example: Parallel processing with Dask\n","import dask.dataframe as dd\n","\n","# Read a large CSV file into a Dask DataFrame\n","df = dd.read_csv('large_data.csv')\n","\n","# Perform computations on the Dask DataFrame in parallel\n","result = df.groupby('category').apply(lambda x: x['value'].mean(), meta=('category', float)).compute()\n","\n","# Display the result\n","print(result)\n","```"]},{"cell_type":"markdown","id":"e1e6efc9","metadata":{"papermill":{"duration":0.05447,"end_time":"2023-08-27T14:38:15.782705","exception":false,"start_time":"2023-08-27T14:38:15.728235","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>60</font></h1>\n"]},{"cell_type":"markdown","id":"8e828303","metadata":{"papermill":{"duration":0.05292,"end_time":"2023-08-27T14:38:15.889236","exception":false,"start_time":"2023-08-27T14:38:15.836316","status":"completed"},"tags":[]},"source":["# Chapter 60. Parallel Processing with Pandas\n","#### Parallel processing is a powerful technique for speeding up data processing tasks by utilizing multiple CPU cores or even distributed computing resources. In this chapter, we will explore how to leverage parallel processing techniques with Pandas to improve the performance of data manipulation and analysis tasks.\n","\n","## 60.1 Introduction to Parallel Processing\n","#### Parallel processing involves breaking down a task into smaller subtasks that can be executed simultaneously. By distributing the workload across multiple cores or machines, parallel processing can significantly reduce the execution time of computationally intensive operations.\n","\n","## 60.2 Parallelizing Computations with Pandas\n","#### Pandas provides several mechanisms for parallelizing computations, allowing you to take advantage of multiple CPU cores. The DataFrame and Series objects in Pandas support parallel operations through the use of the multiprocessing module or by utilizing vectorized operations.\n","\n","### 60.2.1 Using the multiprocessing Module\n","#### The multiprocessing module in Python allows you to create multiple processes to execute tasks in parallel. Pandas provides a convenient way to parallelize computations using the apply function with the multiprocessing backend.\n","\n","```python\n","import pandas as pd\n","\n","# Define a function to be applied to each row of a DataFrame\n","def process_row(row):\n","    # Perform some computation\n","    # ...\n","\n","# Create a DataFrame\n","df = pd.DataFrame(...)\n","\n","# Apply the function in parallel using the multiprocessing backend\n","df_parallel = df.apply(process_row, axis=1, backend='multiprocessing')\n","```\n","\n","\n","\n","## 60.2.2 Vectorized Operations\n","#### Pandas also supports vectorized operations, which can significantly improve performance by executing operations on entire arrays or columns at once instead of iterating over individual elements. Vectorized operations are automatically parallelized and executed efficiently by utilizing optimized libraries such as NumPy.\n","\n","```python\n","import pandas as pd\n","\n","# Create a DataFrame\n","df = pd.DataFrame(...)\n","\n","# Perform vectorized operations on columns\n","df['new_column'] = df['column1'] + df['column2']\n","```\n","\n","\n","## 60.3 Distributed Computing with Pandas\n","#### In addition to parallel processing on a single machine, Pandas can also be used in conjunction with distributed computing frameworks such as Dask or PySpark to perform computations across multiple machines or clusters. These frameworks allow you to scale your data processing tasks to handle larger datasets or leverage distributed computing resources.\n","\n","### 60.3.1 Using Dask with Pandas\n","#### Dask is a flexible parallel computing library that seamlessly integrates with Pandas and provides distributed computing capabilities. By using Dask, you can leverage a distributed cluster to process large datasets in parallel.\n","\n","```python\n","import dask.dataframe as dd\n","\n","# Read a large dataset into a Dask DataFrame\n","df = dd.read_csv('large_data.csv')\n","\n","# Perform computations on the Dask DataFrame\n","result = df.groupby('category').sum()\n","\n","# Compute the result in parallel\n","result = result.compute()\n","```\n","\n","\n","### 60.3.2 Using PySpark with Pandas\n","#### PySpark is the Python API for Apache Spark, a powerful distributed computing framework. PySpark allows you to leverage Spark's distributed processing capabilities while still utilizing the familiar Pandas API.\n","\n","```python\n","from pyspark.sql import SparkSession\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a Spark DataFrame from a Pandas DataFrame\n","spark_df = spark.createDataFrame(pandas_df)\n","\n","# Perform distributed computations using Spark DataFrame operations\n","result = spark_df.groupby('category').sum().toPandas()\n","```\n","\n","\n","\n","#### By utilizing distributed computing frameworks with Pandas, you can efficiently process large datasets and distribute the workload across multiple machines or clusters.\n","\n","#### Parallel processing with Pandas can greatly enhance the performance of data manipulation and analysis tasks, allowing you to work with larger datasets and reduce computation time. Whether through the multiprocessing module, vectorized operations, or distributed computing frameworks like Dask and PySpark, Pandas provides flexible options for parallelizing computations."]},{"cell_type":"markdown","id":"68725677","metadata":{"papermill":{"duration":0.053202,"end_time":"2023-08-27T14:38:15.995764","exception":false,"start_time":"2023-08-27T14:38:15.942562","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>61</font></h1>\n"]},{"cell_type":"markdown","id":"a82db9a0","metadata":{"papermill":{"duration":0.053608,"end_time":"2023-08-27T14:38:16.102631","exception":false,"start_time":"2023-08-27T14:38:16.049023","status":"completed"},"tags":[]},"source":["# Chapter  61 Machine Learning Pipelines with Pandas\n","#### Machine learning pipelines are a powerful tool for streamlining the process of building and deploying machine learning models. In this chapter, we will explore how to create machine learning pipelines using Pandas for data preprocessing, feature engineering, model training, and evaluation.\n","\n","## 61.1 Introduction to Machine Learning Pipelines\n","#### A machine learning pipeline is a sequence of data processing components that are chained together to automate the machine learning workflow. It allows you to streamline the process of data preparation, model training, and model evaluation.\n","\n","## 61.2 Building a Machine Learning Pipeline with Pandas\n","#### Pandas provides a rich set of functionalities for data preprocessing and feature engineering, making it an excellent tool for building machine learning pipelines. Here's an example of how to build a simple machine learning pipeline using Pandas:\n","\n","```python\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load the dataset into a Pandas DataFrame\n","df = pd.read_csv('data.csv')\n","\n","# Split the data into training and test sets\n","X = df.drop('target', axis=1)\n","y = df['target']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a pipeline\n","pipeline = Pipeline([\n","    ('scaler', StandardScaler()),  # Data preprocessing step\n","    ('classifier', LogisticRegression())  # Model training step\n","])\n","\n","# Fit the pipeline to the training data\n","pipeline.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = pipeline.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy}\")\n","```\n","\n","#### In this example, the pipeline consists of two steps: data preprocessing (scaling the features using StandardScaler) and model training (using LogisticRegression). The pipeline is then fitted to the training data and used to make predictions on the test data. Finally, the accuracy of the model is evaluated.\n","    \n","\n","## 61.3 Customizing and Extending the Pipeline\n","#### You can customize and extend the machine learning pipeline by adding additional preprocessing steps, feature selection techniques, or different models. Pandas provides a wide range of functionalities for data manipulation and feature engineering, allowing you to easily incorporate them into your pipeline.\n","\n","```python\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the dataset into a Pandas DataFrame\n","df = pd.read_csv('data.csv')\n","\n","# Split the data into training and test sets\n","X = df.drop('target', axis=1)\n","y = df['target']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a pipeline with additional preprocessing steps and feature selection\n","pipeline = Pipeline([\n","    ('scaler', StandardScaler()),  # Data preprocessing step\n","    ('polynomial_features', PolynomialFeatures(degree=2)),  # Additional feature engineering step\n","    ('feature_selection', SelectKBest(score_func=f_classif, k=10)),  # Feature selection step\n","    ('classifier', RandomForestClassifier())  # Model training step\n","])\n","\n","# Fit the pipeline to the training data\n","pipeline.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = pipeline.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy}\")\n","```\n","\n","#### In this example, we have extended the pipeline by adding a feature engineering step using PolynomialFeatures to create interaction terms between the features. We have also included a feature selection step using SelectKBest to select the top 10 features based on the F-value. The model training step uses RandomForestClassifier for classification. By customizing and extending the pipeline, you can adapt it to your specific machine learning tasks.\n","\n","## 61.4 Conclusion\n","#### Machine learning pipelines provide a structured and efficient approach to building and deploying machine learning models. Pandas, with its powerful data manipulation and feature engineering capabilities, is an excellent tool for constructing machine learning pipelines. By following the examples and guidelines in this chapter, you can leverage the power of Pandas to create robust and scalable machine learning workflows.\n","\n"]},{"cell_type":"markdown","id":"14c731d4","metadata":{"execution":{"iopub.execute_input":"2023-07-04T09:11:27.906035Z","iopub.status.busy":"2023-07-04T09:11:27.90567Z","iopub.status.idle":"2023-07-04T09:11:27.913521Z","shell.execute_reply":"2023-07-04T09:11:27.91195Z","shell.execute_reply.started":"2023-07-04T09:11:27.906006Z"},"papermill":{"duration":0.052982,"end_time":"2023-08-27T14:38:16.209714","exception":false,"start_time":"2023-08-27T14:38:16.156732","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>62</font></h1>\n"]},{"cell_type":"markdown","id":"dd0ad6bb","metadata":{"papermill":{"duration":0.054402,"end_time":"2023-08-27T14:38:16.318123","exception":false,"start_time":"2023-08-27T14:38:16.263721","status":"completed"},"tags":[]},"source":["# Chapter 62 Time Zone Conversion and Localization\n","#### Dealing with time zone information is crucial when working with time series data that spans different regions or when analyzing data collected across various time zones. Pandas provides powerful tools for time zone conversion and localization, allowing you to handle time-related data accurately and effectively.\n","\n","## 62.1 Introduction to Time Zone Conversion and Localization\n","#### Time zone conversion involves converting datetime values from one time zone to another, while time zone localization refers to associating datetime values with specific time zones. Both operations are essential for ensuring consistent and accurate analysis of time series data.\n","\n","## 62.2 Handling Time Zone Conversion\n","#### Pandas provides the tz_convert() method to convert datetime values from one time zone to another. The method takes a time zone string as an argument and returns a new datetime series with the converted time zone.\n","\n","```python\n","import pandas as pd\n","\n","# Create a datetime series with a specific time zone\n","dates = pd.to_datetime(['2022-01-01 10:00', '2022-01-01 15:00', '2022-01-01 20:00'])\n","dates = dates.tz_localize('America/New_York')\n","\n","# Convert the datetime series to a different time zone\n","converted_dates = dates.dt.tz_convert('Europe/Paris')\n","\n","print(converted_dates)\n","```\n","\n","#### In this example, the tz_localize() method is used to associate the datetime series with the 'America/New_York' time zone. Then, the tz_convert() method is applied to convert the datetime series to the 'Europe/Paris' time zone. The resulting datetime series, converted_dates, will have the converted time zone.\n","\n","## 62.3 Handling Time Zone Localization\n","#### To localize datetime values with a specific time zone, you can use the tz_localize() method. This method associates a time zone with a datetime series that doesn't have a time zone assigned.\n","\n","```python\n","import pandas as pd\n","\n","# Create a datetime series without a time zone\n","dates = pd.to_datetime(['2022-01-01 10:00', '2022-01-01 15:00', '2022-01-01 20:00'])\n","\n","# Localize the datetime series with a specific time zone\n","localized_dates = dates.dt.tz_localize('America/New_York')\n","\n","print(localized_dates)\n","```\n","\n","#### In this example, the tz_localize() method is used to associate the datetime series with the 'America/New_York' time zone. The resulting datetime series, localized_dates, will have the specified time zone assigned.\n","\n","## 62.4 Handling Daylight Saving Time (DST)\n","#### Pandas handles Daylight Saving Time (DST) automatically when converting or localizing datetime values. It adjusts the datetime values based on the rules of the respective time zone, accounting for DST transitions.\n","\n","## 62.5 Conclusion\n","#### Time zone conversion and localization are crucial aspects of working with time series data that spans different regions or time zones. Pandas provides powerful tools for handling time zone-related operations, allowing you to convert datetime values from one time zone to another and associate datetime values with specific time zones. By utilizing these tools effectively,\n","#### you can ensure accurate and consistent analysis of time-related data."]},{"cell_type":"markdown","id":"c3670fd3","metadata":{"execution":{"iopub.execute_input":"2023-07-04T09:13:05.054Z","iopub.status.busy":"2023-07-04T09:13:05.053628Z","iopub.status.idle":"2023-07-04T09:13:05.061308Z","shell.execute_reply":"2023-07-04T09:13:05.059576Z","shell.execute_reply.started":"2023-07-04T09:13:05.053969Z"},"papermill":{"duration":0.053549,"end_time":"2023-08-27T14:38:16.42636","exception":false,"start_time":"2023-08-27T14:38:16.372811","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>63</font></h1>\n"]},{"cell_type":"markdown","id":"e7301fdd","metadata":{"papermill":{"duration":0.053413,"end_time":"2023-08-27T14:38:16.532841","exception":false,"start_time":"2023-08-27T14:38:16.479428","status":"completed"},"tags":[]},"source":["# Chapter 63 Advanced Data Visualization: Heatmaps\n","##### Heatmaps are a powerful visualization technique for representing data in a matrix format, where the values are represented by colors. Pandas, in combination with libraries like Matplotlib and Seaborn, provides convenient tools to create informative and visually appealing heatmaps.\n","\n","## 63.1 Introduction to Heatmaps\n","#### Heatmaps are particularly useful for visualizing correlation matrices, categorical data frequency tables, and other types of data that can be represented in a tabular format. They provide a quick and intuitive way to identify patterns, relationships, and variations in the data.\n","\n","## 63.2 Creating Heatmaps with Pandas and Seaborn\n","#### To create a heatmap in Pandas, you can use the heatmap() function provided by the Seaborn library. This function takes a 2D array-like object, such as a Pandas DataFrame, and generates a heatmap based on the values in the array.\n","\n","```python\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Create a correlation matrix\n","data = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\n","corr_matrix = data.corr()\n","\n","# Create a heatmap of the correlation matrix\n","sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n","plt.title('Correlation Matrix Heatmap')\n","plt.show()\n","```\n","\n","\n","#### In this example, we first create a correlation matrix using the corr() function on a Pandas DataFrame. Then, we pass the correlation matrix to the heatmap() function from Seaborn. We set the annot parameter to True to display the correlation values on the heatmap. The cmap parameter specifies the color map to be used. Finally, we add a title to the heatmap using Matplotlib's title() function and display the heatmap using plt.show().\n","\n","## 63.3 Customizing Heatmaps\n","#### Heatmaps can be customized in various ways to improve their readability and aesthetics. Some common customization options include:\n","\n","#### **Changing the color map**: You can choose from a wide range of color maps provided by Matplotlib or create your own custom color map.\n","#### **Adjusting the color intensity**: You can control the intensity of the colors used in the heatmap by setting the vmin and vmax parameters.\n","#### **Adding annotations**: You can display additional information, such as the actual values or data labels, on the heatmap using the annot parameter.\n","#### **Modifying axis labels**: You can customize the labels of the x-axis and y-axis to provide more meaningful information about the data being represented.\n","#### **Controlling the figure size**: You can adjust the size of the heatmap figure to accommodate more data or fit into a specific layout.\n","#### By experimenting with these customization options, you can create heatmaps that effectively convey the intended information and enhance the visual appeal of your data visualization.\n","\n","## 63.4 Conclusion\n","#### Heatmaps are a valuable tool for visualizing data in a matrix format, providing insights into patterns, correlations, and variations in the data. With Pandas and Seaborn, you can easily create heatmaps from 2D array-like objects, such as DataFrames, and customize them to suit your specific needs. By incorporating heatmaps into your data analysis and visualization workflow, you can effectively communicate complex information and uncover hidden insights in your data.\n","\n"]},{"cell_type":"markdown","id":"18ab8164","metadata":{"papermill":{"duration":0.057579,"end_time":"2023-08-27T14:38:16.643393","exception":false,"start_time":"2023-08-27T14:38:16.585814","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>64</font></h1>\n"]},{"cell_type":"markdown","id":"7b3d32ac","metadata":{"papermill":{"duration":0.052118,"end_time":"2023-08-27T14:38:16.748774","exception":false,"start_time":"2023-08-27T14:38:16.696656","status":"completed"},"tags":[]},"source":["##  Chapter 64 Advanced Data Visualization: 3D Plots\n","#### In addition to traditional 2D plots, Pandas provides functionality for creating advanced 3D plots to visualize multidimensional data. These 3D plots offer a unique perspective on the data and allow for a deeper understanding of complex relationships.\n","\n","## 64.1 Introduction to 3D Plots\n","### 3D plots are particularly useful when dealing with data that has multiple independent variables. They enable the visualization of relationships among three or more variables, providing a comprehensive view of the data distribution and interactions.\n","\n","## 64.2 Creating 3D Plots with Pandas and Matplotlib\n","#### Pandas, in combination with Matplotlib, offers several methods for creating 3D plots. One popular approach is to use the plot_surface() function, which generates a 3D surface plot based on the provided data.\n","\n","```python\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","# Create a DataFrame with three variables\n","data = pd.DataFrame({'X': [1, 2, 3], 'Y': [4, 5, 6], 'Z': [7, 8, 9]})\n","\n","# Create a 3D plot\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","ax.plot_surface(data['X'], data['Y'], data['Z'])\n","ax.set_xlabel('X')\n","ax.set_ylabel('Y')\n","ax.set_zlabel('Z')\n","plt.title('3D Plot')\n","plt.show()\n","```\n","\n","\n","\n","#### In this example, we create a DataFrame with three variables: X, Y, and Z. We then create a 3D plot by using the plot_surface() function from Matplotlib's Axes3D module. We pass the data series for X, Y, and Z to this function, which generates a surface plot based on the provided data. Finally, we set the labels for the x-axis, y-axis, and z-axis using the set_xlabel(), set_ylabel(), and set_zlabel() methods, respectively. We add a title to the plot using Matplotlib's title() function and display the plot using plt.show().\n","\n","## 64.3 Customizing 3D Plots\n","#### Similar to 2D plots, 3D plots can be customized in various ways to improve their appearance and convey the intended information more effectively. Some common customization options include:\n","\n","#### **Changing the viewpoint**: You can adjust the camera angle and elevation to change the perspective of the 3D plot and provide different viewpoints.\n","#### **Adding markers and annotations**: You can mark specific data points or add text annotations to highlight important features or provide additional information.\n","#### **Controlling color and shading**: You can customize the colors used in the 3D plot and adjust the shading to enhance the visual representation.\n","#### **Modifying axis limits and scaling**: You can set the limits and scaling of the x-axis, y-axis, and z-axis to focus on specific regions of interest or ensure appropriate scaling for the data.\n","#### By experimenting with these customization options and exploring additional techniques provided by Matplotlib, you can create visually appealing and informative 3D plots that effectively convey complex relationships within your data.\n","\n","## 64.4 Conclusion\n","#### 3D plots offer a unique perspective on multidimensional data, enabling a deeper understanding of complex relationships. With Pandas and Matplotlib, you can easily create 3D plots to visualize data with multiple independent variables. By customizing these plots and exploring different viewpoints, colors, and annotations, you can enhance their visual appeal and uncover valuable insights from your data. Incorporating 3D plots into your data analysis and visualization workflow can provide \n","#### a more comprehensive understanding of your data and facilitate better decision-making."]},{"cell_type":"markdown","id":"3266a5f3","metadata":{"papermill":{"duration":0.052537,"end_time":"2023-08-27T14:38:16.854325","exception":false,"start_time":"2023-08-27T14:38:16.801788","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>65</font></h1>\n"]},{"cell_type":"markdown","id":"9bb37705","metadata":{"papermill":{"duration":0.052492,"end_time":"2023-08-27T14:38:16.960234","exception":false,"start_time":"2023-08-27T14:38:16.907742","status":"completed"},"tags":[]},"source":["## Chapter 65 Handling Network Data\n","#### Network data, such as social networks, transportation networks, or computer networks, can be analyzed and visualized using Pandas. Pandas provides powerful tools for working with network data structures and performing various network analysis tasks.\n","\n","## 65.1 Introduction to Network Data\n","#### Network data consists of nodes (vertices) and edges that connect these nodes. Each node represents an entity, such as a person or a device, and each edge represents a relationship or connection between nodes. Analyzing network data can help uncover patterns, identify influential nodes, and understand the flow of information or resources within a network.\n","\n","## 65.2 Representing Network Data in Pandas\n","#### Pandas offers multiple ways to represent network data. One common approach is to use two DataFrames: one for nodes and one for edges. The nodes DataFrame contains information about each node, such as node IDs and node attributes. The edges DataFrame contains information about each edge, such as the source node, target node, and edge attributes.\n","\n","\n","\n","```python\n","import pandas as pd\n","\n","# Create a DataFrame for nodes\n","nodes_data = {'node_id': [1, 2, 3], 'node_attribute': ['A', 'B', 'C']}\n","nodes_df = pd.DataFrame(nodes_data)\n","\n","# Create a DataFrame for edges\n","edges_data = {'source': [1, 2], 'target': [2, 3], 'edge_attribute': ['X', 'Y']}\n","edges_df = pd.DataFrame(edges_data)\n","```\n","\n","\n","#### In this example, we create two DataFrames: nodes_df for storing node information and edges_df for storing edge information. The nodes_df DataFrame has two columns: node_id and node_attribute, representing the ID and attribute of each node. The edges_df DataFrame has three columns: source, target, and edge_attribute, representing the source node, target node, and attribute of each edge.\n","\n","## 65.3 Network Analysis with Pandas\n","#### Once the network data is represented in DataFrames, Pandas provides various methods for performing network analysis tasks. Some common network analysis tasks include:\n","\n","#### **Computing network measures**: Pandas allows you to calculate various network measures, such as degree centrality, betweenness centrality, and clustering coefficient, to quantify the importance and structure of nodes and edges.\n","#### **Filtering and selecting nodes or edges**: You can use Pandas' filtering and selection capabilities to focus on specific nodes or edges based on certain criteria, such as node attributes or edge weights.\n","#### **Visualizing network data**: Pandas can be used in conjunction with network visualization libraries, such as NetworkX or PyGraphviz, to create visual representations of network data, such as node-link diagrams or network graphs.\n","#### By leveraging the power of Pandas and combining it with other libraries dedicated to network analysis, you can gain valuable insights from network data and make informed decisions based on the underlying network structure and dynamics.\n","\n","## 65.4 Conclusion\n","#### Network data analysis is a powerful technique for understanding relationships, information flow, and structural patterns within complex systems. With Pandas, you can easily represent network data using DataFrames and perform various network analysis tasks. By combining Pandas with other network analysis libraries, you can gain deeper insights into network structures and dynamics. Incorporating network analysis into your data analysis workflow can help\n","#### you uncover hidden patterns and relationships within network data and make data-driven decisions."]},{"cell_type":"markdown","id":"fda0a685","metadata":{"papermill":{"duration":0.05229,"end_time":"2023-08-27T14:38:17.065393","exception":false,"start_time":"2023-08-27T14:38:17.013103","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>66</font></h1>\n"]},{"cell_type":"markdown","id":"a5525b4b","metadata":{"papermill":{"duration":0.052743,"end_time":"2023-08-27T14:38:17.171454","exception":false,"start_time":"2023-08-27T14:38:17.118711","status":"completed"},"tags":[]},"source":["# Chapter 66 Text Mining and Natural Language Processing with Pandas\n","#### Text mining and natural language processing (NLP) are essential techniques for extracting insights and meaning from textual data. Pandas provides useful functionalities for handling and analyzing text data, making it a valuable tool for text mining and NLP tasks.\n","\n","## 66.1 Introduction to Text Mining and NLP\n","#### Text mining involves extracting useful information, patterns, and insights from unstructured text data. NLP, on the other hand, focuses on understanding and interpreting human language by applying computational techniques and algorithms. These techniques are widely used in various applications, including sentiment analysis, text classification, topic modeling, and information retrieval.\n","\n","## 66.2 Preprocessing Text Data with Pandas\n","#### Before performing any text mining or NLP tasks, it is crucial to preprocess the text data to ensure consistency and improve the quality of the analysis. Pandas provides several useful methods for preprocessing text data, including:\n","\n","#### **Lowercasing** : Convert all text to lowercase to ensure case-insensitive analysis.\n","#### **Tokenization**: Split text into individual words or tokens.\n","#### **Stopword Removal**: Eliminate common words (e.g., \"the\", \"is\", \"and\") that do not carry significant meaning.\n","#### **Stemming and Lemmatization**: Reduce words to their base form (e.g., \"running\" to \"run\") to handle variations of the same word.\n","#### **Removing Punctuation**: Eliminate punctuation marks from the text.\n","#### **Removing HTML Tags**: Strip HTML tags from text if working with web data.\n","#### Here's an example of how to preprocess text data using Pandas:\n","\n","```python\n","import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","\n","# Create a DataFrame with text data\n","data = pd.DataFrame({'text': ['This is a sample sentence.',\n","                              'It contains multiple words.',\n","                              'Preprocessing is important!']})\n","\n","# Lowercasing\n","data['text'] = data['text'].str.lower()\n","\n","# Tokenization\n","data['tokens'] = data['text'].apply(lambda x: re.findall(r'\\w+', x))\n","\n","# Stopword Removal\n","stopwords_list = stopwords.words('english')\n","data['tokens'] = data['tokens'].apply(lambda x: [word for word in x if word not in stopwords_list])\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","data['stemmed_tokens'] = data['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n","```\n","\n","\n","\n","#### In this example, we create a DataFrame called data that contains a column named text with some sample sentences. We preprocess the text by first converting it to lowercase using the lower() method. Then, we tokenize the text using regular expressions to split it into individual words or tokens. Next, we remove stopwords using the NLTK library's predefined stopwords list. Finally, we apply stemming using the Porter stemming algorithm from the NLTK library.\n","\n","## 66.3 Text Analysis with Pandas\n","#### Once the text data is preprocessed, Pandas provides powerful functionalities for analyzing and extracting insights from the text. Some common text analysis tasks using Pandas include:\n","\n","#### **Word Frequency Analysis**: Count the frequency of words in the text to identify important or frequently occurring terms.\n","#### **Sentiment Analysis**: Determine the sentiment or polarity of the text to understand the overall opinion or emotion expressed.\n","#### **Topic Modeling**: Discover latent topics or themes within the text using techniques such as Latent Dirichlet Allocation (LDA).\n","#### **Named Entity Recognition**: Identify and extract named entities (e.g., persons, organizations) from the text.\n","#### **Text Classification**: Categorize or classify text documents into predefined categories or classes.\n","#### These tasks can be performed using various Pandas methods, such as value_counts() for word frequency analysis, machine learning algorithms for sentiment analysis or text classification, and topic modeling libraries like Gensim for topic modeling.\n","\n","## 66.4 Conclusion\n","#### Text mining and NLP are powerful techniques for extracting insights and understanding textual data. Pandas provides a range of functionalities for preprocessing, analyzing, and extracting meaningful information from text data. By leveraging Pandas' capabilities along with other NLP libraries and techniques, you can unlock the potential of text data and gain valuable insights from unstructured text sources. Whether it's analyzing customer reviews, social media data, or scientific articles, Pandas can be a valuable tool for text mining and NLP tasks."]},{"cell_type":"markdown","id":"9450d3a3","metadata":{"papermill":{"duration":0.053253,"end_time":"2023-08-27T14:38:17.2783","exception":false,"start_time":"2023-08-27T14:38:17.225047","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>67</font></h1>\n"]},{"cell_type":"markdown","id":"37fb32cf","metadata":{"papermill":{"duration":0.053153,"end_time":"2023-08-27T14:38:17.384234","exception":false,"start_time":"2023-08-27T14:38:17.331081","status":"completed"},"tags":[]},"source":["# Chapter 67 Handling Time Series Anomalies\n","## Time series data often contain anomalies or outliers that deviate significantly from the expected patterns. Detecting and handling these anomalies is crucial for accurate analysis and forecasting. Pandas provides several techniques and methods to identify and handle time series anomalies effectively.\n","\n","## 67.1 Introduction to Time Series Anomalies\n","#### Time series anomalies refer to observations that deviate significantly from the expected behavior or pattern of the data. These anomalies can occur due to various reasons, such as measurement errors, system malfunctions, or unusual events. Detecting and handling anomalies is essential for maintaining data quality and ensuring reliable analysis.\n","\n","## 67.2 Anomaly Detection Techniques with Pandas\n","#### Pandas offers several methods and techniques to identify time series anomalies:\n","\n","#### **Descriptive Statistics**: Calculate summary statistics such as mean, standard deviation, and percentiles to identify data points that fall outside the expected range.\n","#### **Moving Average**: Smooth the time series data using moving average techniques and compare the original data points with the smoothed values to detect anomalies.\n","#### **Z-Score** : Calculate the z-score of each data point by measuring its deviation from the mean in terms of standard deviations. Data points with high z-scores indicate potential anomalies.\n","#### **Rolling Window Functions** : Apply rolling window functions such as rolling mean, rolling standard deviation, or rolling median to identify data points that deviate significantly from the local window.\n","#### **Statistical Models** : Utilize statistical models like ARIMA, SARIMA, or Prophet to forecast the time series data and identify data points with significant differences between the actual and predicted values.\n","#### Here's an example of how to detect anomalies in a time series using Pandas:\n","\n","```python\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Create a DataFrame with time series data\n","data = pd.DataFrame({'date': pd.date_range(start='1/1/2020', periods=100),\n","                     'value': np.random.normal(0, 1, 100)})\n","\n","# Calculate z-score\n","data['z_score'] = (data['value'] - data['value'].mean()) / data['value'].std()\n","\n","# Set anomaly threshold\n","threshold = 3\n","\n","# Identify anomalies\n","anomalies = data[data['z_score'].abs() > threshold]\n","\n","# Plot the time series data with anomalies\n","plt.plot(data['date'], data['value'], label='Time Series')\n","plt.scatter(anomalies['date'], anomalies['value'], color='red', label='Anomalies')\n","plt.legend()\n","plt.show()\n","````\n","\n","\n","#### In this example, we generate a time series dataset in the DataFrame data with random values. We then calculate the z-score for each data point based on its deviation from the mean. We set a threshold value and identify the anomalies by selecting the data points with z-scores exceeding the threshold. Finally, we visualize the time series data with the detected anomalies.\n","\n","## 67.3 Handling Time Series Anomalies\n","#### Once the anomalies are identified, there are several approaches to handle them:\n","\n","#### **Removing Anomalies** : If the anomalies are due to data errors or outliers, they can be removed from the dataset to avoid distorting the analysis. However, caution should be exercised to ensure that genuine anomalies are not discarded.\n","#### **Imputation** : Anomalies can be imputed or replaced with estimated values based on surrounding data points or statistical techniques. This approach helps maintain the overall integrity of the time series.\n","#### **Flagging or Labeling** : Anomalies can be flagged or labeled separately to distinguish them from the regular data points. This allows for further analysis or modeling specifically targeting the anomalies.\n","#### **Model-based Approaches** : Advanced techniques like anomaly detection algorithms or machine learning models can be employed to automatically detect and handle anomalies based on historical patterns and relationships in the data.\n","#### The choice of handling method depends on the specific context and requirements of the analysis or application.\n","\n","## 67.4 Conclusion\n","#### Handling time series anomalies is crucial for ensuring the accuracy and reliability of analysis and forecasting tasks. Pandas provides a range of techniques and methods to detect and handle anomalies in time series data. By utilizing descriptive statistics, rolling window functions, statistical models, and other approaches, you can effectively identify and address anomalies, maintaining the integrity of your time series analysis."]},{"cell_type":"markdown","id":"0bbe120c","metadata":{"papermill":{"duration":0.052337,"end_time":"2023-08-27T14:38:17.490687","exception":false,"start_time":"2023-08-27T14:38:17.43835","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>68</font></h1>\n"]},{"cell_type":"markdown","id":"ff5d574b","metadata":{"papermill":{"duration":0.053095,"end_time":"2023-08-27T14:38:17.596934","exception":false,"start_time":"2023-08-27T14:38:17.543839","status":"completed"},"tags":[]},"source":["# Chapter 68 Data Reshaping: Melting and Pivoting\n","#### Data reshaping involves transforming the structure of a DataFrame to make it more suitable for analysis or visualization. Pandas provides powerful functions for data reshaping, including melting and pivoting, which allow you to convert data between different formats and representations.\n","\n","## 68.1 Introduction to Data Reshaping\n","#### Data reshaping involves changing the layout or structure of the data to facilitate analysis, visualization, or further processing. It often involves transforming data from a wide format to a long format or vice versa.\n","\n","## Two common operations in data reshaping are melting and pivoting:\n","\n","#### **Melting**: Melting a DataFrame involves transforming it from a wide format, where each column represents a variable, into a long format, where each variable is stacked as a single column, and a new column is added to distinguish the original variable. This operation is useful when you have multiple columns that represent different instances of the same variable and you want to consolidate them into a single column.\n","\n","#### **Pivoting** : Pivoting a DataFrame involves transforming it from a long format, where each observation is represented by a separate row, into a wide format, where the values of a particular variable are spread across multiple columns. This operation is useful when you want to reorganize your data to have a clearer view of the relationships between variables.\n","\n","## 68.2 Melting DataFrames\n","#### Melting a DataFrame in Pandas can be done using the melt() function. The function takes the following parameters:\n","\n","#### **id_vars**: A list of column names to keep as identifier variables (variables that will remain as columns after melting).\n","#### **value_vars**: A list of column names to melt into a single column. If not specified, all remaining columns will be melted.\n","#### **var_name** : The name of the new column that will contain the variable names after melting.\n","#### **value_name** : The name of the new column that will contain the values after melting.\n","#### Here's an example of how to melt a DataFrame:\n","\n","```python\n","import pandas as pd\n","\n","# Create a DataFrame\n","data = pd.DataFrame({'Name': ['John', 'Emma'],\n","                     'Maths': [85, 90],\n","                     'Science': [92, 88],\n","                     'History': [78, 85]})\n","\n","# Melting the DataFrame\n","melted_data = pd.melt(data, id_vars='Name', var_name='Subject', value_name='Score')\n","\n","print(melted_data)\n","Output:\n","\n","Copy code\n","   Name  Subject  Score\n","0  John    Maths     85\n","1  Emma    Maths     90\n","2  John  Science     92\n","3  Emma  Science     88\n","4  John  History     78\n","5  Emma  History     85\n","```\n","\n","#### In this example, we have a DataFrame with student names and their scores in different subjects. By using the melt() function, we melt the DataFrame and create a new DataFrame where each row represents a specific student-subject combination, with columns for Name, Subject, and Score.\n","\n","## 68.3 Pivoting DataFrames\n","#### Pivoting a DataFrame in Pandas can be done using the pivot() function. The function takes the following parameters:\n","\n","#### **index** : The column(s) to use as the index for the pivoted DataFrame.\n","#### **columns** : The column(s) to use as the new columns in the pivoted DataFrame.\n","#### **values** : The column(s) to fill the pivoted DataFrame values.\n","#### Here's an example of how to pivot a DataFrame:\n","\n","```python\n","import pandas as pd\n","\n","# Create a DataFrame\n","data = pd.DataFrame({'Name': ['John', 'Emma'],\n","                     'Subject': ['Maths', 'Science'],\n","                     'Score': [85, 92]})\n","\n","# Pivoting the DataFrame\n","pivoted_data = data.pivot(index='Name', columns='Subject', values='Score')\n","\n","print(pivoted_data)\n","```\n","\n","#### In this example, we have a DataFrame with student names, subjects, and scores. By using the pivot() function, we pivot the DataFrame and create a new DataFrame where each unique Name becomes an index, each unique Subject becomes a column, and the corresponding Score values are filled in.\n","\n","## 68.4 Conclusion\n","#### Data reshaping is an important step in the data analysis process, and Pandas provides powerful functions like melt() and pivot() to facilitate this task. By understanding how to melt a DataFrame to convert it from wide to long format, and how to pivot a DataFrame to convert it from long to wide format, you can effectively reshape your data to suit your analysis needs. These operations enable you to transform and restructure your data for better visualization, exploration, and modeling."]},{"cell_type":"markdown","id":"d33c8a13","metadata":{"papermill":{"duration":0.053891,"end_time":"2023-08-27T14:38:17.705337","exception":false,"start_time":"2023-08-27T14:38:17.651446","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>69</font></h1>\n"]},{"cell_type":"markdown","id":"d4bd9503","metadata":{"papermill":{"duration":0.053466,"end_time":"2023-08-27T14:38:17.812948","exception":false,"start_time":"2023-08-27T14:38:17.759482","status":"completed"},"tags":[]},"source":["# Chapter 69 Advanced Grouping Techniques: Groupby and Filter\n","#### In data analysis, grouping is a fundamental operation that allows you to split the data into groups based on one or more variables, and then perform calculations or transformations within each group. Pandas provides the groupby() function to facilitate grouping operations, and you can further enhance the grouping capabilities by combining it with filtering techniques.\n","\n","## 69.1 Introduction to Grouping and Filtering\n","#### Grouping data involves splitting a DataFrame into groups based on unique values in one or more columns. Once the data is grouped, you can perform aggregate operations on each group, such as calculating the sum, mean, or count of values within each group.\n","\n","#### Filtering, on the other hand, involves selecting subsets of data based on certain conditions. You can filter a DataFrame to include only rows that meet specific criteria, such as selecting rows with values above a certain threshold or rows that satisfy a particular logical condition.\n","\n","#### By combining grouping and filtering techniques, you can perform advanced data analysis tasks that involve applying group-wise operations to subsets of the data.\n","\n","## 69.2 Grouping and Aggregating Data\n","#### The groupby() function in Pandas allows you to group a DataFrame based on one or more columns. After grouping, you can apply aggregate functions to calculate summary statistics for each group.\n","\n","## Here's an example:\n","\n","```python\n","import pandas as pd\n","\n","# Create a DataFrame\n","data = pd.DataFrame({'Category': ['A', 'A', 'B', 'B', 'A'],\n","                     'Value': [10, 20, 15, 25, 30]})\n","\n","# Group the data by the 'Category' column\n","grouped_data = data.groupby('Category')\n","\n","# Calculate the mean value for each group\n","mean_value = grouped_data['Value'].mean()\n","\n","print(mean_value)\n","```\n","\n","#### In this example, we have a DataFrame with two columns: 'Category' and 'Value'. We group the data by the 'Category' column using the groupby() function and then calculate the mean value of the 'Value' column for each group using the mean() function.\n","\n","## 69.3 Filtering Data within Groups\n","#### After grouping a DataFrame, you can apply filtering techniques to select specific subsets of data within each group. Pandas provides the filter() function, which allows you to define a custom filtering function to apply to each group.\n","\n","## Here's an example:\n","\n","```python\n","import pandas as pd\n","\n","# Create a DataFrame\n","data = pd.DataFrame({'Category': ['A', 'A', 'B', 'B', 'A'],\n","                     'Value': [10, 20, 15, 25, 30]})\n","\n","# Group the data by the 'Category' column\n","grouped_data = data.groupby('Category')\n","\n","# Define a custom filtering function\n","def filter_func(x):\n","    return x['Value'].sum() > 30\n","\n","# Apply the custom filter function to each group\n","filtered_data = grouped_data.filter(filter_func)\n","\n","print(filtered_data)\n","```\n","\n","\n","#### In this example, we group the data by the 'Category' column and define a custom filtering function filter_func() that checks if the sum of values in the 'Value' column for each group is greater than 30. The filter() function applies this custom filter function to each group and returns only the rows that satisfy the filter condition.\n","\n","## 69.4 Conclusion\n","#### Combining grouping and filtering techniques allows you to perform advanced data analysis tasks with Pandas. By grouping data based on one or more columns and then applying aggregate functions or custom filters within each group, you can gain valuable insights from your data. These techniques enable you to perform group-wise calculations, identify subsets of data that meet\n","#### specific criteria, and extract meaningful information from your datasets."]},{"cell_type":"markdown","id":"9f429f68","metadata":{"papermill":{"duration":0.053649,"end_time":"2023-08-27T14:38:17.919361","exception":false,"start_time":"2023-08-27T14:38:17.865712","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>70</font></h1>\n"]},{"cell_type":"markdown","id":"fdae93c9","metadata":{"papermill":{"duration":0.053516,"end_time":"2023-08-27T14:38:18.026018","exception":false,"start_time":"2023-08-27T14:38:17.972502","status":"completed"},"tags":[]},"source":["# Chapter  70 Statistical Analysis with Pandas\n","#### Pandas provides a wide range of statistical functions that enable you to perform various statistical analyses on your data. These functions allow you to calculate descriptive statistics, measure correlation between variables, conduct hypothesis testing, and more. In this chapter, we will explore some of the key statistical analysis capabilities of Pandas.\n","\n","## 70.1 Descriptive Statistics\n","#### Descriptive statistics provide summary information about the distribution, central tendency, and spread of a dataset. Pandas offers a comprehensive set of functions to compute descriptive statistics for numerical data.\n","\n","## Here are a few examples:\n","\n","```python\n","import pandas as pd\n","\n","# Create a DataFrame\n","data = pd.DataFrame({'A': [1, 2, 3, 4, 5],\n","                     'B': [5, 4, 3, 2, 1]})\n","\n","# Calculate the mean of each column\n","mean_values = data.mean()\n","\n","# Calculate the median of each column\n","median_values = data.median()\n","\n","# Calculate the standard deviation of each column\n","std_values = data.std()\n","\n","print(mean_values)\n","print(median_values)\n","print(std_values)\n","```\n","\n","#### In this example, we create a DataFrame with two columns, 'A' and 'B', and calculate the mean, median, and standard deviation for each column using the mean(), median(), and std() functions, respectively.\n","\n","## 70.2 Correlation Analysis\n","#### Correlation analysis measures the strength and direction of the linear relationship between two variables. Pandas provides the corr() function to compute the correlation matrix for a DataFrame.\n","\n","### Here's an example:\n","\n","```python\n","import pandas as pd\n","\n","# Create a DataFrame\n","data = pd.DataFrame({'A': [1, 2, 3, 4, 5],\n","                     'B': [5, 4, 3, 2, 1]})\n","\n","# Compute the correlation matrix\n","correlation_matrix = data.corr()\n","\n","print(correlation_matrix)\n","```\n","\n","### In this example, we create a DataFrame with two columns, 'A' and 'B', and compute the correlation matrix using the corr() function. The resulting matrix shows the correlation coefficients between each pair of columns.\n","\n","## 70.3 Hypothesis Testing\n","#### Hypothesis testing allows you to make inferences about population parameters based on sample data. Pandas integrates with the SciPy library to provide a wide range of statistical tests for hypothesis testing.\n","\n","#### Here's an example of performing a t-test:\n","\n","```python\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","\n","# Create two samples\n","sample1 = [1, 2, 3, 4, 5]\n","sample2 = [6, 7, 8, 9, 10]\n","\n","# Perform a t-test\n","t_statistic, p_value = ttest_ind(sample1, sample2)\n","\n","print(\"T-Statistic:\", t_statistic)\n","print(\"P-Value:\", p_value)\n","```\n","\n","#### In this example, we have two samples, sample1 and sample2, and we perform an independent t-test using the ttest_ind() function from SciPy. The resulting t-statistic and p-value provide insights into the significance of the difference between the two samples.\n","\n","#### Pandas also supports other statistical tests such as chi-square test, ANOVA, and more through integration with the SciPy library.\n","\n","#### these are just a few examples of the statistical analysis capabilities offered by Pandas. With its integration with other libraries like NumPy and SciPy, Pandas provides a powerful platform for conducting various statistical analyses on your data.\n","\n","#### In the next chapter, we will explore additional features and functionality of Pandas, including advanced techniques for data manipulation and analysis."]},{"cell_type":"markdown","id":"1de98ef2","metadata":{"papermill":{"duration":0.053343,"end_time":"2023-08-27T14:38:18.132719","exception":false,"start_time":"2023-08-27T14:38:18.079376","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>71</font></h1>\n"]},{"cell_type":"markdown","id":"17b0da73","metadata":{"papermill":{"duration":0.056065,"end_time":"2023-08-27T14:38:18.243409","exception":false,"start_time":"2023-08-27T14:38:18.187344","status":"completed"},"tags":[]},"source":["# Chapter 71 Handling Streaming Data with Pandas\n","#### Pandas is not only capable of handling static datasets, but it can also handle streaming data in real-time. Streaming data refers to data that is continuously generated and processed in a sequential manner. This chapter will explore how you can use Pandas to handle streaming data efficiently.\n","\n","## 71.1 Reading Streaming Data\n","#### To read streaming data into Pandas, you can use the read_csv() function with additional parameters to specify the chunk size or the number of rows to read at a time. By doing so, you can process the data in smaller chunks and avoid memory issues.\n","\n","## Here's an example:\n","\n","```python\n","import pandas as pd\n","\n","# Read streaming data from a CSV file\n","stream = pd.read_csv('streaming_data.csv', chunksize=1000)\n","\n","for chunk in stream:\n","    # Process the chunk of data\n","    # Perform data manipulation, analysis, etc.\n","    print(chunk.head())\n"," ```\n","    \n","#### In this example, we read the streaming data from a CSV file using the read_csv() function with the chunksize parameter set to 1000. We then iterate over each chunk of data and perform the desired operations.\n","\n","## 71.2 Real-time Data Processing\n","#### Pandas also provides functionality to process and analyze streaming data in real-time. You can continuously update your analysis as new data arrives.\n","\n","#### Here's an example:\n","\n","```python\n","import pandas as pd\n","\n","# Initialize an empty DataFrame\n","stream_data = pd.DataFrame(columns=['timestamp', 'value'])\n","\n","# Continuously update the DataFrame with new data\n","while True:\n","    # Receive new data from a data source\n","    new_data = receive_data()\n","\n","    # Append new data to the DataFrame\n","    stream_data = stream_data.append(new_data, ignore_index=True)\n","\n","    # Perform real-time analysis on the updated DataFrame\n","    # Calculate statistics, generate visualizations, etc.\n","\n","    # Pause for a given interval\n","    time.sleep(1)\n","    \n","```\n","    \n","    \n","#### In this example, we continuously update the DataFrame stream_data with new data received from a data source. We then perform real-time analysis on the updated DataFrame, which may include calculations, visualizations, or any other operations you need.\n","\n","## 71.3 Streaming Data with External Libraries\n","#### In addition to native Pandas functionality, you can also combine Pandas with external libraries like Apache Kafka, Apache Spark, or Dask to handle and process streaming data efficiently. These libraries provide powerful capabilities for distributed computing and real-time processing.\n","\n","### By leveraging the strengths of these libraries alongside Pandas, you can handle large-scale streaming data with ease and perform complex data analysis tasks.\n","\n","#### In the next chapter, we will explore time series forecasting \n","#### techniques using Pandas, which is a crucial aspect of analyzing time-dependent data."]},{"cell_type":"markdown","id":"da04d60a","metadata":{"papermill":{"duration":0.053846,"end_time":"2023-08-27T14:38:18.353011","exception":false,"start_time":"2023-08-27T14:38:18.299165","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>72</font></h1>\n"]},{"cell_type":"markdown","id":"a037f322","metadata":{"papermill":{"duration":0.055017,"end_time":"2023-08-27T14:38:18.469153","exception":false,"start_time":"2023-08-27T14:38:18.414136","status":"completed"},"tags":[]},"source":["# Chapter 72 Time Series Forecasting with Pandas\n","#### Time series forecasting is a key area of analysis when working with data that is dependent on time. Pandas provides powerful tools and functions for time series analysis and forecasting. This chapter will introduce you to some of the essential techniques and functionalities in Pandas for time series forecasting.\n","\n","## 72.1 Time Series Data\n","#### Time series data consists of observations collected at different points in time, often at regular intervals. Examples of time series data include stock prices, weather data, economic indicators, and sensor measurements.\n","\n","#### Pandas provides specialized data structures, such as the DateTimeIndex and PeriodIndex, to handle time series data efficiently. These data structures allow for easy indexing, slicing, and manipulation of time-dependent data.\n","\n","#### Here's an example of creating a time series DataFrame in Pandas:\n","\n","```python\n","import pandas as pd\n","\n","# Create a DataFrame with time series data\n","dates = pd.date_range('2023-01-01', periods=365, freq='D')\n","data = pd.DataFrame({'Date': dates, 'Value': range(365)})\n","\n","# Set the 'Date' column as the index\n","data.set_index('Date', inplace=True)\n","\n","print(data.head())\n","```\n","\n","#### In this example, we generate a sequence of dates starting from '2023-01-01' to '2023-12-31' using the pd.date_range() function. We then create a DataFrame data with two columns: 'Date' and 'Value', where 'Value' contains values from 0 to 364. Finally, we set the 'Date' column as the index of the DataFrame using the set_index() function.\n","\n","## 72.2 Time Series Analysis\n","#### Pandas provides a wide range of tools and functions for analyzing time series data. Some of the commonly used techniques include:\n","\n","#### **Resampling** : Pandas allows you to resample time series data to a different frequency, such as converting daily data to monthly or yearly data. This can be achieved using the resample() function.\n","\n","#### **Time shifting** : You can shift the values of a time series forward or backward in time using the shift() function. This can be useful for calculating differences or percentage changes between consecutive observations.\n","\n","#### **Rolling windows**: Pandas provides the rolling() function to calculate rolling statistics on a time series. Rolling statistics involve calculating metrics like the moving average or moving sum over a specified window of time.\n","\n","#### These are just a few examples of the time series analysis capabilities offered by Pandas. Depending on your specific needs, Pandas provides a wide range of tools and functions to handle different aspects of time series data.\n","\n","## 72.3 Time Series Forecasting\n","#### Time series forecasting involves predicting future values based on historical observations. Pandas integrates with popular libraries like scikit-learn and statsmodels to provide powerful time series forecasting capabilities.\n","\n","## Here's an example of using Pandas with the statsmodels library to perform time series forecasting using the ARIMA model:\n","\n","```python\n","import pandas as pd\n","import statsmodels.api as sm\n","\n","# Load time series data\n","data = pd.read_csv('time_series_data.csv', parse_dates=['Date'], index_col='Date')\n","\n","# Perform time series forecasting using ARIMA model\n","model = sm.tsa.ARIMA(data, order=(1, 1, 1))\n","results = model.fit()\n","\n","# Generate future forecasts\n","forecast = results.predict(start='2023-07-01', end='2023-07-31')\n","\n","print(forecast)\n","```\n","\n","#### In this example, we load the time series data from a CSV file into a DataFrame using Pandas. We then use the ARIMA model from the statsmodels library to fit the data and\n","#### obtain the model results. Finally, we generate future forecasts using the predict() method of the model.\n","\n","#### Pandas also supports other forecasting techniques, such as exponential smoothing, seasonal decomposition, and machine learning algorithms, through integration with external libraries.\n","\n","#### Time series forecasting is a vast and specialized field, and this chapter only scratches the surface of what is possible with Pandas. For more advanced forecasting techniques and applications, further exploration and study are recommended."]},{"cell_type":"markdown","id":"f08cfe25","metadata":{"papermill":{"duration":0.102756,"end_time":"2023-08-27T14:38:18.626232","exception":false,"start_time":"2023-08-27T14:38:18.523476","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>73</font></h1>\n"]},{"cell_type":"markdown","id":"9da60f8d","metadata":{"papermill":{"duration":0.053134,"end_time":"2023-08-27T14:38:18.73298","exception":false,"start_time":"2023-08-27T14:38:18.679846","status":"completed"},"tags":[]},"source":["# Chapter 73 Advanced Data Cleaning Techniques\n","#### Data cleaning is a crucial step in the data analysis process to ensure accurate and reliable results. While Pandas provides basic data cleaning functionalities, there are advanced techniques that can be applied to handle more complex cleaning tasks. This chapter explores some advanced data cleaning techniques in Pandas.\n","\n","## 73.1 Handling Missing Values\n","#### Missing values are a common issue in datasets and can significantly affect the analysis if not handled properly. Pandas provides various methods to handle missing values, such as dropna(), fillna(), and interpolate(). However, there are additional techniques you can apply to deal with missing values more effectively.\n","\n","#### **Imputation** : Imputation involves filling missing values with estimated values based on the available data. Pandas provides several imputation techniques, such as mean imputation, median imputation, and forward/backward filling.\n","```python\n","# Mean imputation\n","df['column'].fillna(df['column'].mean(), inplace=True)\n","\n","# Forward fill\n","df['column'].fillna(method='ffill', inplace=True)\n","\n","# Backward fill\n","df['column'].fillna(method='bfill', inplace=True)\n","```\n","\n","#### **Multiple Imputation** : Multiple imputation is a technique that generates multiple plausible imputations to capture the uncertainty associated with missing values. The fancyimpute library integrates with Pandas and provides advanced imputation methods like MICE (Multiple Imputation by Chained Equations).\n","\n","\n","```python\n","from fancyimpute import IterativeImputer\n","\n","imputer = IterativeImputer()\n","df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n","```\n","\n","#### **Interpolation** : Interpolation is another technique to fill missing values by estimating them based on the existing values. Pandas provides the interpolate() method, which uses different interpolation algorithms, such as linear, polynomial, and spline.\n","\n","```python\n","\n","# Linear interpolation\n","df['column'].interpolate(method='linear', inplace=True)\n","\n","# Spline interpolation\n","df['column'].interpolate(method='spline', order=3, inplace=True)\n","\n","```\n","\n","#### These are just a few examples of advanced techniques to handle missing values. The choice of technique depends on the nature of the data and the specific requirements of your analysis.\n","\n","## 73.2 Outlier Detection and Treatment\n","#### Outliers are data points that deviate significantly from the rest of the data and can affect the statistical analysis. Identifying and handling outliers is an essential step in data cleaning. Pandas provides methods like describe() and boxplot() to detect outliers visually and statistically.\n","\n","```python\n","# Detect outliers using descriptive statistics\n","stats = df['column'].describe()\n","lower_bound = stats['25%'] - 1.5 * (stats['75%'] - stats['25%'])\n","upper_bound = stats['75%'] + 1.5 * (stats['75%'] - stats['25%'])\n","\n","outliers = df[(df['column'] < lower_bound) | (df['column'] > upper_bound)]\n","\n","# Remove outliers\n","df = df[(df['column'] >= lower_bound) & (df['column'] <= upper_bound)]\n","```\n","\n","#### Other techniques for outlier detection include z-score method, Mahalanobis distance, and clustering-based methods.\n","\n","## 73.3 Handling Inconsistent Data\n","#### Inconsistent data refers to data that violates the expected structure or rules. It can include inconsistent formats, inconsistent units of measurement, or inconsistent categorical values. Pandas provides functions like str.replace(), str.extract(), and str.contains() to handle inconsistent data based on pattern matching and regular expressions.\n","\n","```python\n","#Replace inconsistent formats\n","\n","df['column'] = df['column'].str.replace('USD', '$')\n","\n","df['column'] = df['column'].str.replace(',', '')\n","\n","# Extract information from inconsistent formats\n","df['column'] = df['column'].str.extract(r'(\\d+)').astype(int)\n","\n","# Filter rows based on inconsistent values\n","filtered_df = df[df['column'].str.contains('pattern')]\n","````\n","\n","\n","#### Regular expressions are powerful tools for handling inconsistent data patterns and extracting meaningful information.\n","\n","## 73.4 Data Transformation and Scaling\n","#### Data transformation involves converting the data to a different scale or distribution. Pandas provides functions like apply(), map(), and replace() to perform custom transformations on columns or specific values.\n","\n","```python \n","# Apply a custom transformation to a column\n","df['column'] = df['column'].apply(lambda x: custom_transform(x))\n","\n","# Map values to new values\n","df['column'] = df['column'].map({1: 'A', 2: 'B', 3: 'C'})\n","\n","# Replace values with new values\n","df['column'] = df['column'].replace({'old_value': 'new_value'})\n","```\n","\n","\n","#### Data scaling is another transformation technique that brings data to a standard scale, such as normalization or standardization. Pandas integrates with libraries like scikit-learn to apply scaling techniques.\n","\n","```python\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n","```\n","\n","#### These are some advanced data cleaning techniques you can apply using Pandas. Depending on your specific data and analysis requirements, you may need to combine multiple techniques or explore domain-specific methods for more complex data cleaning tasks."]},{"cell_type":"markdown","id":"d650fb94","metadata":{"papermill":{"duration":0.052963,"end_time":"2023-08-27T14:38:18.839161","exception":false,"start_time":"2023-08-27T14:38:18.786198","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>74</font></h1>\n"]},{"cell_type":"markdown","id":"0bc8a670","metadata":{"papermill":{"duration":0.053699,"end_time":"2023-08-27T14:38:18.946987","exception":false,"start_time":"2023-08-27T14:38:18.893288","status":"completed"},"tags":[]},"source":["# Chapter 74 Regular Expressions in Pandas\n","#### Regular expressions are powerful tools for pattern matching and text manipulation. Pandas integrates with regular expressions to provide advanced data processing capabilities. This chapter explores how to use regular expressions in Pandas for tasks like pattern matching, string manipulation, and data extraction.\n","\n","## 74.1 Pattern Matching\n","### Pattern matching involves searching for specific patterns within text data. Pandas provides the str.contains() and str.match() methods to perform pattern matching on DataFrame columns.\n","\n","### Here's an example:\n","\n","```python\n","import pandas as pd\n","\n","# Create a DataFrame with text data\n","data = {'Text': ['Hello', '1234', 'ABC123', 'def']}\n","df = pd.DataFrame(data)\n","\n","# Check if a pattern is present in the 'Text' column\n","pattern = r'\\d+'\n","matches = df['Text'].str.contains(pattern)\n","\n","print(df[matches])\n","```\n","\n","\n","#### In this example, we create a DataFrame df with a 'Text' column containing different text values. We define a pattern \\d+ to match one or more digits. We then use the str.contains() method to check if the pattern is present in each row of the 'Text' column. Finally, we use the boolean mask matches to filter and print the rows that match the pattern.\n","\n","## 74.2 String Manipulation\n","#### Regular expressions can be used for string manipulation tasks like replacing, splitting, and extracting substrings. Pandas provides the str.replace(), str.split(), and str.extract() methods to perform these operations on DataFrame columns.\n","\n","#### Here's an example:\n","\n","```python\n","import pandas as pd\n","\n","# Create a DataFrame with text data\n","data = {'Text': ['Hello, World!', '123-456-7890', 'ABC-123', 'def']}\n","df = pd.DataFrame(data)\n","\n","# Replace characters in the 'Text' column\n","df['Text'] = df['Text'].str.replace(r'[^\\w\\s]', '')\n","\n","# Split the 'Text' column into multiple columns\n","df[['Word1', 'Word2']] = df['Text'].str.split(',', expand=True)\n","\n","# Extract digits from the 'Text' column\n","df['Digits'] = df['Text'].str.extract(r'(\\d+)')\n","\n","print(df)\n","```\n","\n","#### In this example, we create a DataFrame df with a 'Text' column containing different text values. We use the str.replace() method to remove non-alphanumeric characters from the 'Text' column. We then use the str.split() method to split the 'Text' column by comma and expand it into multiple columns. Finally, we use the str.extract() method to extract digits from the 'Text' column and create a new 'Digits' column.\n","\n","## 74.3 Data Extraction\n","#### Regular expressions can be used to extract specific data patterns from text data. Pandas provides the str.extract() method to extract data based on regular expressions.\n","\n","#### Here's an example:\n","\n","```python\n","import pandas as pd\n","\n","# Create a DataFrame with text data\n","data = {'Text': ['John, 25', 'Jane, 30', 'Mark, 35']}\n","df = pd.DataFrame(data)\n","\n","# Extract names and ages from the 'Text' column\n","df[['Name', 'Age']] = df['Text'].str.extract(r'(\\w+),\\s(\\d+)')\n","\n","print(df)\n","```\n","\n","\n","#### In this example, we create a DataFrame df with a 'Text' column containing text values in the format 'Name, Age'. We use the str.extract() method with the regular expression r'(\\w+),\\s(\\d+)' to extract the name and age patterns from the 'Text' column. We then create new 'Name' and 'Age' columns to store the extracted data.\n","\n","### Regular expressions provide a flexible and powerful way to work with text data in Pandas. By combining regular expressions with Pandas' string methods, you can perform advanced text processing and data extraction tasks."]},{"cell_type":"markdown","id":"c528e15a","metadata":{"papermill":{"duration":0.052946,"end_time":"2023-08-27T14:38:19.054875","exception":false,"start_time":"2023-08-27T14:38:19.001929","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>75</font></h1>\n"]},{"cell_type":"markdown","id":"b398edda","metadata":{"papermill":{"duration":0.053739,"end_time":"2023-08-27T14:38:19.161912","exception":false,"start_time":"2023-08-27T14:38:19.108173","status":"completed"},"tags":[]},"source":["# Chapter 75  Handling Encrypted Data\n","#### Handling encrypted data is crucial when working with sensitive information that needs to be protected from unauthorized access. While Pandas does not provide built-in encryption functionalities, it can be integrated with external libraries to handle encrypted data effectively.\n","\n","## Here are some approaches to handling encrypted data using Pandas:\n","\n","#### **Encrypting and Decrypting Data** : You can use external encryption libraries, such as cryptography or pycryptodome, to encrypt and decrypt data. Before reading data into Pandas, you would decrypt the encrypted data using appropriate decryption techniques. Once decrypted, the data can be loaded into a Pandas DataFrame for further analysis.\n","\n","#### **Secure Data Transmission** : If you need to transmit encrypted data, you can use secure communication protocols such as HTTPS or SSH to ensure that the data is encrypted during transmission. Once the encrypted data is received, you can decrypt it using appropriate decryption techniques and load it into a Pandas DataFrame.\n","\n","#### **Working with Encrypted Databases** : If your data is stored in an encrypted database, you can establish a connection to the database using encryption protocols and retrieve the encrypted data. After decrypting the data, you can load it into a Pandas DataFrame for analysis.\n","\n","#### **Encrypted File Formats** : If you are working with encrypted files, you can use libraries like pyAesCrypt or cryptography to decrypt the file before loading it into Pandas. Once decrypted, the data can be read into a DataFrame for further processing.\n","\n","#### It's important to note that encryption and decryption techniques depend on the specific encryption algorithms and protocols being used. Therefore, it's recommended to consult the documentation of the encryption library or framework you are using for detailed instructions on encryption and decryption procedures.\n","\n","#### Remember to follow best practices for encryption, such as using strong encryption algorithms, managing encryption keys securely, and adhering to any compliance requirements specific to your industry or application.\n","\n","#### These were some techniques for handling encrypted data in conjunction with Pandas. By integrating Pandas with appropriate encryption libraries or protocols, you can ensure the security and confidentiality of sensitive data throughout your data analysis workflow."]},{"cell_type":"markdown","id":"ca1fa20a","metadata":{"papermill":{"duration":0.053016,"end_time":"2023-08-27T14:38:19.268304","exception":false,"start_time":"2023-08-27T14:38:19.215288","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>76</font></h1>\n"]},{"cell_type":"markdown","id":"16bec3cb","metadata":{"papermill":{"duration":0.053843,"end_time":"2023-08-27T14:38:19.375552","exception":false,"start_time":"2023-08-27T14:38:19.321709","status":"completed"},"tags":[]},"source":["# Chapter 76 Handling Encrypted Data\n","#### Handling encrypted data is crucial when working with sensitive information that needs to be protected from unauthorized access. While Pandas does not provide built-in encryption functionalities, it can be integrated with external libraries to handle encrypted data effectively.\n","\n","#### Here are some approaches to handling encrypted data using Pandas:\n","\n","#### **Encrypting and Decrypting Data** : You can use external encryption libraries, such as cryptography or pycryptodome, to encrypt and decrypt data. Before reading data into Pandas, you would decrypt the encrypted data using appropriate decryption techniques. Once decrypted, the data can be loaded into a Pandas DataFrame for further analysis.\n","\n","#### **Secure Data Transmission** : If you need to transmit encrypted data, you can use secure communication protocols such as HTTPS or SSH to ensure that the data is encrypted during transmission. Once the encrypted data is received, you can decrypt it using appropriate decryption techniques and load it into a Pandas DataFrame.\n","\n","#### **Working with Encrypted Databases** : If your data is stored in an encrypted database, you can establish a connection to the database using encryption protocols and retrieve the encrypted data. After decrypting the data, you can load it into a Pandas DataFrame for analysis.\n","\n","#### **Encrypted File Formats** : If you are working with encrypted files, you can use libraries like pyAesCrypt or cryptography to decrypt the file before loading it into Pandas. Once decrypted, the data can be read into a DataFrame for further processing.\n","\n","#### It's important to note that encryption and decryption techniques depend on the specific encryption algorithms and protocols being used. Therefore, it's recommended to consult the documentation of the encryption library or framework you are using for detailed instructions on encryption and decryption procedures.\n","\n","#### Remember to follow best practices for encryption, such as using strong encryption algorithms, managing encryption keys securely, and adhering to any compliance requirements specific to your industry or application."]},{"cell_type":"markdown","id":"b1d04af5","metadata":{"papermill":{"duration":0.053599,"end_time":"2023-08-27T14:38:19.48203","exception":false,"start_time":"2023-08-27T14:38:19.428431","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>77</font></h1>\n"]},{"cell_type":"markdown","id":"b76608c5","metadata":{"papermill":{"duration":0.052743,"end_time":"2023-08-27T14:38:19.588212","exception":false,"start_time":"2023-08-27T14:38:19.535469","status":"completed"},"tags":[]},"source":["# Chapter 77 Data Sampling Techniques\n","#### Data sampling techniques are used to select a subset of data from a larger dataset for analysis. Sampling helps in reducing computational requirements and processing time when working with large datasets. Pandas provides various sampling techniques that can be applied to DataFrames or Series objects.\n","\n","## Here are some commonly used data sampling techniques in Pandas:\n","\n","#### **Random Sampling**: Random sampling involves selecting data points randomly from the dataset. The sample() method in Pandas can be used to perform random sampling. You can specify the number of samples or the proportion of the dataset to be selected.\n","```python \n","# Randomly sample 10 data points\n","sample_data = df.sample(n=10)\n","\n","# Randomly sample 20% of the dataset\n","sample_data = df.sample(frac=0.2)\n","```\n","\n","\n","#### **Stratified Sampling**: Stratified sampling involves dividing the dataset into subgroups or strata based on a specific feature and then sampling from each stratum. This ensures representation from each subgroup. Pandas provides the groupby() and apply() methods to perform stratified sampling.\n","```python\n","\n","\n","\n","# Perform stratified sampling based on the 'category' column\n","sample_data = df.groupby('category').apply(lambda x: x.sample(n=2))\n","\n","\n","``` \n","#### **Time-based Sampling** : Time-based sampling involves selecting data points based on a specific time interval. Pandas provides the resample() method to perform time-based sampling on time series data.\n","\n","```python\n","# Perform time-based sampling on a time series DataFrame\n","sample_data = df.resample('D').mean()  # Sample daily data\n","```\n","\n","\n","\n","#### **Systematic Sampling**: Systematic sampling involves selecting data points at regular intervals from an ordered dataset. Pandas provides various indexing and slicing techniques to perform systematic sampling.\n","```python\n","# Perform systematic sampling by selecting every 10th data point\n","sample_data = df.iloc[::10]\n","```\n","\n","\n","#### These are just a few examples of data sampling techniques available in Pandas. The choice of sampling technique depends on the specific requirements of your analysis and the nature of your dataset"]},{"cell_type":"markdown","id":"bc0a0d34","metadata":{"papermill":{"duration":0.053178,"end_time":"2023-08-27T14:38:19.696995","exception":false,"start_time":"2023-08-27T14:38:19.643817","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>78</font></h1>\n"]},{"cell_type":"markdown","id":"e2a9ec9a","metadata":{"papermill":{"duration":0.053509,"end_time":"2023-08-27T14:38:19.803611","exception":false,"start_time":"2023-08-27T14:38:19.750102","status":"completed"},"tags":[]},"source":["# Chapter 78 Working with Sparse Time Series Data\n","#### Sparse time series data refers to time series data where most of the observations are missing or not available. Handling sparse time series data efficiently is important for accurate analysis and modeling. Pandas provides functionality to work with sparse time series data using the SparseDataFrame and SparseSeries classes.\n","\n","## Here's an example of working with sparse time series data in Pandas:\n","\n","```python\n","import pandas as pd\n","\n","# Create a sparse time series DataFrame\n","index = pd.date_range(start='2021-01-01', end='2021-12-31', freq='D')\n","data = [1, None, None, 2, 3, None, 4, None, None, 5]\n","s = pd.SparseSeries(data, index=index)\n","df = pd.SparseDataFrame({'Values': s})\n","\n","# Fill missing values with a specific value\n","df_filled = df.fillna(-1)\n","\n","# Forward fill missing values\n","df_ffilled = df.ffill()\n","\n","# Backward fill missing values\n","df_bfilled = df.bfill()\n","```\n","\n","\n","#### In this example, we create a sparse time series DataFrame df with a sparse series s containing some missing values denoted by None. We can fill the missing values using the fillna() method by specifying a specific value, or use the ffill() and bfill() methods to forward fill and backward fill the missing values, respectively.\n","\n","#### Working with sparse time series data can help reduce memory usage and improve computational efficiency, especially when dealing with large datasets. However, it's important to note that not all operations and methods are supported on sparse data. Refer to the Pandas documentation for more information on working with sparse data."]},{"cell_type":"markdown","id":"4a1ac567","metadata":{"papermill":{"duration":0.053139,"end_time":"2023-08-27T14:38:19.910518","exception":false,"start_time":"2023-08-27T14:38:19.857379","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>79</font></h1>\n"]},{"cell_type":"markdown","id":"d9663bb2","metadata":{"papermill":{"duration":0.053616,"end_time":"2023-08-27T14:38:20.018468","exception":false,"start_time":"2023-08-27T14:38:19.964852","status":"completed"},"tags":[]},"source":["# Chapter 79Advanced String Manipulation with Pandas\n","#### Pandas provides a wide range of string manipulation functions and methods that can be applied to string columns in DataFrames or Series. These functions allow you to perform advanced string operations efficiently and effectively.\n","\n","#### Here are some advanced string manipulation techniques in Pandas:\n","\n","#### **String Splitting**: You can split strings based on a specific delimiter using the str.split() method. This can be useful when you want to extract specific parts of a string.\n","\n","\n","```python\n","# Split a string column into multiple columns based on a delimiter\n","df[['First Name', 'Last Name']] = df['Name'].str.split(', ', expand=True)\n","```\n","\n","\n","#### **String Joining** : You can join multiple string columns together using the str.join() method. This is useful when you want to combine multiple string columns into a single column.\n","\n","\n","```python\n","# Join multiple string columns into a single column\n","df['Full Name'] = df[['First Name', 'Last Name']].apply(lambda x: ', '.join(x), axis=1)\n","````\n","\n","\n","#### **String Extraction** : You can extract specific patterns or substrings from strings using regular expressions and the str.extract() method. This allows you to extract valuable information from unstructured text data.\n","\n","\n","```python\n","# Extract numbers from a string column\n","df['Numbers'] = df['Text'].str.extract(r'(\\d+)')\n","````\n","\n","\n","#### **String Replacement** : You can replace specific substrings or patterns within strings using the str.replace() method. This can be useful for data cleaning or standardization tasks.\n","\n","```python\n","# Replace a substring in a string column\n","df['Text'] = df['Text'].str.replace('old', 'new')\n","String Matching: You can perform advanced string matching and pattern recognition using functions such as str.contains() or str.match(). \n","    This allows you to search for specific patterns or substrings within strings.\n","```\n","\n","```python\n","# Check if a string contains a specific pattern\n","df['Contains Pattern'] = df['Text'].str.contains(r'\\bpattern\\b')\n","```\n","\n","#### These are just a few examples of the advanced string manipulation techniques available in Pandas. The str accessor provides a wide range of string functions and methods that can be combined and applied creatively to perform complex string operations on your data."]},{"cell_type":"markdown","id":"0896455f","metadata":{"papermill":{"duration":0.052876,"end_time":"2023-08-27T14:38:20.124418","exception":false,"start_time":"2023-08-27T14:38:20.071542","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>80 </font></h1>\n"]},{"cell_type":"markdown","id":"927d4b5c","metadata":{"papermill":{"duration":0.053263,"end_time":"2023-08-27T14:38:20.231232","exception":false,"start_time":"2023-08-27T14:38:20.177969","status":"completed"},"tags":[]},"source":["# Chapter 80 : Handling Multi-dimensional Data: Panel, Panel4D\n","#### Pandas provides the Panel and Panel4D data structures for handling multi-dimensional data. While these data structures have been deprecated in recent versions of Pandas, they can still be used for certain use cases.\n","\n","#### The Panel data structure represents three-dimensional data, while the Panel4D data structure represents four-dimensional data. Both structures are essentially containers for holding data with axes labeled as items, major axis, minor axis, and possibly a fourth dimension for Panel4D. They provide an additional dimension for organizing data beyond the two-dimensional structure of a DataFrame.\n","\n","### Here's an example of working with a Panel in Pandas:\n","\n","```python\n","import pandas as pd\n","import numpy as np\n","\n","# Create a sample Panel\n","data = np.random.rand(2, 3, 4)\n","panel = pd.Panel(data, items=['Item1', 'Item2'], major_axis=pd.date_range('2023-01-01', periods=3), minor_axis=['A', 'B', 'C', 'D'])\n","\n","# Access data in the Panel\n","print(panel['Item1'])  # Access data for 'Item1'\n","print(panel['Item2'])  # Access data for 'Item2'\n","\n","# Access data using the major and minor axis labels\n","print(panel.loc[:, '2023-01-02', 'B'])  # Access data for all items on '2023-01-02' and 'B'\n","\n","# Convert the Panel to a DataFrame\n","df = panel.to_frame()\n","```\n","\n","#### In this example, we create a Panel called panel with random data. We specify the items, major axis, and minor axis labels to organize the data. We can access data within the Panel using indexing and label-based indexing. \n","#### Finally, we can convert the Panel to a DataFrame using the to_frame() method.\n","\n","#### While the Panel and Panel4D data structures are deprecated, you can still use them for certain use cases. However, for most multi-dimensional data analysis tasks, it is recommended to use other data structures or reshape the data into a suitable format using DataFrames and MultiIndexing."]},{"cell_type":"markdown","id":"4ce9f910","metadata":{"papermill":{"duration":0.05354,"end_time":"2023-08-27T14:38:20.339493","exception":false,"start_time":"2023-08-27T14:38:20.285953","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>81</font></h1>\n"]},{"cell_type":"markdown","id":"0c77947d","metadata":{"papermill":{"duration":0.053647,"end_time":"2023-08-27T14:38:20.447016","exception":false,"start_time":"2023-08-27T14:38:20.393369","status":"completed"},"tags":[]},"source":["# Chapter 81 Data Normalization and Scaling :\n","#### Data normalization and scaling are important preprocessing steps in data analysis and machine learning. They help to ensure that features are on a similar scale, which can improve the performance of certain algorithms and prevent certain features from dominating the analysis.\n","\n","#### Pandas doesn't have built-in methods for data normalization and scaling, but you can easily achieve these operations using libraries like Scikit-learn or NumPy.\n","\n","## Here's an example of data normalization using Scikit-learn's MinMaxScaler:\n","\n","```python\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Create a MinMaxScaler object\n","scaler = MinMaxScaler()\n","\n","# Fit and transform the data\n","normalized_data = scaler.fit_transform(df)\n","```\n","\n","#### In this example, we create a MinMaxScaler object and apply it to the DataFrame df using the fit_transform() method. This scales the values in each column to a range between 0 and 1.\n","\n","#### Similarly, you can perform other types of scaling such as standardization using Scikit-learn's StandardScaler or logarithmic scaling using NumPy's log() function.\n","\n","\n","<h1 align=\"left\"><font color='red'>82</font></h1>\n","\n","\n","\n","# Chapter 82 Handling Multi-index Data: Indexing and Slicing\n","#### Multi-indexing in Pandas allows you to work with hierarchical or multi-level indexed data. It provides a way to organize and represent data with multiple dimensions or levels of granularity.\n","\n","#### Here's an example of indexing and slicing with a multi-index DataFrame:\n","\n","```python\n","# Create a multi-index DataFrame\n","index = pd.MultiIndex.from_product([['A', 'B'], ['X', 'Y']])\n","columns = ['Value1', 'Value2']\n","data = [[1, 2], [3, 4], [5, 6], [7, 8]]\n","df = pd.DataFrame(data, index=index, columns=columns)\n","\n","# Access data using the outer and inner levels of the index\n","print(df.loc['A'])  # Access data for the outer level 'A'\n","print(df.loc[('A', 'X')])  # Access data for the inner levels 'A' and 'X'\n","\n","# Perform slicing on the index levels\n","print(df.loc['A':'B'])  # Slice on the outer level\n","print(df.loc[('A', 'X'):('B', 'Y')])  # Slice on both levels\n","\n","# Access specific columns using the index levels\n","print(df.loc[:, ('Value1', 'Value2')])  # Access specific columns for all index levels\n","```\n","\n","\n","#### In this example, we create a multi-index DataFrame df with two levels of indexing. We can access data using the outer and inner levels of the index by using the loc indexer. Slicing can be performed on one or both levels of the index. We can also access specific columns for all index levels using the loc indexer.\n","\n","#### Multi-indexing provides powerful capabilities for working with complex and structured data. It allows for efficient querying, filtering, and aggregation across multiple dimensions.\n","\n","<h1 align=\"left\"><font color='red'>83 </font></h1>\n","\n","\n","# Chapter 83 Time Series Decomposition\n","#### Time series decomposition is a technique used to break down a time series into its underlying components, such as trend, seasonality, and noise. It helps in understanding the underlying patterns and structure of the time series data.\n","\n","#### Pandas provides functionality for time series decomposition through the seasonal_decompose() function in the statsmodels library. Here's an example:\n","\n","```python\n","import pandas as pd\n","import statsmodels.api as sm\n","\n","# Create a time series DataFrame\n","index = pd.date_range(start='2020-01-01', periods=365, freq='D')\n","data = pd.Series(range(365), index=index)\n","\n","# Perform time series decomposition\n","decomposition = sm.tsa.seasonal_decompose(data)\n","\n","# Access the components\n","trend = decomposition.trend\n","seasonal = decomposition.seasonal\n","residual = decomposition.resid\n","```\n","\n","#### In this example, we create a time series DataFrame data with daily values. We use the seasonal_decompose() function from the statsmodels library to perform time series decomposition. The resulting components, including trend, seasonal, and residual, can be accessed and used for further analysis.\n","\n","#### Time series decomposition can be helpful in identifying patterns, making forecasts, and understanding the underlying dynamics of the time series data.\n","\n","<h1 align=\"left\"><font color='red'>84</font></h1>\n","\n","\n","# Chapter 84 : Handling Multi-index Data: Aggregation and Reshaping\n","#### When working with multi-index data in Pandas, you can perform various aggregation and reshaping operations to summarize and transform the data.\n","\n","#### Here's an example of aggregating and reshaping a multi-index DataFrame:\n","```python\n","# Create a multi-index DataFrame\n","index = pd.MultiIndex.from_product([['A', 'B'], ['X', 'Y']])\n","columns = ['Value1', 'Value2']\n","data = [[1, 2], [3, 4], [5, 6], [7, 8]]\n","df = pd.DataFrame(data, index=index, columns=columns)\n","\n","# Aggregate the data by the outer and inner levels of the index\n","aggregated = df.groupby(level=[0, 1]).sum()\n","\n","# Reshape the DataFrame using pivot\n","reshaped = df.pivot(columns='Value1', values='Value2')\n","```\n","\n","#### In this example, we create a multi-index DataFrame df with two levels of indexing. We can aggregate the data using the groupby() method and specify the levels of the index to group by. The resulting DataFrame aggregated contains the sum of values for each combination of outer and inner levels.\n","\n","#### We can also reshape the data using the pivot() method. In this case, we pivot the DataFrame df by specifying the column to be used as columns in the resulting DataFrame and the values to be placed in the columns.\n","\n","#### Aggregation and reshaping operations allow you to transform the structure of the data and derive new insights from multi-index data.\n","\n","<h1 align=\"left\"><font color='red'>85</font></h1>\n","\n","\n","# Chapter 85 Time Series Smoothing Techniques\n","#### Time series smoothing techniques are used to remove noise or fluctuations from time series data, making underlying patterns and trends more apparent.\n","\n","#### Pandas provides several methods for time series smoothing, including moving averages and exponential smoothing.\n","\n","#### Here's an example of applying a simple moving average to a time series:\n","\n","```python\n","import pandas as pd\n","\n","# Create a time series DataFrame\n","index = pd.date_range(start='2020-01-01', periods=365, freq='D')\n","data = pd.Series(range(365), index=index)\n","\n","# Apply a simple moving average with window size 7\n","smoothed_data = data.rolling(window=7).mean()\n","````\n","\n","#### In this example, we create a time series DataFrame data with daily values. We apply a simple moving average using the rolling() method and specify the window size as 7, indicating a 7-day moving average. The resulting smoothed data is stored in the smoothed_data DataFrame.\n","\n","#### Exponential smoothing techniques, such as the Holt-Winters method, can also be applied using the statsmodels library.\n","\n"]},{"cell_type":"markdown","id":"3c173bdd","metadata":{"papermill":{"duration":0.054016,"end_time":"2023-08-27T14:38:20.554879","exception":false,"start_time":"2023-08-27T14:38:20.500863","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>86</font></h1>\n","\n","# Chapter 86 Outlier Detection and Treatment\n","#### Outliers are data points that significantly deviate from the overall pattern or distribution of a dataset. Outlier detection is the process of identifying and handling these extreme values. Pandas offers various techniques to detect and treat outliers in your data.\n","\n","## 86.1 Here are some common approaches for outlier detection and treatment using Pandas:\n","\n","#### **Statistical methods** : Calculate statistical measures such as z-score, standard deviation, or percentiles to identify data points that fall outside a certain threshold. You can use Pandas' describe() function to obtain summary statistics and quantile() function to calculate percentiles.\n","#### **Box plots**: Visualize the distribution of your data using box plots and identify data points that fall outside the whiskers, which represent the typical range of values. Pandas' boxplot() function can be used to create box plots.\n","#### **IQR method** : Calculate the interquartile range (IQR) and identify outliers as data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR, where Q1 and Q3 represent the first and third quartiles, respectively. Pandas' quantile() function can be used to calculate quartiles.\n","#### **Domain knowledge** : Utilize domain knowledge or expert judgment to define specific thresholds or rules for identifying outliers. This approach is particularly useful when the data has specific context or constraints that may not be captured by statistical methods alone.\n","#### **Treatment options** : Once outliers are detected, you can choose to remove them from the dataset, replace them with missing values, or transform them to reduce their impact. Pandas provides functions such as drop() and fillna() to remove or replace values, and you can use data transformation techniques like winsorization or logarithmic transformation to adjust extreme values.\n","#### By leveraging these techniques and Pandas' data manipulation capabilities, you can effectively detect and handle outliers in your dataset.\n","\n","<h1 align=\"left\"><font color='red'>87</font></h1>\n","\n","\n","# Chapter 87 Feature Engineering with Pandas\n","#### Feature engineering is the process of creating new features or transforming existing features to enhance the predictive power of a dataset for machine learning models. Pandas offers a wide range of functionalities to perform feature engineering tasks.\n","\n","## Here are some common feature engineering techniques using Pandas:\n","\n","#### **Creating new features**: Generate new features by combining or transforming existing features. For example, you can create interaction features by multiplying or dividing two numeric columns, create polynomial features by raising a column to a higher power, or create indicator variables based on certain conditions using functions like apply() or assign().\n","#### **Handling datetime data**: Extract meaningful information from datetime columns, such as day of the week, month, year, or time of the day. Pandas' dt accessor provides a set of properties and methods to extract and manipulate datetime components.\n","#### **Encoding categorical variables**: Convert categorical variables into numerical representations that can be used in machine learning models. Pandas provides functions like get_dummies() for one-hot encoding and LabelEncoder() for label encoding.\n","#### **Binning and discretization**: Group continuous variables into bins or discrete intervals to capture non-linear relationships or reduce the impact of outliers. You can use Pandas' cut() or qcut() functions to perform binning.\n","#### **Handling missing values**: Generate new features indicating the presence or absence of missing values in the dataset. Pandas' isnull() function can be used to identify missing values, and new columns can be created based on the results.\n","#### **Aggregation and summary statistics**: Compute aggregate statistics such as mean, median, sum, or standard deviation for groups of data points using Pandas' groupby() function. These aggregated values can serve as new features representing the behavior of the data at different levels of granularity.\n","#### By utilizing these feature engineering techniques with Pandas, you can extract valuable information from your dataset and improve the performance of your machine learning models.\n","\n","<h1 align=\"left\"><font color='red'>88</font></h1>\n","\n","# Chapter 88 Handling Imbalanced Time Series Data\n","#### Imbalanced time series data refers to a dataset where the distribution of target classes or outcomes is highly skewed. In such cases, standard machine learning algorithms may struggle to learn patterns effectively, as they tend to be biased towards the majority class.\n","\n","## Here are some strategies for handling imbalanced time series data using Pandas:\n","\n","#### **Resampling techniques**: Use resampling techniques to balance the distribution of classes in the dataset. Oversampling involves duplicating or creating new samples from the minority class, while undersampling involves reducing the number of samples from the majority class. Pandas provides functions like sample() and concat() to perform resampling operations.\n","#### **Window-based sampling**: Divide the time series data into smaller windows or segments and balance the class distribution within each window. This approach can be useful when the class distribution varies across different time periods. Pandas' rolling() function can be used to create rolling windows.\n","#### **Feature engineering**: Generate new features that capture the patterns or characteristics of the imbalanced classes. For example, you can calculate statistical measures like mean, median, or variance for each class within specific time windows. Pandas provides functions like rolling() and groupby() to compute these features.\n","#### **Ensemble methods**: Utilize ensemble methods that combine multiple models to handle imbalanced data. Techniques such as bagging, boosting, or stacking can improve the predictive performance by considering the minority class more effectively. Pandas can be used in combination with ensemble libraries like Scikit-learn to implement these methods.\n","#### By applying these techniques in combination with Pandas' data manipulation and analysis capabilities, you can address the challenges of imbalanced time series data and improve the accuracy of your models.\n","\n","<h1 align=\"left\"><font color='red'>89</font></h1>\n","\n","# Chapter 89 Dimensionality Reduction with Pandas\n","#### Dimensionality reduction is the process of reducing the number of features or variables in a dataset while preserving the most important information. It can be useful in reducing noise, speeding up computation, and visualizing high-dimensional data. Pandas provides functionalities that can be used for dimensionality reduction tasks.\n","\n","## 89.1Here are some common dimensionality reduction techniques using Pandas:\n","\n","#### **Feature selection**: Select a subset of the most relevant features from the original dataset. This can be done by evaluating the importance or relevance of each feature using statistical methods or machine learning models. Pandas provides functions like corr() to calculate feature correlations and SelectKBest from Scikit-learn to select top-k features based on statistical tests.\n","#### **Feature extraction** : Create new features that capture the most important information from the original features. Principal Component Analysis (PCA) is a popular technique for feature extraction. Pandas can be used in combination with PCA implementations from libraries like Scikit-learn to perform PCA on the dataset.\n","#### **t-SNE**: t-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction technique specifically designed for visualizing high-dimensional data. It maps high-dimensional data points to a lower-dimensional space while preserving the local structure of the data. Pandas can be used to preprocess the data before applying t-SNE and to visualize the results.\n","#### It's important to note that dimensionality reduction should be applied with caution, as it may result in some loss of information. It's recommended to assess the impact of dimensionality reduction on the performance of your downstream tasks, such as machine learning models, before applying it to your data.\n","\n","<h1 align=\"left\"><font color='red'>90</font></h1>\n","\n","# Chaper 90 Working with Database APIs: SQLAlchemy\n","#### SQLAlchemy is a popular Python library that provides a high-level interface for interacting with databases. It allows you to connect to various database systems, execute SQL queries, and perform data manipulation operations. Pandas integrates well with SQLAlchemy, enabling you to leverage the power of both libraries for working with databases.\n","\n","## Here are some key capabilities of Pandas when working with database APIs using SQLAlchemy:\n","\n","#### **Connecting to databases**: Pandas provides the read_sql() function, which allows you to execute SQL queries and retrieve the results directly into a DataFrame. You can pass a SQLAlchemy connection object or a database URL to establish a connection.\n","#### **Reading data from tables** : You can use Pandas' read_sql_table() function to read data from a specific table in a database. This function takes the table name and the database connection as input and returns a DataFrame containing the table data.\n","#### **Writing data to tables** : Pandas offers the to_sql() function to write DataFrame data into a database table. You can specify the table name and the database connection, and Pandas will handle the creation of the table and the insertion of the data.\n","#### **Executing custom SQL queries**: With Pandas and SQLAlchemy, you have the flexibility to execute custom SQL queries using the read_sql_query() function. You can write complex queries involving joins, aggregations, or subqueries and retrieve the results as a DataFrame.\n","#### By combining the capabilities of Pandas and SQLAlchemy, you can seamlessly work with databases, perform data analysis and manipulation, and leverage the rich functionality of both libraries for your data projects."]},{"cell_type":"markdown","id":"4ce4b336","metadata":{"papermill":{"duration":0.055723,"end_time":"2023-08-27T14:38:20.669215","exception":false,"start_time":"2023-08-27T14:38:20.613492","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>91</font></h1>"]},{"cell_type":"markdown","id":"ee265d7a","metadata":{"papermill":{"duration":0.052953,"end_time":"2023-08-27T14:38:20.775744","exception":false,"start_time":"2023-08-27T14:38:20.722791","status":"completed"},"tags":[]},"source":["# Chapter 91 Time Series Analysis: Autocorrelation and Partial Autocorrelation\n","#### Autocorrelation and partial autocorrelation are important tools in time series analysis for understanding the correlation structure within a time series.\n","\n","#### Autocorrelation measures the correlation between a time series and its lagged values. It helps identify patterns or dependencies within the time series. The autocorrelation function (ACF) is commonly used to plot the autocorrelation at various lags.\n","\n","#### Partial autocorrelation, on the other hand, measures the correlation between a time series and its lagged values after removing the effects of intermediate lags. It helps identify the direct relationship between a time series and its lagged values. The partial autocorrelation function (PACF) is commonly used to plot the partial autocorrelation at various lags.\n","\n","#### In Pandas, you can compute and plot autocorrelation and partial autocorrelation using the autocorr() and plot_acf() functions from the statsmodels library.\n","\n","## Here's an example:\n","\n","```python\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","\n","# Create a time series DataFrame\n","index = pd.date_range(start='2020-01-01', periods=365, freq='D')\n","data = pd.Series(range(365), index=index)\n","\n","# Plot autocorrelation and partial autocorrelation\n","fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n","plot_acf(data, ax=ax1, lags=30)\n","plot_pacf(data, ax=ax2, lags=30)\n","plt.show()\n","```\n","\n","#### In this example, we create a time series DataFrame data with daily values. We then use the plot_acf() and plot_pacf() functions to plot the autocorrelation and partial autocorrelation functions, respectively. The lags parameter specifies the number of lags to consider in the plots.\n","\n","#### Autocorrelation and partial autocorrelation plots help in identifying the order of autoregressive (AR) and moving average (MA) terms in time series models and guide the selection of appropriate models for forecasting.\n","\n","<h1 align=\"left\"><font color='red'>92</font></h1>\n","\n","# Chapter  92 Working with Financial Time Series Data\n","#### Working with financial time series data requires specialized techniques and tools due to the unique characteristics of financial markets, such as irregular trading hours, missing data, and the presence of outliers.\n","\n","#### In Pandas, you can use the DataFrame object to handle financial time series data. Here are some key operations and considerations when working with financial time series data:\n","\n","#### **Importing data**: Use Pandas' read_csv() or read_excel() functions to import financial data from CSV or Excel files, respectively.\n","#### **Handling missing data**: Use Pandas' fillna() or dropna() methods to handle missing values in the financial time series.\n","#### **Resampling**: Use Pandas' resample() method to convert the frequency of the financial time series data (e.g., from daily to monthly).\n","#### **Calculating returns**: Use Pandas' pct_change() method to calculate the returns of financial time series data.\n","#### **Technical analysis**: Use Pandas' rolling window functions, such as rolling_mean() or rolling_std(), to calculate moving averages, volatility, and other technical indicators.\n","#### **Visualization**: Use Pandas' integration with Matplotlib or other visualization libraries to create charts and plots for analyzing financial time series data.\n","#### Pandas provides a powerful and flexible framework for working with financial time series data, allowing you to perform various data manipulation, analysis, and visualization tasks.\n","\n","<h1 align=\"left\"><font color='red'>93</font></h1>\n","\n","# Chaptewr 93 Handling Seasonal Time Series Data\n","#### Seasonal time series data exhibits repeating patterns or fluctuations at fixed intervals, such as daily, monthly, or yearly. Handling seasonal time series data requires specific techniques to capture and model these seasonal patterns.\n","\n","#### In Pandas, you can handle seasonal time series data using the following approaches:\n","\n","#### **Resampling**: Use Pandas' resample() method to convert the frequency of the time series data. For example, you can resample daily data to monthly or quarterly data to smooth out seasonal effects.\n","#### **Seasonal decomposition**: Use time series decomposition techniques, such as additive or multiplicative decomposition, to separate the seasonal component from the trend and residual components. Pandas provides functionality for seasonal decomposition using the seasonal_decompose() function from the statsmodels library.\n","#### **Seasonal differencing**: Compute the difference between the time series data and its lagged values at the seasonal frequency. This helps in removing the seasonal patterns and making the data stationary, which can facilitate modeling and analysis.\n","#### **Seasonal models**: Use specialized models, such as SARIMA (Seasonal Autoregressive Integrated Moving Average), to explicitly model and forecast seasonal time series data. The statsmodels library in Pandas provides support for SARIMA modeling.\n","#### Handling seasonal time series data requires a combination of data manipulation, visualization, and modeling techniques. Pandas provides a range of tools and functions to assist in these tasks.\n","\n","<h1 align=\"left\"><font color='red'>94</font></h1>\n","\n","# Chapter 94 Sentiment Analysis with Pandas\n","#### Sentiment analysis is a technique used to determine the sentiment or emotional tone of a piece of text. Pandas can be used in conjunction with other libraries, such as NLTK (Natural Language Toolkit), to perform sentiment analysis on textual data.\n","\n","#### Here's a high-level overview of performing sentiment analysis with Pandas:\n","\n","#### Preprocess the text data: Use Pandas to clean and preprocess the text data by removing stopwords, punctuation, and performing other text cleaning operations.\n","#### Tokenize the text: Split the text into individual words or tokens using Pandas' string manipulation methods or NLTK's tokenization functions.\n","#### Perform sentiment analysis: Use a sentiment analysis model or lexicon, such as VADER (Valence Aware Dictionary and sEntiment Reasoner), to assign sentiment scores to each token or the entire text.\n","#### Aggregate sentiment scores: Use Pandas' grouping and aggregation functions to calculate aggregate sentiment scores at various levels, such as by document, author, or time period.\n","#### Analyze and visualize the sentiment: Use Pandas' data manipulation and visualization capabilities to analyze and visualize the sentiment trends and patterns in the data.\n","#### Pandas provides a convenient and efficient way to preprocess, analyze, and visualize textual data for sentiment analysis tasks.\n","\n","<h1 align=\"left\"><font color='red'>95</font></h1>\n","\n","# Chapter 95 :Time Series Forecast Evaluation Metrics\n","#### When working with time series forecasting, it is essential to evaluate the performance of the forecasted values compared to the actual values. Pandas provides various evaluation metrics to assess the accuracy and reliability of time series forecasts.\n","\n","#### Here are some commonly used time series forecast evaluation metrics:\n","\n","#### Mean Absolute Error (MAE): Measures the average absolute difference between the forecasted values and the actual values.\n","#### Mean Squared Error (MSE): Measures the average squared difference between the forecasted values and the actual values.\n","#### Root Mean Squared Error (RMSE): Calculates the square root of the MSE, providing a measure of the average absolute difference between the forecasted values and the actual values.\n","#### Mean Absolute Percentage Error (MAPE): Calculates the average percentage difference between the forecasted values and the actual values.\n","#### R-squared (R2) Score: Measures the proportion of the variance in the dependent variable (actual values) that can be explained by the independent variable (forecasted values).\n","#### Pandas provides functions to compute these evaluation metrics, such as mean_absolute_error(), mean_squared_error(), mean_absolute_percentage_error(), and r2_score(), which can be used to assess the performance of time series forecasts.\n","\n","#### By evaluating these metrics, you can assess the accuracy, bias, and overall quality of your time series forecasts, helping you make informed decisions and improve your forecasting models."]},{"cell_type":"code","execution_count":null,"id":"7ac7933c","metadata":{"papermill":{"duration":0.052861,"end_time":"2023-08-27T14:38:20.881716","exception":false,"start_time":"2023-08-27T14:38:20.828855","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"c005d9d9","metadata":{"papermill":{"duration":0.053465,"end_time":"2023-08-27T14:38:20.988253","exception":false,"start_time":"2023-08-27T14:38:20.934788","status":"completed"},"tags":[]},"source":["<h1 align=\"left\"><font color='red'>96</font></h1>\n","\n","# 96 Working with Geospatial Time Series Data\n","#### Geospatial time series data combines temporal information with geographic location data. Pandas can be used to handle geospatial time series data by integrating it with geospatial libraries like GeoPandas and GeoPandas-TDA.\n","\n","#### Here are some steps to work with geospatial time series data using Pandas:\n","\n","#### Import geospatial data: Use Pandas to read geospatial data files, such as shapefiles or GeoJSON files, and load them into a DataFrame.\n","#### Handle time series data: Convert the timestamp information in your data to a Pandas datetime format to enable time-based operations and analysis.\n","#### Merge geospatial and time series data: Use Pandas' merging and joining capabilities to combine the geospatial data with the time series data based on a common identifier or spatial proximity.\n","#### Perform spatial operations: Utilize the spatial functionality provided by GeoPandas to perform spatial operations such as spatial joins, spatial filtering, and spatial aggregations on the geospatial time series data.\n","#### Visualize the data: Use Pandas' plotting functions in combination with geospatial libraries like GeoPandas and Matplotlib to create visualizations of the geospatial time series data, such as time series maps or animations.\n","#### By leveraging the capabilities of Pandas and geospatial libraries, you can effectively analyze and visualize geospatial time series data, unlocking insights and patterns specific to both time and location.\n","\n","<h1 align=\"left\"><font color='red'>97</font></h1>\n","\n","# Chapter 97 Advanced Data Visualization: Maps and Geographical Plots\n","#### Advanced data visualization techniques in Pandas include creating maps and geographical plots to visualize spatial data. Pandas can be combined with libraries such as GeoPandas, Matplotlib, and Plotly to generate interactive and visually appealing maps.\n","\n","#### Here's an overview of the steps involved in creating maps and geographical plots using Pandas:\n","\n","#### Load geospatial data: Import geospatial data, such as shapefiles or GeoJSON files, into a GeoDataFrame using GeoPandas.\n","#### Prepare the data: Ensure the geospatial data is in a suitable format for visualization, with the necessary geometries and attributes.\n","#### Merge with other data: If required, merge the geospatial data with other data sources using Pandas' merging capabilities, based on common identifiers or spatial relationships.\n","#### Create the plot: Use Matplotlib or Plotly to generate the map or geographical plot. Customize the appearance, colors, legends, and other visual elements to convey the desired information effectively.\n","#### Add interactivity: With Plotly, you can add interactive elements to your maps, such as hover tooltips, zooming, panning, and animations, to enhance the user experience.\n","#### Save or display the plot: Save the plot as an image file or display it interactively in a Jupyter Notebook or web application.\n","#### By combining Pandas with geospatial libraries and advanced visualization tools, you can create compelling maps and geographical plots to showcase and explore your spatial data.\n","\n","<h1 align=\"left\"><font color='red'>98</font></h1>\n","\n","# Chapter 98 Handling Unstructured Text Data with Pandas\n","#### Unstructured text data refers to textual information that does not adhere to a specific structure or format, such as social media posts, customer reviews, or news articles. Pandas can be utilized to handle unstructured text data by leveraging its string manipulation functions and integration with natural language processing (NLP) libraries like NLTK or spaCy.\n","\n","### Here's a general approach to handling unstructured text data with Pandas:\n","\n","#### Import the text data: Load the unstructured text data into a Pandas DataFrame, ensuring that each text document is represented as a separate row or cell in the DataFrame.\n","#### Preprocess the text data: Apply preprocessing techniques such as tokenization, removing punctuation, converting to lowercase, and removing stop words to clean the text data. Pandas' string manipulation functions, combined with regular expressions, can be used for these tasks.\n","#### Perform text analysis: Utilize NLP libraries like NLTK or spaCy to perform advanced text analysis tasks, such as part-of-speech tagging, named entity recognition, sentiment analysis, or topic modeling. Pandas can facilitate the integration of these libraries by providing convenient methods to apply functions to text data.\n","#### Extract features: Convert the text data into numerical or categorical features that can be used for further analysis or modeling. This can involve techniques such as bag-of-words representation, TF-IDF (Term Frequency-Inverse Document Frequency), or word embeddings.\n","#### Analyze and visualize: Use Pandas' data analysis and visualization capabilities to gain insights from the processed text data. Generate summary statistics, word frequency counts, word clouds, or other visualizations to understand the patterns and characteristics of the text data.\n","#### By leveraging Pandas' string manipulation functions, data manipulation capabilities, and integration with NLP libraries, you can effectively handle and analyze unstructured text data within a Pandas workflow.\n","\n","<h1 align=\"left\"><font color='red'>99</font></h1>\n","\n","# chapter 99 Anomaly Detection in Time Series Data\n","#### Anomaly detection refers to the process of identifying patterns or data points that deviate significantly from the expected behavior in a time series. Pandas provides several techniques and algorithms that can be used for anomaly detection in time series data.\n","\n","#### Here's an overview of some common approaches to anomaly detection in time series data using Pandas:\n","\n","#### Statistical methods: Apply statistical techniques such as z-score, standard deviation, or percentile-based methods to identify data points that fall outside a certain threshold of the expected range.\n","#### Moving average: Calculate the moving average of the time series data and identify data points that deviate significantly from the moving average.\n","#### Exponential smoothing: Apply exponential smoothing techniques such as the Holt-Winters method to detect anomalies based on the smoothed forecasted values.\n","#### Machine learning algorithms: Utilize machine learning algorithms, such as isolation forest, one-class SVM (Support Vector Machines), or autoencoders, to train models on normal behavior and detect anomalies based on deviations from the learned patterns.\n","#### Time series decomposition: Decompose the time series into its trend, seasonality, and residual components using techniques like seasonal decomposition of time series (STL) or moving average decomposition, and identify anomalies in the residual component.\n","#### Rule-based methods: Define specific rules or thresholds based on domain knowledge or expert judgment to flag data points as anomalies if they violate these rules.\n","#### By leveraging Pandas' time series manipulation capabilities and integrating with statistical methods or machine learning algorithms, you can effectively detect anomalies in time series data and gain insights into unusual patterns or behaviors.\n","\n","<h1 align=\"left\"><font color='red'>100</font></h1>\n","\n","# Chapter 100 Data Cleaning: Handling Inconsistent and Noisy Data\n","#### Data cleaning is a critical step in the data preprocessing pipeline, as it involves handling inconsistent and noisy data to ensure its quality and reliability. Pandas provides a range of functionalities that can be utilized for data cleaning tasks.\n","\n","## Here are some common techniques for handling inconsistent and noisy data using Pandas:\n","\n","#### Handling missing values: Use Pandas' functions such as isnull(), fillna(), or dropna() to identify and handle missing values in the data. You can choose to impute missing values with mean, median, or mode, or drop rows or columns with a high percentage of missing values based on your data analysis requirements.\n","#### Dealing with duplicates: Use Pandas' duplicated() and drop_duplicates() functions to identify and remove duplicate rows in the dataset. You can specify columns to consider when identifying duplicates and choose whether to keep the first occurrence or only keep the last occurrence of each duplicated row.\n","#### Correcting inconsistent data: Apply string manipulation functions in Pandas to standardize inconsistent values, such as converting strings to lowercase, removing leading or trailing whitespace, or replacing incorrect values with the correct ones based on domain knowledge.\n","#### Handling outliers: Use statistical techniques such as z-score or interquartile range (IQR) to identify and handle outliers in the data. You can choose to remove outliers or transform them to reduce their impact on the analysis.\n","#### Handling inconsistent formats: Convert data to consistent formats by utilizing Pandas' string manipulation functions, regular expressions, or custom functions. For example, you can convert date strings to a standardized date format or convert numerical values with different units to a consistent unit.\n","#### Error handling: Use exception handling techniques to handle errors that may arise during data cleaning processes. Pandas provides functionalities to catch and handle errors, allowing you to handle exceptional cases gracefully.\n","#### By applying these techniques and utilizing Pandas' data manipulation capabilities, you can effectively clean and preprocess inconsistent and noisy data, ensuring its quality and reliability for further analysis and modeling."]},{"cell_type":"code","execution_count":null,"id":"50f93224","metadata":{"papermill":{"duration":0.053227,"end_time":"2023-08-27T14:38:21.094581","exception":false,"start_time":"2023-08-27T14:38:21.041354","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"daba25eb","metadata":{"execution":{"iopub.execute_input":"2023-07-04T13:12:08.764405Z","iopub.status.busy":"2023-07-04T13:12:08.764027Z","iopub.status.idle":"2023-07-04T13:12:08.772302Z","shell.execute_reply":"2023-07-04T13:12:08.771243Z","shell.execute_reply.started":"2023-07-04T13:12:08.764375Z"},"papermill":{"duration":0.053618,"end_time":"2023-08-27T14:38:21.201393","exception":false,"start_time":"2023-08-27T14:38:21.147775","status":"completed"},"tags":[]},"source":["```python\n","print(\" AUTHOR - MLV PRASAD \")\n","print(\"Congratulations to all the students on completing all 100 chapters!\")\n","print(\"It's a remarkable achievement that showcases your dedication, hard work, and perseverance.\")\n","print(\"By completing this extensive journey, you have demonstrated your commitment to learning and growth.\")\n","print(\"You've gained valuable knowledge and skills throughout these chapters, which will undoubtedly benefit you in your future endeavors.\")\n","print(\"Keep up the great work, and continue to explore and excel in your educational pursuits!\")\n","\n","```"]},{"cell_type":"markdown","id":"93f59f1d","metadata":{"papermill":{"duration":0.052746,"end_time":"2023-08-27T14:38:21.307412","exception":false,"start_time":"2023-08-27T14:38:21.254666","status":"completed"},"tags":[]},"source":["![mlv prasad](https://raw.githubusercontent.com/MlvPrasadOfficial/ref/master/pandasmlv.png)"]},{"cell_type":"markdown","id":"0422fd6f","metadata":{"papermill":{"duration":0.052975,"end_time":"2023-08-27T14:38:21.414092","exception":false,"start_time":"2023-08-27T14:38:21.361117","status":"completed"},"tags":[]},"source":["1. ![nump](https://raw.githubusercontent.com/MlvPrasadOfficial/ref/main/4.png)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":42.274811,"end_time":"2023-08-27T14:38:22.130179","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-08-27T14:37:39.855368","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}